{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/UNED/blob/main/TP1_Analisis_de_sentimientos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "smid80_coronavirus_covid19_tweets_late_april_path = kagglehub.dataset_download('smid80/coronavirus-covid19-tweets-late-april')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "2Mr-shYcuJVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5497e82-9ed0-4181-f265-ac3f2c0b399a"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/smid80/coronavirus-covid19-tweets-late-april?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 946M/946M [00:20<00:00, 48.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "_IamYF9iuJV4"
      },
      "cell_type": "markdown",
      "source": [
        "# TP1 - Analisis de sentimientos\n",
        "Vamos a realizar un análisis de sentimiento en base a los tweets obtenidos de la segunda quincena de abril de 2020 sobre el covid19."
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "uCU218BbuJV8"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33cmTcyZuJV9"
      },
      "cell_type": "markdown",
      "source": [
        "## Unión y selección de datos\n",
        "Lo primero que hacemos es unir todos los csv para poder procesarlos en un único archivo. Vamos a usar DataFrame de Pandas."
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "8uPCml-euJV-"
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "path = '/kaggle/input/coronavirus-covid19-tweets-late-april/'\n",
        "all_files = glob.glob(os.path.join(path, \"*.CSV\"))\n",
        "\n",
        "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
        "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0JUgXdzuJV-"
      },
      "cell_type": "markdown",
      "source": [
        "* Nos quedamos únicamente con la columna del texto correspondiente al tweet, y del lenguaje:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lo6MT19guJV_"
      },
      "cell_type": "code",
      "source": [
        "df = concatenated_df[['text', 'lang']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rM6-M1AYuJV_"
      },
      "cell_type": "markdown",
      "source": [
        "* Luego nos quedamos solo con los tweets que están en ingles (\"en\"):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-DXHBhDJuJWA"
      },
      "cell_type": "code",
      "source": [
        "filter = df.mask(lambda x: x['lang'] != 'en').dropna()\n",
        "filter.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGcivrPRuJWB"
      },
      "cell_type": "markdown",
      "source": [
        "* Eliminamos los tweet duplicados:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uaFuhHTLuJWB"
      },
      "cell_type": "code",
      "source": [
        "texts = filter['text'].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nXRDD-4JuJWC"
      },
      "cell_type": "markdown",
      "source": [
        "## Clasificación de los datos\n",
        "En este punto ya tenemos en nuestro dataset 3.260.790 tweets.\n",
        "\n",
        "A continuación vamos a extraer los que en sus textos contienen los iconos que representan los sentimientos para clasificar.\n",
        "\n",
        "* Comenzamos con los de \"Alegría\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "D9G6CPAzuJWC"
      },
      "cell_type": "code",
      "source": [
        "joy_icons = '😁|😂|😃|😄|😅|😆|😉|😊|😍'\n",
        "\n",
        "joy = pd.Series(texts, dtype=\"string\", name=\"joy\").str.contains(joy_icons)\n",
        "joy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dGDx1iZuJWD"
      },
      "cell_type": "markdown",
      "source": [
        "* Continuamos con los de \"tristeza - miedo\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1FC3aTBquJWD"
      },
      "cell_type": "code",
      "source": [
        "sad_icons = '😓|😖|😢|😭|😰|😱|🙍|🙎'\n",
        "\n",
        "sad = pd.Series(texts, dtype=\"string\", name=\"sad\").str.contains(sad_icons)\n",
        "sad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZbnlCd5uJWE"
      },
      "cell_type": "markdown",
      "source": [
        "* Continuamos con los de \"enojo\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fqTbQnjTuJWE"
      },
      "cell_type": "code",
      "source": [
        "angry_icons = '😠|😡|😤|🤬|👿|💀|☠'\n",
        "\n",
        "angry = pd.Series(texts, dtype=\"string\", name=\"angry\").str.contains(angry_icons)\n",
        "angry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHA3NatyuJWE"
      },
      "cell_type": "markdown",
      "source": [
        "* Y terminamos con los de \"apoyo - esperanza\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tC9Z3ZfAuJWF"
      },
      "cell_type": "code",
      "source": [
        "hope_icons = '😷|🙋|🙌|🙏'\n",
        "\n",
        "hope = pd.Series(texts, dtype=\"string\", name=\"hope\").str.contains(hope_icons)\n",
        "hope"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BV2hKM7_uJWF"
      },
      "cell_type": "markdown",
      "source": [
        "* Unimos los resultados en un nuevo data frame:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "05ZhD09kuJWF"
      },
      "cell_type": "code",
      "source": [
        "df = pd.concat([texts, joy, sad, angry, hope], axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQwNs8MXuJWG"
      },
      "cell_type": "markdown",
      "source": [
        "* Ahora dividimos el dataset en 2, por un lado los que tienen alguna categoría, y por otro los que no tienen ninguna:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CawthshfuJWG"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = df[(df['joy'] | df['sad'] | df['angry'] | df['hope'])]\n",
        "texts_not_classified = df[~(df['joy'] | df['sad'] | df['angry'] | df['hope'])]['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yiBpLx7nuJWG"
      },
      "cell_type": "markdown",
      "source": [
        "* Con los que estan clasificados, tenemos que limpiar los que tienen más de 1 categoría. Para eso vamos a sumar la cantidad de categorias en una nueva columna y quedarnos unicamente con los que tienen 1:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fY1Iji0EuJWG"
      },
      "cell_type": "code",
      "source": [
        "count = texts_classified.apply(lambda row: row['joy'] + row['sad'] + row['angry'] + row['hope'], axis=1)\n",
        "count = count.rename(\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TJoy4N7duJWG"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = pd.concat([texts_classified, count], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OP7rXn8duJWH"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = texts_classified[texts_classified['count'] == 1]\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "719o6TJOuJWH"
      },
      "cell_type": "markdown",
      "source": [
        "* Ahora en base a lo obtenido, vamos a etiquetar según qué categoría sean, los resultados irán en la columna 'target':"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YIznQeY9uJWH"
      },
      "cell_type": "code",
      "source": [
        "def label_feeling (row):\n",
        "   if row['joy'] :\n",
        "      return 'JOY'\n",
        "   if row['sad'] :\n",
        "      return 'SAD'\n",
        "   if row['angry'] :\n",
        "      return 'ANGRY'\n",
        "   if row['hope'] :\n",
        "      return 'HOPE'\n",
        "   return 'NONE'\n",
        "\n",
        "texts_classified['target'] = texts_classified.apply(lambda row: label_feeling(row), axis=1)\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QHzO_OcuJWH"
      },
      "cell_type": "markdown",
      "source": [
        "* Y finalmente nos quedamos solo con las columnas 'text' y 'target', así nos queda nuestro set de datos clasificado:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "y4Agzr-LuJWH"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = texts_classified[['text', 'target']]\n",
        "texts_classified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-_Do8WKuJWI"
      },
      "cell_type": "markdown",
      "source": [
        "## Limpieza de datos\n",
        "Antes de entrenar nuestro algoritmo, vamos a hacer una limpieza de datos a los textos de los tweets"
      ]
    },
    {
      "metadata": {
        "id": "cZVtcU9wuJWI"
      },
      "cell_type": "markdown",
      "source": [
        "* Primero vamos a aplicar una limpieza con regex para dejar solo las palabras sin puntuaciones ni números:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Xrk5eiSouJWI"
      },
      "cell_type": "code",
      "source": [
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "\n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jS3dyFqAuJWJ"
      },
      "cell_type": "code",
      "source": [
        "clean_text(texts_classified, 'text')\n",
        "texts_classified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "km-xcefKuJWJ"
      },
      "cell_type": "markdown",
      "source": [
        "* Luego vamos a eliminar todas las \"stop words\" para que queden las palabras más importantes:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PYH1Z3JauJWJ"
      },
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "\n",
        "texts_classified['text'] = texts_classified['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vp51vCd0uJWK"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento\n",
        "Vamos ahora a pasar a la etapa de entrenamiento, a continuación detallamos los valores actuales"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_C-7P5lVuJWK"
      },
      "cell_type": "code",
      "source": [
        "texts_classified['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-zJ9cJKuJWS"
      },
      "cell_type": "markdown",
      "source": [
        "El dataset clasificado cuenta con 104.232 tweets. De los cuales tenemos:\n",
        "\n",
        "* HOPE con 48.484\n",
        "* JOY con 36.174\n",
        "* SAD con 10.660\n",
        "* ANGRY con 8.914"
      ]
    },
    {
      "metadata": {
        "id": "A3gl7qUTuJWT"
      },
      "cell_type": "markdown",
      "source": [
        "Como vemos, los datos están desbalanceados, entonces procedemos a acortar el dataset para poder hacer un entrenamiento balanceado.\n",
        "\n",
        "Tomamos como número màximo el de **ANGRY** ya que es el más chico:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JPmbsMBquJWT"
      },
      "cell_type": "code",
      "source": [
        "hope_target = texts_classified[texts_classified['target'] == 'HOPE'].sample(8914)\n",
        "joy_target = texts_classified[texts_classified['target'] == 'JOY'].sample(8914)\n",
        "sad_target = texts_classified[texts_classified['target'] == 'SAD'].sample(8914)\n",
        "angry_target = texts_classified[texts_classified['target'] == 'ANGRY'].sample(8914)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CYuiQ3AZuJWT"
      },
      "cell_type": "code",
      "source": [
        "balanced_data = pd.concat([hope_target, joy_target, sad_target, angry_target], ignore_index=True)\n",
        "balanced_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HF27pwYjuJWU"
      },
      "cell_type": "markdown",
      "source": [
        "* Con este dataset reducido, dividimos en datos de entrenamiento (66%) y datos de prueba (33%), sin perder el balance (para eso se usa stratify):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gS8APxgvuJWU"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(balanced_data['text'], balanced_data['target'], test_size=0.33, random_state=0, stratify=balanced_data['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RcLHvh7guJWU"
      },
      "cell_type": "markdown",
      "source": [
        "* Preparamos el pipeline con el algoritmo de Naive Bayes:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aOVt8HBGuJWV"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xST-1E5uJWW"
      },
      "cell_type": "markdown",
      "source": [
        "* Y lo entrenamos con el dataset de entrenamiento:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NrGYilxkuJWX"
      },
      "cell_type": "code",
      "source": [
        "text_clf = text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOyOt1YSuJWX"
      },
      "cell_type": "markdown",
      "source": [
        "* Finalmente, predecimos en base a los datos de prueba y evaluamos los resultados:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Aw41ex9zuJWY"
      },
      "cell_type": "code",
      "source": [
        "predicted = text_clf.predict(X_test)\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr76vkpouJWY"
      },
      "cell_type": "markdown",
      "source": [
        "* Y así obtenemos los siguientes resultados:"
      ]
    },
    {
      "metadata": {
        "id": "hUz35xgquJWZ"
      },
      "cell_type": "markdown",
      "source": [
        "                precision    recall  f1-score   support\n",
        "\n",
        "       ANGRY       0.33      0.82      0.47      2942\n",
        "        HOPE       0.81      0.61      0.70     16000\n",
        "         JOY       0.71      0.50      0.58     11937\n",
        "         SAD       0.35      0.66      0.45      3518\n",
        "\n",
        "    accuracy                           0.59     34397\n",
        "    macro avg      0.55      0.65      0.55     34397\n",
        "    weighted avg   0.69      0.59      0.61     34397"
      ]
    },
    {
      "metadata": {
        "id": "rPcCT6EhuJWa"
      },
      "cell_type": "markdown",
      "source": [
        "Como conclusión de este entrenamiento, podemos decir que no son los resultados que esperabamos, en el medio se intentaron diversas tecnicas de limpieza de datos que no aportaron mucha mejora. El problema base recae en la cantidad de datos usados para el entrenamiento, ya que el tener que rebalancear, me vi obligado a desprenderme de ciertos datos ya clasificados. La tendencia de clases al rebalancear termina siendo distinta a la obtenida a partir de los emojis, esto también puede influir cuando pasemos a la predicción de los tweets sin clasificar."
      ]
    },
    {
      "metadata": {
        "id": "qwe_Udt2uJWb"
      },
      "cell_type": "markdown",
      "source": [
        "La ditribución nos quedo:\n",
        "* HOPE con 11996\n",
        "* JOY con 8334\n",
        "* ANGRY con 7335\n",
        "* SAD con 6732"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hLmxlapEuJWb"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(predicted).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdHp_ALluJWd"
      },
      "cell_type": "markdown",
      "source": [
        "## Predicción\n",
        "Pasamos a predecir los tweets que nos quedaron sin clasificar. En total son 3.150.070.\n",
        "\n",
        "* Primero que nada, limpiamos los datos de igual manera que los de entrenamiento:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kOiogTeEuJWd"
      },
      "cell_type": "code",
      "source": [
        "clean_data = texts_not_classified.str.lower()\n",
        "clean_data = clean_data.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "clean_data = clean_data.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ekIVWZ9yuJWe"
      },
      "cell_type": "code",
      "source": [
        "clean_data = clean_data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLWkBIsyuJWe"
      },
      "cell_type": "markdown",
      "source": [
        "* Una vez preparados, pasamos a la predicción, utilizando el algoritmo ya entrena previamente:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8EpnxVgpuJWe"
      },
      "cell_type": "code",
      "source": [
        "results = text_clf.predict(clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZlMVSbtuJWf"
      },
      "cell_type": "markdown",
      "source": [
        "* Y vemos los resultados;"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4rgh9JsvuJWf"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(results).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m32WbUDHuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "Los resultados finales son:\n",
        "\n",
        "* ANGRY con 1.169.389 (37,12%)\n",
        "* HOPE con 1.042.784 (33,1%)\n",
        "* SAD con 528.640 (16,78%)\n",
        "* JOY con 409.257 (12,99%)\n",
        "\n",
        "Los tweets positivos representan el 46.1%, mientras que los negativos el 53,9%"
      ]
    },
    {
      "metadata": {
        "id": "S_3S315SuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "A primera vista, si comparamos con el balance de los tweets clasificados, no hay correlación. En estos, hay mayor tendencia hacia a los negativos, en cambio en los de entrenamiento era hacia los positivos. Ahora vamos a comparar contra otro algoritmo."
      ]
    },
    {
      "metadata": {
        "id": "FTzdugIcuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 2\n",
        "Para esta parte utilizamos el algoritmo sacado de la página https://nlpforhackers.io/sentiment-analysis-intro/ que utiliza la libreria de SentiWordNet de Natural Language ToolKits.\n",
        "\n",
        "Previo a eso hace un preprocesamiento de los datos, con lematización de por medio."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Pi4mp2IJuJWg"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    \"\"\"\n",
        "    Convert between the PennTreebank tags to simple Wordnet tags\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"<br />\", \" \")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def swn_polarity(text):\n",
        "    \"\"\"\n",
        "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
        "    \"\"\"\n",
        "\n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        "\n",
        "    text = clean_text(text)\n",
        "\n",
        "\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    for raw_sentence in raw_sentences:\n",
        "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
        "\n",
        "        for word, tag in tagged_sentence:\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "            if not lemma:\n",
        "                continue\n",
        "\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        "\n",
        "            # Take the first sense, the most common\n",
        "            synset = synsets[0]\n",
        "            swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "            tokens_count += 1\n",
        "\n",
        "    # judgment call ? Default to positive or negative\n",
        "    if not tokens_count:\n",
        "        return 0\n",
        "\n",
        "    # sum greater than 0 => positive sentiment\n",
        "    if sentiment >= 0:\n",
        "        return 1\n",
        "\n",
        "    # negative sentiment\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "87Z9oYdBuJWi"
      },
      "cell_type": "markdown",
      "source": [
        "* Corremos el algoritmo (no recomiendo correr de nuevo, ya que no está optimizado y tomó mucho tiempo llegar a resultados):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WSpLzL7KuJWi"
      },
      "cell_type": "code",
      "source": [
        "pred_y = [swn_polarity(text) for text in clean_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fRFlEE3muJWi"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(pred_y).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECgIipsCuJWj"
      },
      "cell_type": "markdown",
      "source": [
        "Los resultados obtenidos fueron:\n",
        "\n",
        "* Positivos: 2.205.853 (70,03%)\n",
        "* Negativos: 944.217 (29.97%)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1dAIGMQUuJWj"
      },
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, los datos obtenidos con el SentiWordNet, tienen mucha mas correlación con los obtenidos y clasificados a partir de los emojis. Esto quiere decir, que estabamos en lo correcto cuando deciamos que nuestro algoritmo de naive bayes no nos había dado muy buenos resultados, a pesar de tener una precisión cercana al 70% en la prueba."
      ]
    },
    {
      "metadata": {
        "id": "T3qmE-nyuJWk"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "A pesar de no poder lograr mejorar el algoritmo de naive bayes, siento que se hizo un buen trabajo con el manejo y limpieza de datos. Quizas se tendría que haber profundizado más en la identificación de elementos dentro de los tweets. Más allá de eso, los resultados a primera vista no parecían ser tan malos, las comparaciones finales demostraron lo contrario.\n",
        "\n",
        "Podemos concluir que una de las causas raices de la mala performance fue la baja cantidad de datos negativos más que nada. Eso causo un desbalanceo en los datos de entrenamiento. A pesa de que agregamos emojis, no pudimos subir los porcentajes de clases dentro de los clasificados. Esto nos llevo a reducir datos de entrenamiento para poder entrenar el algoritmo de manera balanceada.\n",
        "\n",
        "Como mejora a futuro, probaría correr de vuelta el algoritmo pero sin rebalancear las clases, teniendo en cuenta que es muy probable que la tendencia que sigan los tweets sin clasificar se correlacione con los clasificados. Esto quedó demostrado en parte al correr el algoritmo de SentiWordNet."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}