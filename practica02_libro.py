# -*- coding: utf-8 -*-
"""practica02_libro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MDiJUS8mgyokMXHG6KVgQhsG1BvJXGfE

<a href="https://colab.research.google.com/github/Viny2030/UNED/blob/main/practica02_libro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/1_Introduccion/Introduccion.ipynb"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>
"""

print("A: Carga del programa Stanza")

!pip install stanza

print("B: Carga de bibliotecas adicionales")

import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

print("C-Descarga del libro y  limpieza, salida   en impresion y archivo json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import re
# import requests
# import json
# from bs4 import BeautifulSoup
# 
# 
# def download_and_clean_gutenberg_book(book_id):
#     """
#     Descargamos un libro de Proyecto Gutenberg y lo limpiamos
#     de encabezados, pies de página y comentarios.
# 
#     Args:
#         book_id: El ID del libro en Proyecto Gutenberg.
# 
#     Returns:
#         El texto del libro limpio.
#     """
#     url = f"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt"
#     response = requests.get(url)
#     text = response.text
#     # Identificar y eliminar secciones no deseadas
#     start_index = text.find("*** START OF THIS PROJECT GUTENBERG EBOOK ***")  # Corrected indentation
#     end_index = text.find("*** END OF THIS PROJECT GUTENBERG EBOOK ***")  # Corrected indentation
# 
#     if start_index != -1 and end_index != -1:
#         cleaned_text = text[start_index:end_index]  # Corrected indentation
#     else:
#         cleaned_text = text  # Corrected indentation
# 
#     # Eliminar líneas en blanco y espacios iniciales/finales
#     cleaned_text = "\n".join([line.strip() for line in cleaned_text.splitlines() if line.strip()])  # Corrected indentation
# 
#     # Eliminar líneas con comentarios (ejemplo: líneas que comienzan con "*")
#     cleaned_text = "\n".join([line for line in cleaned_text.splitlines() if not line.startswith("*")])  # Corrected indentation
# 
#     return cleaned_text  # Corrected indentation
# 
# 
# # Ejemplo de uso:
# book_id = 1342  # Reemplaza con el ID del libro que deseas descargar
# output_file = "libro_limpio.json"
# 
# # Call the function to get the cleaned text
# cleaned_text = download_and_clean_gutenberg_book(book_id)
# 
# # Creamos un diccionario con el texto limpio
# book_data = {
#     "text": cleaned_text
# }
# 
# # Guardamos los datos en un archivo JSON
# with open(output_file, 'w') as f:
#     json.dump(book_data, f, indent=4)
# 
# with open('libro_limpio.json', 'r') as f:
#     data = json.load(f)
# 
# print(data['text'])

len(data['text'])

print("C-Frecuencia de palabras del libro , salida   en impresion y archivo json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import json
# from collections import Counter
# 
# def contar_palabras(archivo_json):
#   """Cuenta las palabras en un archivo JSON que contiene un campo 'text'.
# 
#   Args:
#     archivo_json: La ruta al archivo JSON.
# 
#   Returns:
#     Un diccionario donde las claves son las palabras y los valores son las frecuencias.
#   """
# 
#   with open(archivo_json, 'r') as f:
#     data = json.load(f)
#     texto = data['text']
# 
#   # Dividir el texto en palabras usando espacios como separador
#   palabras = texto.split()
# 
#   # Contar la frecuencia de cada palabra
#   contador_palabras = Counter(palabras)
# 
#   return contador_palabras
# 
# # Ejemplo de uso:
# archivo = "libro_limpio.json"
# resultado = contar_palabras(archivo)
# 
# # Guardar el resultado en un archivo JSON
# with open('conteo_palabras.json', 'w') as f:
#   json.dump(resultado, f, indent=4)
# 
# # Imprimir el resultado
# for palabra, frecuencia in resultado.items():
#   print(f"La palabra '{palabra}' aparece {frecuencia} veces.")

print("D-Analisis morfosintactico del libro  , salida   en impresion y archivo json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import json
# 
# # Descargar el modelo en ingles
# stanza.download('en')
# 
# # Cargar el texto del libro desde el archivo JSON
# with open('libro_limpio.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#     text = data['text']
# 
# # Crear el pipeline de Stanza con el procesador 'ner' incluido
# nlp = stanza.Pipeline('es', processors=[
#     'tokenize',  # No need for an empty dictionary here
#     'pos',
#     'lemma',
#     'depparse',
#     'ner'
# ])
# 
# # Procesar el texto
# doc = nlp(text)
# 
# # Función para extraer la información de una palabra
# def extraer_info_palabra(word):
#     return {
#         'word': word.text,
#         'pos': word.upos,
#         'lemma': word.lemma,
#         'dep': word.head,
#         'deprel': word.deprel,
#         'ner': word.end_char  # Now the extraction of 'ent' is valid
#     }
# 
# # Crear una lista para almacenar el análisis de cada palabra
# analisis = []
# 
# # Iterar sobre las oraciones y palabras, extrayendo la información
# for sentence in doc.sentences:
#     for word in sentence.words:
#         analisis.append(extraer_info_palabra(word))
# 
# # Guardar el análisis en un archivo JSON
# with open('analisis_morfosintactico.json', 'w', encoding='utf-8') as f:
#     json.dump(analisis, f, indent=4, ensure_ascii=False)
# 
# # Imprimir  elementos del análisis por pantalla (para verificación)
# print(analisis)

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# print(data['text'])

print("E-Extrae las frecuencias de verbos del libro, salida   en impresion y archivo json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import json
# from collections import defaultdict
# 
# def extract_nv_propositions(text):
#   """
#   Extrae proposiciones sujeto-verbo (NV) de un texto utilizando Stanza.
# 
#   Args:
#     text: El texto del libro.
# 
#   Returns:
#     Una lista de diccionarios, donde cada diccionario representa una proposición NV.
#   """
# 
#   nlp = stanza.Pipeline('en')
#   doc = nlp(text)
# 
#   propositions = []
#   for sentence in doc.sentences:
#     for word in sentence.words:
#       if word.upos == 'VERB':
#         subject = find_subject(word, sentence)
#         object_ = find_object(word, sentence)
#         propositions.append({
#           'subject': subject,
#           'verb': word.lemma,
#           'object': object_
#         })
# 
#   return propositions
# 
# def find_subject(verb, sentence):
#   """
#   Finds the subject of a verb in a sentence.
# 
#   Args:
#     verb: The verb object.
#     sentence: The sentence object.
# 
#   Returns:
#     The text of the subject, or None if not found.
#   """
#   # Implement logic to find the subject based on dependency relations
#   # or other linguistic features.
#   # Example using dependency parsing:
#   for word in sentence.words:
#       if word.deprel == 'nsubj' and word.head == verb.id:
#           return word.text  # Found the subject
# 
#   # Return None if no subject is found
#   return None
# 
# def find_object(verb, sentence):
#   """
#   Finds the object of a verb in a sentence.
# 
#   Args:
#     verb: The verb object.
#     sentence: The sentence object.
# 
#   Returns:
#     The text of the object, or None if not found.
#   """
#   # Implement logic to find the object based on dependency relations
#   # or other linguistic features.
#   # Example using dependency parsing:
#   for word in sentence.words:
#       if word.deprel in ('obj', 'dobj') and word.head == verb.id:
#           return word.text  # Found the object
# 
#   # Return None if no object is found
#   return None
# def analyze_propositions(propositions):
#   """
#   Realiza un análisis más profundo de las proposiciones.
# 
#   Args:
#     propositions: Una lista de diccionarios representando las proposiciones.
# 
#   Returns:
#     Un diccionario con estadísticas y análisis de las proposiciones.
#   """
#   verb_frequency = defaultdict(int)
#   for prop in propositions:
#     verb_frequency[prop['verb']] += 1
# 
#   # Otros análisis posibles:
#   # - Identificación de temas recurrentes
#   # - Análisis de la red de relaciones semánticas
#   # - Clasificación de proposiciones según su tipo (acción, estado, etc.)
# 
#   return verb_frequency
# 
# # Ejemplo de uso con Moby Dick
# book_id = 1342
# with open("libro_limpio.json", "r") as f:
#   cleaned_text = json.load(f)["text"]
#   propositions = extract_nv_propositions(cleaned_text)
# 
# # Guardar las proposiciones en un archivo JSON
# with open(f"nv_propositions_frecuencia_verbos_{book_id}.json", "w") as f:
#   json.dump(propositions, f, indent=4)
# 
# # Analizar las proposiciones
# analysis = analyze_propositions(propositions)
# print("Frecuencia de verbos:", analysis)

print("F-Extrae las proposiciones sujeto-verbo del texto limpio, salida impresa y en archiv json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import json
# from stanza.pipeline.core import Pipeline
# 
# def extract_subject_verb_propositions(text):
#   """
#   Extracts subject-verb propositions from the given text using Stanza.
# 
#   Args:
#     text: The text to be processed.
# 
#   Returns:
#     A list of dictionaries, where each dictionary represents a proposition
#     containing the subject and verb.
#   """
# 
#   nlp = Pipeline(lang='en', processors='tokenize,pos,depparse,lemma')  # Include 'depparse'
#   doc = nlp(text)
#   propositions = []
# 
#   for sent in doc.sentences:
#     for word in sent.words:
#       if word.upos == 'VERB':  # Check if the word is a verb
#         subject = None
#         # Check if word.deps is not None before iterating
#         if word.deps is not None:
#           for dep in word.deps: # Change to word.deps
#             if dep[1] == 'nsubj' or dep[1] == 'nsubj:pass':  # Find the subject, comparing strings directly
#               subject_index = dep[0] - 1  # Adjust index for zero-based indexing
#               subject = sent.words[subject_index].text
#               break
#         if subject:
#           propositions.append({'subject': subject, 'verb': word.text})
# 
#   return propositions
# 
# # Load the text from libro_limpio.json
# with open('libro_limpio.json', 'r', encoding='utf-8') as f:
#   data = json.load(f)
#   text = data['text']  # Assuming the text is stored in a 'text' key
# 
# # Extract subject-verb propositions
# propositions = extract_subject_verb_propositions(text)
# 
# # Print the extracted propositions
# for prop in propositions:
#   print(f"Subject: {prop['subject']}, Verb: {prop['verb']}")
# 
# # Save the propositions to a JSON file
# with open('propositions.json', 'w', encoding='utf-8') as f:
#   json.dump(propositions, f, indent=4, ensure_ascii=False)
# 
# print("Propositions saved to propositions.json")

print("G-Extraccion de palabras, etiquetas y lemas, salida en archivo impreso y json ")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import json
# 
# def extract_and_write_propositions(book_id, cleaned_text):
#   """
#   Extracts subject-verb propositions and writes them to a JSON file.
# 
#   Args:
#     book_id: The ID of the book.
#     cleaned_text: The cleaned text of the book.
#   """
#   # Utilize Stanza or another library to extract subject-verb propositions (NV)
#   # (This part is not included in the provided code)
#   nvn_propositions = extract_nv_propositions(cleaned_text)  # Placeholder function
# 
#   # Write the propositions to a JSON file
#   with open(f"nv_propositions_{book_id}.json", "w") as f:
#     json.dump(nvn_propositions, f, indent=4)
# 
#   print(f"NV propositions written to 'nv_propositions_{book_id}.json'")
# 
# # Load the cleaned text from libro_limpio.json
# with open("libro_limpio.json", "r") as f:
#   data = json.load(f)
#   cleaned_text = data["text"]  # Assuming the text is stored in a 'text' key
# 
# # Process the text using Stanza
# nlp = stanza.Pipeline('en')  # Load the Spanish pipeline
# doc = nlp(cleaned_text)
# 
# # Print word information for each word in each sentence
# for sent in doc.sentences:
#   for word in sent.words:
#     print(f"{word.text}\t{word.upos}\t{word.lemma}")

print("H Extraccion e identificacion los verbos en cada oración,busca la dependencia del sujeto (nsubj o nsubj:pass) del verbo., salida en archivo impreso y json ")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import json
# 
# def extract_nv_propositions(text):
#   """
#   Extracts subject-verb propositions (NV) from the given text using Stanza.
# 
#   Args:
#     text: The text to be processed.
# 
#   Returns:
#     A list of dictionaries, where each dictionary represents a proposition
#     containing the subject and verb.
#   """
# 
#   nlp = stanza.Pipeline('es')  # Load the Spanish pipeline
#   doc = nlp(text)
# 
#   for sent in doc.sentences:
#     for word in sent.words:
#       if word.upos == 'VERB':  # Check if the word is a verb
#         subject = None
#         # Iterate through word.deps instead of word.dependencies
#         for dep in word.deps:
#           if dep[1] == 'nsubj' or dep[1] == 'nsubj:pass':  # Find the subject
#             subject_index = dep[0] - 1  # Adjust index for zero-based indexing
#             subject = sent.words[subject_index].text
#             break
#         if subject:
#           [].append({'subject': subject, 'verb': word.text})
# 
#   return []
# 
# def process_and_save_propositions(book_id, cleaned_text):
#   """
#   Extracts subject-verb propositions and saves them to a JSON file.
# 
#   Args:
#     book_id: The ID of the book.
#     cleaned_text: The cleaned text of the book.
#   """
# 
#   propositions = extract_nv_propositions(cleaned_text)
# 
#   # Write the propositions to a JSON file
#   with open(f"nv_propositions_{book_id}.json", "w", encoding='utf-8') as f:
#     json.dump(propositions, f, indent=4)
# 
#   print(f"NV propositions written to 'nv_propositions_{book_id}.json'")
# 
# # Load the cleaned text from libro_limpio.json
# with open("libro_limpio.json", "r", encoding='utf-8') as f:
#   data = json.load(f)
#   cleaned_text = data["text"]  # Assuming the text is stored in a 'text' key
# 
# # Process the text and save propositions
# process_and_save_propositions("book1", cleaned_text)
# 
# # Print word information for each word in each sentence
# print("Word Information:")
# nlp = stanza.Pipeline('es')  # Load the Spanish pipeline
# doc = nlp(cleaned_text)
# for sent in doc.sentences:
#   for word in sent.words:
#     print(f"{word.text}\t{word.upos}\t{word.lemma}")

print("I-Cantidad de palabras del libro en archiv de salida")

palabras = text.split()
cantidad_palabras = len(palabras)
print(cantidad_palabras)  # Imprimirá

print("J-Frecuencia de las mil primeras palabras del libro, salida impresa en archivo json")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# import pandas as pd
# import json
# 
# def generate_word_frequency_and_cloud(book_data_file, word_frequency_file, cloud_image_file):
#   """
#   Calculates word frequencies from a JSON file (libro_limpio.json),
#   creates a word cloud visualization, and saves both the data and image.
# 
#   Args:
#     book_data_file: Path to the JSON file containing book text (e.g., "libro_limpio.json").
#     word_frequency_file: Path to save the word frequency JSON file (e.g., "word_frequencies_book.json").
#     cloud_image_file: Path to save the word cloud image (e.g., "word_cloud_book.png").
#   """
# 
#   # Load book text from JSON (assuming 'text' key stores the content)
#   with open(book_data_file, "r") as f:
#     book_text = json.load(f)["text"]
# 
#   # Preprocess text (optional): lowercase, remove punctuation, etc.
#   # (You might want to implement this based on your specific needs)
#   # processed_text = preprocess_text(book_text)
# 
#   # Count word frequencies
#   word_counts = {}
#   for word in book_text.split():
#     word_counts[word.lower()] = word_counts.get(word.lower(), 0) + 1
# 
#   # Sort words by frequency (descending order)
#   sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
# 
#   # Create a DataFrame for easier manipulation (optional)
#   df = pd.DataFrame.from_dict(dict(sorted_words), orient='index', columns=['frecuencia'])
# 
#   # Save word frequencies to JSON
#   with open(word_frequency_file, "w") as f:
#     json.dump(sorted_words, f, indent=4)
# 
#   # Create the word cloud visualization
#   wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)
#   plt.figure(figsize=(10, 7))
#   plt.imshow(wordcloud, interpolation='bilinear')
#   plt.axis("off")
# 
#   # Save word cloud image
#   plt.savefig(cloud_image_file)
#   plt.close()  # Close the plot to avoid memory issues
# 
#   # Print a summary of word frequencies (optional)
#   print("Top 1000 Most Frequent Words:")
#   for i in range(1000):
#     word, count = sorted_words[i]
#     print(f"\t- {word}: {count}")
# 
# # Example usage:
# book_data_file = "libro_limpio.json"  # Replace with your actual file path
# word_frequency_file = "word_frequencies_book.json"
# cloud_image_file = "word_cloud_book.png"
# generate_word_frequency_and_cloud(book_data_file, word_frequency_file, cloud_image_file)

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# import pandas as pd
# import json
# 
# def generate_word_cloud(book_data_file, cloud_image_file):
#   """
#   Genera una nube de palabras a partir de un archivo JSON y la guarda como una imagen.
# 
#   Args:
#     book_data_file: Ruta al archivo JSON con el texto del libro.
#     cloud_image_file: Ruta donde se guardará la imagen de la nube de palabras.
#   """
# 
#   # Cargar el texto del libro desde el archivo JSON
#   with open(book_data_file, "r") as f:
#     book_text = json.load(f)["text"]
# 
#   # Preprocesar el texto (opcional): convertir a minúsculas, eliminar puntuación, etc.
#   # processed_text = preprocess_text(book_text)
# 
#   # Contar la frecuencia de las palabras
#   word_counts = {}
#   for word in book_text.split():
#     word = word.lower()  # Convertir a minúsculas
#     word_counts[word] = word_counts.get(word, 0) + 1
# 
#   # Crear la nube de palabras
#   wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)
# 
#   # Mostrar y guardar la imagen
#   plt.figure(figsize=(10, 7))
#   plt.imshow(wordcloud, interpolation='bilinear')
#   plt.axis("off")
#   plt.savefig(cloud_image_file)
#   plt.show()  # Si quieres mostrar la imagen en pantalla
# 
# # Ejemplo de uso:
# book_data_file = "libro_limpio.json"
# cloud_image_file = "word_cloud_book.png"
# generate_word_cloud(book_data_file, cloud_image_file)

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# import pandas as pd
# import json
# import base64
# from io import BytesIO
# 
# def generate_word_cloud_and_save_to_json(book_data_file, cloud_json_file):
#   """
#   Generates a word cloud, saves it as an image, and then saves the image data
#   to a JSON file.
# 
#   Args:
#     book_data_file: Path to the JSON file containing book text.
#     cloud_json_file: Path to save the JSON file containing the word cloud image data.
#   """
# 
#   # ... (Rest of the code for generating the word cloud as before)
# 
#   # Save the word cloud image as a PNG in memory
#   img_buffer = BytesIO()
#   plt.savefig(img_buffer, format='png')
#   img_buffer.seek(0)
# 
#   # Encode the image data as base64 string
#   image_data = base64.b64encode(img_buffer.read()).decode('utf-8')
# 
#   # Create a dictionary to store the image data
#   cloud_data = {'image_data': image_data}
# 
#   # Save the image data to a JSON file
#   with open(cloud_json_file, 'w') as f:
#     json.dump(cloud_data, f)
# 
#   plt.close()  # Close the plot
# 
# # Example usage:
# book_data_file = "libro_limpio.json"
# cloud_json_file = "word_cloud_book.json"
# generate_word_cloud_and_save_to_json(book_data_file, cloud_json_file)

print("J-Frecuencia de entidades y grafico, en impresion  ")

import stanza
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

def process_with_stanza(cleaned_text, book_title="Moby Dick", entity_types=None):
    """
    Procesa un texto utilizando Stanza, realizando NER y generando un DataFrame con las entidades encontradas.

    Args:
        cleaned_text: El texto a procesar.
        book_title: El título del libro (opcional, para personalizar los resultados).
        entity_types: Una lista de tipos de entidades a filtrar (opcional).

    Returns:
        Un DataFrame de Pandas con las entidades encontradas, sus tipos y sus spans.
    """

    # Crear un pipeline de Stanza con el procesador NER
    nlp = stanza.Pipeline('en', processors='tokenize,ner')

    # Procesar el texto
    doc = nlp(cleaned_text)

    # Extraer las entidades
    entities = []
    for sent in doc.sentences:
        for ent in sent.ents:
            if entity_types is None or ent.type in entity_types:
                entities.append((ent.text, ent.type))

    # Crear un DataFrame de Pandas
    df = pd.DataFrame(entities, columns=['entity', 'type'])

    # Visualización
    def plot_entity_frequency(df):
        plt.figure(figsize=(12, 6))
        df['type'].value_counts().plot(kind='bar')
        plt.title(f"Frecuencia de entidades en {book_title}")
        plt.xlabel('Tipo de entidad')
        plt.ylabel('Frecuencia')
        plt.xticks(rotation=45)
        plt.show()

    def generate_entity_wordcloud(df):
        text = ' '.join(df['entity'])
        wordcloud = WordCloud(width=800, height=400).generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.show()

    plot_entity_frequency(df)
    generate_entity_wordcloud(df)

    return df

# Ejemplo de uso:
book_id = 1342
text = download_and_clean_gutenberg_book(book_id)
df_entities = process_with_stanza(text, entity_types=['PERSON', 'LOC'])

print("K")



print("L")

# Preparar los datos para el JSON
data_scatter = df[['longitud', 'frecuencia']].to_dict(orient='records')

# Combinar los datos de la nube de palabras y el gráfico de dispersión (si es necesario)
data = {
    "top_10_words": top_10.to_dict(orient='records'),
    "scatter_data": data_scatter
}

# Exportar los datos a un archivo JSON
with open("datos_visualizaciones.json", "w") as f:
    json.dump(data, f, indent=4)

print("Los datos de las visualizaciones se han exportado correctamente a datos_visualizaciones.json")
# 3. Gráfico de dispersión (opcional, si tienes datos de otra variable)
# Supongamos que tienes una columna adicional en el DataFrame con la longitud de cada palabra
df['longitud'] = df.index.map(len)
plt.scatter(df['longitud'], df['frecuencia'])
plt.xlabel('Longitud de la palabra')
plt.ylabel('Frecuencia')
plt.title('Relación entre Longitud y Frecuencia')
plt.show()

# Preparar los datos para el JSON
data_scatter = df[['longitud', 'frecuencia']].to_dict(orient='records')

# Combinar los datos de la nube de palabras y el gráfico de dispersión (si es necesario)
data = {
    "top_10_words": top_10.to_dict(orient='records'),
    "scatter_data": data_scatter
}

# Exportar los datos a un archivo JSON
with open("datos_visualizaciones.json", "w") as f:
    json.dump(data, f, indent=4)

print("Los datos de las visualizaciones se han exportado correctamente a datos_visualizaciones.json")

print("LL-Analisis de frecuencia, salida impresa y archivo json ")

import stanza
import requests
from bs4 import BeautifulSoup
import json
import nltk
from nltk.corpus import stopwords

def download_and_clean_gutenberg_book(book_id):
    """Descargamos un libro de Proyecto Gutenberg y lo limpiamos de manera más sofisticada."""
    url = f"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt"
    response = requests.get(url)
    response.encoding = 'utf-8'  # Set encoding to handle potential issues
    book_id = 1342
    if response.status_code == 200:
        text = response.text

        # Dividimos el texto en líneas
        lines = text.splitlines()

        # Identificamos la sección del cuerpo del libro (ejemplo simplificado)
        start_index = 0
        end_index = len(lines) - 1
        for i, line in enumerate(lines):
            if "*** START OF THIS PROJECT GUTENBERG EBOOK" in line.upper():  # Case-insensitive match
                start_index = i + 1
            if "*** END OF THIS PROJECT GUTENBERG EBOOK" in line.upper():  # Case-insensitive match
                end_index = i - 1
                break

        # Extraemos la sección del cuerpo
        cleaned_text = "\n".join(lines[start_index:end_index+1])

        return cleaned_text
    else:
        print(f"Error al descargar el libro {book_id}: {response.status_code}")
        return None  # or raise an exception


def process_with_stanza(cleaned_text, book_title="Moby Dick"):
    """
    Procesa un texto utilizando Stanza, realizando stemming y lematización, y generando un DataFrame con las palabras y sus frecuencias.

    Args:
        text: El texto a procesar.
        book_title: El título del libro (opcional, para personalizar los resultados).

    Returns:
        Un DataFrame de Pandas con las palabras, sus lemas, y sus frecuencias.
    """

    # Descargar los datos necesarios de NLTK
    nltk.download('punkt')
    nltk.download('wordnet')
    nltk.download('stopwords')

    # Crear un pipeline de Stanza
    nlp = stanza.Pipeline('en', processors='tokenize,lemma')

    # Procesar el texto
    doc = nlp(cleaned_text)

    # Obtener las palabras, lemas y frecuencias
    words = []
    lemmas = []
    for sent in doc.sentences:
        for word in sent.words:
            words.append(word.text)
            lemmas.append(word.lemma)

    # Eliminar stop words
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word, lemma in zip(words, lemmas) if word not in stop_words]
    filtered_lemmas = [lemma for lemma in lemmas if lemma not in stop_words]

    # Contar las frecuencias de los lemas
    from collections import Counter
    lemma_counts = Counter(filtered_lemmas)

    # Crear un DataFrame de Pandas
    import pandas as pd
    df = pd.DataFrame.from_dict(lemma_counts, orient='index', columns=['frecuencia'])
    df.index.name = 'lema'
    df = df.sort_values(by='frecuencia', ascending=False)

    print(f"Análisis de frecuencia de palabras para {book_title}")  # This line was improperly indented
    return df

df = process_with_stanza(text)
print(df.head(10))

import pandas as pd
import json

# Suponiendo que ya tienes un DataFrame llamado df



# Guardar el DataFrame completo en un archivo JSON
# Opción 1: Como una lista de diccionarios (cada fila es un diccionario)
df_json = df.to_dict('records')
with open('df.json', 'w') as f:
    json.dump(df_json, f, indent=4)

# Opción 2: Como un diccionario de listas (cada columna es una lista)
df_json = df.to_json(orient='records')
with open('df_records.json', 'w') as f:
    f.write(df_json)

print("MM-Analisis de la frecuencia de las palabras del libro en salida impresa y archivo json ")

import json

import nltk
from nltk.corpus import stopwords

def process_with_stanza(cleaned_text, book_title="Moby Dick"):
    """
    Procesa un texto utilizando Stanza, realizando stemming y lematización, y generando un DataFrame con las palabras y sus frecuencias.

    Args:
        text: El texto a procesar.
        book_title: El título del libro (opcional, para personalizar los resultados).

    Returns:
        Un DataFrame de Pandas con las palabras, sus lemas, y sus frecuencias.
    """

    # Descargar los datos necesarios de NLTK
    nltk.download('punkt')
    nltk.download('wordnet')
    nltk.download('stopwords')

    # Crear un pipeline de Stanza
    nlp = stanza.Pipeline('en', processors='tokenize,lemma')

    # Procesar el texto
    doc = nlp(cleaned_text)

    # Obtener las palabras, lemas y frecuencias
    words = []
    lemmas = []
    for sent in doc.sentences:
        for word in sent.words:
            words.append(word.text)
            lemmas.append(word.lemma)

    # Eliminar stop words
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word, lemma in zip(words, lemmas) if word not in stop_words]
    filtered_lemmas = [lemma for lemma in lemmas if lemma not in stop_words]

    # Contar las frecuencias de los lemas
    from collections import Counter
    lemma_counts = Counter(filtered_lemmas)

    # Crear un DataFrame de Pandas
    import pandas as pd
    df = pd.DataFrame.from_dict(lemma_counts, orient='index', columns=['frecuencia'])
    df.index.name = 'lema'
    df = df.sort_values(by='frecuencia', ascending=False)

    print(f"Análisis de frecuencia de palabras para {book_title}") #This line was improperly indented
    return df

# Ejemplo de uso:
book_id = 1342  # Moby Dick
text = download_and_clean_gutenberg_book(book_id)# Asumiendo que tienes una función para descargar libros de Gutenberg
df_moby_dick = process_with_stanza(text)
print(df_moby_dick.head(10))

# Convertir el DataFrame a un diccionario
df_moby_dick = df.to_dict('records')

# Escribir el diccionario a un archivo JSON
with open('moby_dick_word_freq.json', 'w') as outfile:
    json.dump(df_moby_dick, outfile, indent=4)

print("MM-Analisis morfosintactico del libro en trasnformers, con salida impresa y archivo json ")

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# import stanza
# import torch
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# import json
# 
# # Cargar el modelo de Stanza
# nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse,ner')
# 
# # Cargar el modelo de Transformers para análisis de sentimiento
# tokenizer_sentiment = AutoTokenizer.from_pretrained('bert-base-uncased')
# model_sentiment = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')
# 
# # The indentation for this function was incorrect
# def analizar_texto(text):
# 
#     ###Realiza un análisis lingüístico completo del texto utilizando Stanza y Transformers.
# 
#     ##Args:
#     ###   texto: El texto a analizar.
# 
#    ## Returns:
#    ##     Un diccionario con los resultados del análisis.
# 
#     doc = nlp(text)
#     resultados = []
# 
#     for oracion in doc.sentences:
#         # Análisis morfosintáctico
#         analisis_morfo = []
#         for palabra in oracion.words:
#             analisis_morfo.append({
#                 "texto": palabra.text,
#                 "pos": palabra.upos,
#                 "lemma": palabra.lemma,
#                 "dependencia": [dependencia for dependencia in oracion.dependencies if dependencia[1] == palabra.id]
#             })
# 
#         # Extracción de entidades nombradas
#         entidades = [ent.text for ent in oracion.ents]
# 
#         # Análisis de sentimiento
#         inputs = tokenizer_sentiment(oracion.text, return_tensors='pt')
#         outputs = model_sentiment(**inputs)
#         probabilidades = torch.softmax(outputs.logits, dim=1)
#         etiqueta_sentimiento = torch.argmax(probabilidades, dim=1).item()
#         sentimiento = {
#             "etiqueta": etiqueta_sentimiento,
#             "probabilidades": probabilidades.tolist()[0]
#         }
# 
#         resultado_oracion = {
#             "texto": oracion.text,
#             "analisis_morfo": analisis_morfo,
#             "entidades": entidades,
#             "sentimiento": sentimiento
#         }
#         resultados.append(resultado_oracion)
# 
#     return resultados
# 
# # Cargar el texto del libro desde el archivo JSON
# with open('libro_limpio.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#     text = data['text']
# 
# resultados_analisis = analizar_texto(text)
# 
# # Imprimir los resultados
# print("Análisis Lingüístico Completo:")
# for resultado in resultados_analisis:
#     print(f"Oración: {resultado['texto']}")
#     print(f"  Análisis Morfosintáctico: {resultado['analisis_morfo']}")
#     print(f"  Entidades Nombradas: {resultado['entidades']}")
#     print(f"  Sentimiento: {resultado['sentimiento']}")
#     print("-" * 30)
# 
# # Guardar los resultados en un archivo JSON
# with open("analisis_completo.json", "w") as archivo:
#     json.dump(resultados_analisis, archivo, indent=4)
# 
# print("Resultados guardados en analisis_completo.json")

print("ÑÑ-Analisis de entidades del libro en salida impresa y archivo json")

import stanza
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

def process_with_stanza(cleaned_text, book_title="Moby Dick"):
    # ... (código original)

    # Crear un DataFrame de Pandas

    # Visualización: Gráfico de barras
    entity_counts = pd.DataFrame(entities, columns=['entity', 'type'])['type'].value_counts()
    entity_counts.plot(kind='bar')
    plt.title(f'Distribución de entidades en {book_title}')
    plt.xlabel('Tipo de entidad')
    plt.ylabel('Frecuencia')
    plt.show()

    # Visualización: Nube de palabras
    text_for_wordcloud = ' '.join(pd.DataFrame(entities, columns=['entity', 'type'])['entity'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_for_wordcloud)
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()

    # Guardar resultados en JSON
    pd.DataFrame(entities, columns=['entity', 'type']).to_json('entities.json', orient='records')
    print("Resultados guardados en entities.json")

    return pd.DataFrame(entities, columns=['entity', 'type'])

import stanza

def process_with_stanza(cleaned_text, book_title="Moby Dick"):
    """
    Procesa un texto utilizando Stanza, realizando NER y generando un DataFrame con las entidades encontradas.

    Args:
        text: El texto a procesar.
        book_title: El título del libro (opcional, para personalizar los resultados).

    Returns:
        Un DataFrame de Pandas con las entidades encontradas, sus tipos y sus spans.
    """

    # Crear un pipeline de Stanza con el procesador NER
    nlp = stanza.Pipeline('en', processors='tokenize,ner')

    # Procesar el texto
    doc = nlp(cleaned_text)

    # Extraer las entidades
    entities = []
    for sent in doc.sentences:
        for ent in sent.ents:
            entities.append((ent.text, ent.type))

    # Crear un DataFrame de Pandas
    import pandas as pd
    df = pd.DataFrame(entities, columns=['entities', 'tipe'])
    return df

# Ejemplo de uso:
book_id = 1342  # Moby Dick
text = download_and_clean_gutenberg_book(book_id)
df_entities = process_with_stanza(text)
print(df_entities.head())

# Ejemplo de uso:
book_id = 1342  # Moby Dick
text = download_and_clean_gutenberg_book(book_id)  # Reemplazar con la función real
results = process_with_stanza(text)
book_title="Moby Dick"
# Guardar los resultados en un archivo JSON
# Use the book_title variable instead of results['book_title']
with open(f"{book_id}_{book_title}_entities.json", "w") as f:
    json.dump(results.to_dict(orient='records'), f, indent=4) # Convert DataFrame to dictionary before saving

print(f"Resultados guardados en {book_id}_{book_title}_entities.json")

print("OO")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import stanza
# 
# def process_with_stanza(text, book_title="Moby Dick"):
#     """
#     Procesa un texto utilizando Stanza, realizando NER y análisis de sentimientos.
# 
#     Args:
#         text: El texto a procesar.
#         book_title: El título del libro (opcional, para personalizar los resultados).
# 
#     Returns:
#         Un diccionario con las entidades encontradas, el sentimiento general y posiblemente otras estadísticas.
#     """
# 
#     # Crear un pipeline de Stanza con los procesadores NER y sentimiento
#     nlp = stanza.Pipeline('en', processors='tokenize,ner,sentiment')
# 
#     # Procesar el texto
#     doc = nlp(text)
# 
#     # Extraer las entidades y el sentimiento
#     entities = []
#     sentiments = []
#     for sent in doc.sentences:
#         entities.extend([(ent.text, ent.type) for ent in sent.ents])
#         sentiments.append(sent.sentiment)
# 
#     # Calcular el sentimiento promedio
#     average_sentiment = sum(sentiments) / len(sentiments)
# 
#     # Crear un diccionario con los resultados
#     results = {
#         'entities': entities,
#         'average_sentiment': average_sentiment
#     }
#     return results
# 
# # Ejemplo de uso:
# book_id = 1342  # Moby Dick
# text = download_and_clean_gutenberg_book(book_id)
# analysis = process_with_stanza(text)
# print(analysis)

# Ejemplo de uso:
book_id = 1342
text = download_and_clean_gutenberg_book(book_id)
analysis = process_with_stanza(text)

# Guardar los resultados en un archivo JSON
with open(f"analysis_{book_id}_{book_title}.json", "w") as f:
    json.dump(analysis, f, indent=4)

print("PP")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import stanza
# import pandas as pd
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# 
# def process_with_stanza(cleaned_text, book_title="Moby Dick", entity_types=None):
#     """
#     Procesa un texto utilizando Stanza, realizando NER y generando un DataFrame con las entidades encontradas.
# 
#     Args:
#         cleaned_text: El texto a procesar.
#         book_title: El título del libro (opcional, para personalizar los resultados).
#         entity_types: Una lista de tipos de entidades a filtrar (opcional).
# 
#     Returns:
#         Un DataFrame de Pandas con las entidades encontradas, sus tipos y sus spans.
#     """
# 
#     # Crear un pipeline de Stanza con el procesador NER
#     nlp = stanza.Pipeline('en', processors='tokenize,ner')
# 
#     # Procesar el texto
#     doc = nlp(cleaned_text)
# 
#     # Extraer las entidades
#     entities = []
#     for sent in doc.sentences:
#         for ent in sent.ents:
#             if entity_types is None or ent.type in entity_types:
#                 entities.append((ent.text, ent.type))
# 
#     # Crear un DataFrame de Pandas
#     df = pd.DataFrame(entities, columns=['entity', 'type'])
# 
#     # Visualización
#     def plot_entity_frequency(df):
#         plt.figure(figsize=(12, 6))
#         df['type'].value_counts().plot(kind='bar')
#         plt.title(f"Frecuencia de entidades en {book_title}")
#         plt.xlabel('Tipo de entidad')
#         plt.ylabel('Frecuencia')
#         plt.xticks(rotation=45)
#         plt.show()
# 
#     def generate_entity_wordcloud(df):
#         text = ' '.join(df['entity'])
#         wordcloud = WordCloud(width=800, height=400).generate(text)
#         plt.figure(figsize=(10, 5))
#         plt.imshow(wordcloud, interpolation='bilinear')
#         plt.axis("off")
#         plt.show()
# 
#     plot_entity_frequency(df)
#     generate_entity_wordcloud(df)
# 
#     return df
# 
# # Ejemplo de uso:
# book_id = 1342
# text = download_and_clean_gutenberg_book(book_id)
# df_entities = process_with_stanza(text, entity_types=['PERSON', 'LOC'])

# Ejemplo de uso:
book_id = 1342
book_title="Moby Dick"
text = download_and_clean_gutenberg_book(book_id)
analysis = process_with_stanza(text, entity_types=['PERSON', 'LOC'])

# Guardar los resultados en un archivo JSON (opcional)
# Use the book_title variable instead of results['book_title']
with open(f"analysis_{book_id}_{book_title}.json", "w") as f:
    json.dump(analysis.to_dict(orient='records'), f, indent=4) # Convert DataFrame to dictionary before saving

print(f"Resultados guardados en {book_id}_{book_title}_entities.json")

print("QQ")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import stanza
# import networkx as nx
# import matplotlib.pyplot as plt
# 
# def download_gutenberg_book(book_id):
#     """
#     Descargamos un libro de Proyecto Gutenberg y lo limpiamos.
# 
#     Args:
#         book_id: El ID del libro en Proyecto Gutenberg.
# 
#     Returns:
#         El texto del libro sin la cabecera y la licencia.
#     """
#     url = f"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt"
#     response = requests.get(url)
#     text = response.text
# 
#     # Eliminamos las primeras líneas (típicamente cabecera y licencia)
#     lines = text.splitlines()
#     while lines[0].startswith("*"):
#         lines.pop(0)
#     text = "\n".join(lines)
# 
#     return text
# 
# def process_with_stanza(text, book_title="Moby Dick"):
#     """
#     Procesa un texto utilizando Stanza, extrae relaciones de dependencia y crea un grafo.
# 
#     Args:
#         text: El texto a procesar.
#         book_title: El título del libro (opcional, para personalizar los resultados).
# 
#     Returns:
#         Un objeto de grafo de NetworkX que representa las relaciones de dependencia.
#     """
# 
#     # Crear un pipeline de Stanza con el procesador de análisis de dependencias
#     nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
#     doc = nlp(text)
# 
#     # Crear un grafo vacío
#     G = nx.Graph()
# 
#     # Agregar nodos y aristas al grafo
#     for sentence in doc.sentences:
#         for word in sentence.words:
#             G.add_node(word.text, pos=word.upos, lemma=word.lemma)
#             if word.head != 0:  # Si la palabra tiene una cabeza
#                 head_word = sentence.words[word.head - 1].text
#                 G.add_edge(head_word, word.text, label=word.deprel)
# 
#     return G
# 
# # Ejemplo de uso:
# book_id = 1342  # Moby Dick
# text = download_and_clean_gutenberg_book(book_id)
# graph = process_with_stanza(text)
# 
# # Visualizar el grafo (puede tomar tiempo para grafos grandes)
# plt.figure(figsize=(15, 10))
# pos = nx.spring_layout(graph, k=0.15)  # Algoritmo de posicionamiento de nodos (ajustar 'k' según sea necesario)
# nx.draw(graph, pos, with_labels=True, node_size=50, font_size=8)
# nx.draw_networkx_edge_labels(graph, pos, edge_labels=nx.get_edge_attributes(graph, 'label'), font_size=8)
# plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !pip install networkx
# 
# import networkx as nx
# import json
# 
# def graph_to_json(graph):
#     """Converts a NetworkX graph to a JSON-serializable dictionary."""
#     # Get node and edge data
#     nodes = list(graph.nodes(data=True))
#     edges = list(graph.edges(data=True))
# 
#     # Convert to dictionary format
#     json_data = {
#         "nodes": nodes,
#         "edges": edges
#     }
# 
#     return json_data
# 
# # Example usage
# # Assuming you have a graph object named 'graph'
# # graph = nx.spring_layout(graph, k=0.15)
# # The above line overwrites your graph object, instead access the layout
# # pos = nx.spring_layout(graph, k=0.15)  # This line was causing the error
# # The NetworkX graph is created in the 'process_with_stanza' function (ipython-input-63)
# # and stored in the variable 'graph', so we will call that function to get the graph
# book_id = 1342  # Moby Dick
# text = download_and_clean_gutenberg_book(book_id)  # Assuming this function exists
# graph = process_with_stanza(text)  # Get the graph object from the function
# pos = nx.spring_layout(graph, k=0.15)  # Calculate layout, but do not overwrite 'graph'
# json_data = graph_to_json(graph)  # Pass the original graph to 'graph_to_json'
# 
# # Save to JSON file
# with open("graph.json", "w") as f:
#     json.dump(json_data, f, indent=4)

print("RR")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import stanza
# import networkx as nx
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np
# 
# def download_gutenberg_book(book_id):
#     # ... (código para descargar el libro, como en el ejemplo anterior)
#     pass  # Placeholder for the download function
# 
# def process_with_stanza(cleaned_text, book_title="Moby Dick"):
#     # ... (código para procesar el texto y crear el grafo, como en el ejemplo anterior)
#     # Assuming you have the graph creation logic from previous examples
#     G = nx.Graph()  # Replace with your actual graph creation code
# 
#     # Asignar colores a los nodos según su POS
#     pos_colors = {'NOUN': 'blue', 'VERB': 'red', 'ADJ': 'green', 'ADV': 'purple'}
#     node_colors = [pos_colors.get(nx.get_node_attributes(G, 'pos')[n], 'gray') for n in G.nodes]
# 
#     # Variar el tamaño de los nodos según su grado
#     node_sizes = [d for n, d in G.degree()]
# 
#     # Calcular la frecuencia de cada tipo de relación y asignar el grosor de las aristas
#     edge_labels = nx.get_edge_attributes(G, 'label')
#     edge_counts = pd.Series(edge_labels).value_counts()
#     edge_colors = [edge_counts[label] for label in edge_labels.values()]
# 
#     # Crear la visualización
#     plt.figure(figsize=(15, 10))
#     pos = nx.spring_layout(G, k=0.15)
#     nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color=node_colors, font_size=8)
#     nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, 'label'), font_size=8)
#     plt.show()
# 
#     # Calcular métricas de centralidad
#     degree_centrality = nx.degree_centrality(G)
#     betweenness_centrality = nx.betweenness_centrality(G)
# 
#     # Crear un DataFrame con las métricas
#     centrality_df = pd.DataFrame({'degree': degree_centrality, 'betweenness': betweenness_centrality})
# 
#     return G, centrality_df
# 
# # Ejemplo de uso:
# book_id = 1342
# # Replace with your actual download and cleaning function:
# # cleaned_text = download_and_clean_gutenberg_book(book_id)
# 
# graph, centrality_df = process_with_stanza(cleaned_text)
# 
# # Guardar el grafo en formato GraphML
# nx.write_graphml(graph, "dependency_graph.graphml")
# 
# # Analizar las métricas de centralidad
# print(centrality_df.head())

# Ejemplo de uso
graph, centrality_df = process_with_stanza(cleaned_text)
json_data = graph_to_json(graph)

# Guardar en un archivo JSON
with open("dependency_graph.json", "w") as f:
  json.dump(json_data, f, indent=4)

print("SS")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import nltk
# nltk.download('punkt_tab')
# import nltk
# nltk.download('averaged_perceptron_tagger_eng')
# import stanza
# import nltk
# from nltk.tag import pos_tag
# from nltk.stem import WordNetLemmatizer
# from nltk.tokenize import word_tokenize
# import requests # Import the requests module
# book_id = 1342
# def download_and_clean_gutenberg_book(book_id):
#     """Descargamos un libro de Proyecto Gutenberg y lo limpiamos de manera más sofisticada."""
#     url = f"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt" # Fixed indentation here
#     response = requests.get(url) # Fixed indentation here
#     text = response.text # Fixed indentation here
# 
#     # Remove lines starting with asterisks (header, license, etc.)
#     lines = text.splitlines()
#     while lines[0].startswith("*"):
#         lines.pop(0)
#     cleaned_text = "\n".join(lines)
# 
#     return cleaned_text
# 
# # Stanza setup for CPU usage
# stanza.download('en')  # Download necessary models (one-time setup)
# nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma', use_gpu=False)
# 
# # Main processing
# book_id = int(input("Enter a Project Gutenberg book ID: "))  # Prompt user for book ID
# 
# cleaned_text = download_and_clean_gutenberg_book(book_id)
# 
# # Process with NLTK and Stanza
# words = word_tokenize(cleaned_text)
# pos_tags_nltk = pos_tag(words)  # NLTK POS tags
# 
# doc = nlp(cleaned_text)  # Stanza pipeline for tokens, POS tags, and lemmas
# 
# # Access words, POS tags, and lemmas using sentences
# stanza_words = []
# stanza_pos_tags = []
# stanza_lemmas = []
# 
# for sentence in doc.sentences:
#     for word in sentence.words:  # Iterate through words in each sentence
#         stanza_words.append(word.text)
#         stanza_pos_tags.append(word.xpos)
#         stanza_lemmas.append(word.lemma)
# 
# 
# # Print results
# print("NLTK Part-of-Speech Tags:")
# print(pos_tags_nltk)
# print("\nStanza Words:")
# print(stanza_words)
# print("\nStanza Part-of-Speech Tags:")
# print(stanza_pos_tags)
# print("\nStanza Lemmas:")
# print(stanza_lemmas)

print("-Extraccion de lista de tuplas, sus etiquetas POS, lista de palabras, lemas, en salida archivo impreso json ")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import json
# import stanza
# 
# def process_text(text):
#   """
#   Processes the given text using Stanza and NLTK, and generates a JSON output.
# 
#   Args:
#     text: The text to be processed.
# 
#   Returns:
#     str: JSON string containing word information from both libraries.
#   """
# 
#   # Process text with Stanza
#   nlp = stanza.Pipeline('en')  # Load the Spanish pipeline
#   doc = nlp(text)
# 
#   stanza_words = []
#   stanza_pos_tags = []
#   stanza_lemmas = []
# 
#   for sent in doc.sentences:
#     for word in sent.words:
#       stanza_words.append(word.text)
#       stanza_pos_tags.append(word.upos)
#       stanza_lemmas.append(word.lemma)
# 
#   # (You would need to integrate NLTK here for POS tagging)
#   # Example using NLTK (replace with your actual NLTK processing)
#   from nltk import word_tokenize, pos_tag
#   nltk_pos_tags = pos_tag(stanza_words)
# 
#   # Create JSON output
#   json_output = create_json_output(nltk_pos_tags, stanza_words, stanza_pos_tags, stanza_lemmas)
# 
#   return json_output
# 
# def create_json_output(nltk_pos_tags, stanza_words, stanza_pos_tags, stanza_lemmas):
#   """
#   Converts the given data into a JSON format.
# 
#   Args:
#     nltk_pos_tags: List of tuples containing words and their NLTK POS tags.
#     stanza_words: List of words from Stanza.
#     stanza_pos_tags: List of POS tags from Stanza.
#     stanza_lemmas: List of lemmas from Stanza.
# 
#   Returns:
#     str: JSON string representing the data.
#   """
# 
#   data = {
#       "nltk_pos_tags": nltk_pos_tags,
#       "stanza_words": stanza_words,
#       "stanza_pos_tags": stanza_pos_tags,
#       "stanza_lemmas": stanza_lemmas
#   }
# 
#   return json.dumps(data, indent=4)
# 
# # Load the cleaned text from libro_limpio.json
# with open("libro_limpio.json", "r", encoding='utf-8') as f:
#   data = json.load(f)
#   cleaned_text = data["text"]  # Assuming the text is stored in a 'text' key
# 
# # Process the text and generate JSON output
# json_output = process_text(cleaned_text)
# 
# # Print the JSON output
# print(json_output)
# 
# # Save the JSON output to a file
# with open('output.json', 'w') as f:
#   f.write(json_output)

print(" - Construccion de una matriz de confusion con las etiquetas del libro, con salida impresa y archivo json")

import nltk
nltk.download('punkt_tab')

import nltk
nltk.download('averaged_perceptron_tagger_eng')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import stanza
# import nltk
# from nltk.tag import pos_tag
# from nltk.stem import WordNetLemmatizer
# from nltk.tokenize import word_tokenize
# from sklearn.metrics import confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt
# 
# # ... (código para descargar Moby Dick y procesar con Stanza y NLTK)
# # Assuming 'text' contains the downloaded content of Moby Dick
# 
# # Stanza processing
# nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')
# doc = nlp(text)
# stanza_pos_tags = [word.upos for sent in doc.sentences for word in sent.words]
# 
# # NLTK processing
# words = word_tokenize(text)
# pos_tags = pos_tag(words)
# nltk_pos_tags = [tag for word, tag in pos_tags]
# 
# # Ensure both lists have the same length by truncating the longer one to match the shorter one
# min_len = min(len(stanza_pos_tags), len(nltk_pos_tags))
# stanza_pos_tags = stanza_pos_tags[:min_len]
# nltk_pos_tags = nltk_pos_tags[:min_len]
# 
# # Now create the confusion matrix
# confusion_mat = confusion_matrix(nltk_pos_tags, stanza_pos_tags)
# 
# # Calculate the accuracy
# accuracy = (confusion_mat.trace() / confusion_mat.sum()).round(2)
# print(f"Precisión de las etiquetas POS de Stanza: {accuracy}")
# 
# # Visualize the confusion matrix
# plt.figure(figsize=(10, 8))
# sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')
# plt.xlabel('Stanza')
# plt.ylabel('NLTK')
# plt.title('Matriz de Confusión para Etiquetas POS')
# plt.show()

import json
import numpy as np

def confusion_matrix_to_json(confusion_mat, stanza_labels, nltk_labels):
    """
    Converts a confusion matrix to JSON format.

    Args:
        confusion_mat: The confusion matrix as a NumPy array.
        stanza_labels: A list of labels for the Stanza model's predictions.
        nltk_labels: A list of labels for the NLTK model's predictions.

    Returns:
        A JSON-serializable dictionary representing the confusion matrix.
    """
    json_data = {
        "stanza_labels": stanza_labels,
        "nltk_labels": nltk_labels,
        "matrix": confusion_mat.tolist()
    }
    return json_data

# ... (previous code from ipython-input-74-0c2b7dd88bd5) ...

# Get unique labels from both stanza_pos_tags and nltk_pos_tags
stanza_labels = sorted(list(set(stanza_pos_tags)))
nltk_labels = sorted(list(set(nltk_pos_tags)))

# ... (rest of the code from ipython-input-74-0c2b7dd88bd5) ...

# Now, in ipython-input-75-0c2b7dd88bd5
json_data = confusion_matrix_to_json(confusion_mat, stanza_labels, nltk_labels)

# Guardar en un archivo JSON
with open("confusion_matrix.json", "w") as f:
  json.dump(json_data, f, indent=4)