{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/UNED/blob/main/input19_LSI_Ejemplo_Basico_Topic_Modeling_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmWk3K02QrCZ"
      },
      "source": [
        "# 19 - Latent Semantic Index/Analysis (LSI/A)\n",
        "\n",
        "\n",
        "* El LSI es una técnica de factorización matricial aplicable al campo del NLP, que tiene como objetivo analizar las relaciones entre los documentos de un corpus y las palabras.\n",
        "\n",
        "\n",
        "* El LSI asume que las palabras que tienen un significado similar, aparecerán en partes de documentos similares.\n",
        "\n",
        "\n",
        "* El LSI tiene como finalidad extraer una serie de factores latentes que caractericen a los documentos y las palabras del corpus.\n",
        "\n",
        "\n",
        "* Para ello necesita una matriz de frecuencias que represente el número de veces que aparecen las palabras en los documentos:\n",
        "\n",
        "    - ***Filas***: Cada fila representa a una palabra.\n",
        "    - ***Columnas***: Cada columna representa a un documento..\n",
        "    - ***Celda***: Cada celda representa el número de veces que aparece la palabra en el documento.\n",
        "    \n",
        "    \n",
        "* El LSI descompone una matriz “A” (de frecuencias); que esta formada por el numero de apariciones de cada palabra en cada documento, en tres matrices “U”, “S” y “V” cuyo producto matricial es igual a la matriz original “A”:\n",
        "\n",
        "\n",
        "$$ SVD(A) = U \\cdot S \\cdot V^{t}$$\n",
        "\n",
        "\n",
        "* Donde cada una de estas matrices contiene la siguiente información:\n",
        "\n",
        "    - ***Matriz “U”*** contiene los valores de los ***factores latentes de las palabras***.\n",
        "    - ***Matriz “V”*** contiene los valores de los ***factores latentes de los documentos***.\n",
        "    - ***Matriz “S”*** es una matriz en cuya diagonal están los llamados valores singulares que son decrecientes y no negativos. Esta matriz representa la ***importancia que tiene cada uno de los factores latentes de las palabras y de los documentos***.\n",
        "    \n",
        "    \n",
        "<img src=\"./imgs/015_LSI_Matrix.png\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "* Como la matriz “S” nos informa de la importancia que tiene cada uno de los factores latentes, podemos coger solo los ***K-factores más importantes*** para caracterizar cada una de las palabras y de los items.\n",
        "\n",
        "\n",
        "* De esta forma se trabaja con matrices más reducidas lo que nos permite comprimir la información de la matriz de apariciones.\n",
        "\n",
        "\n",
        "* El SVD tiene una propiedad muy importante que viene dada por el ***teorema de Eckart-Young***, que afirma que la mejor aproximación a la matriz “A” la obtenemos poniendo a ceros los “K” valores singulares de menor a mayor valor; es decir, reduciendo la matriz “S”. Por tanto si multiplicásemos las submatrices “U<sub>k</sub>”, “S<sub>k</sub>” y “V<sub>k</sub>” obtendríamos una matriz A’ de rango K que mejor aproxima (de acuerdo con la norma de Frobenius) a la matriz “A”.\n",
        "\n",
        "\n",
        "<img src=\"./imgs/016_LSI_Matrix_reduc.png\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "* El valor de K, va a representar el número de temas en que vamos a dividir (o Clusterizar) nuestro corpus, por lo tanto la selección del número de temas es un valor que tenemos que definir a priori.\n",
        "\n",
        "\n",
        "* Para poder trabajar en la extracción de tópicos y ver las relaciones entre los documentos y las palabras, vamos trabajar con las matrices:\n",
        "\n",
        "    - ***U<sub>k</sub>***: Estudiar las relaciones entre palabras\n",
        "    - ***V<sub>k</sub>***: Estudiar las relaciones entre documentos\n",
        "    - ***K***: Número de temas que tendrá a priori el Corpus\n",
        "    \n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "# Ejemplo de LSI con Gensim\n",
        "\n",
        "\n",
        "* Veamos a continuación un ejemplo sencillo sobre el siguiente Corpus del cual podemos ver que habla de 3 temas:\n",
        "    - Fútbol\n",
        "    - Política\n",
        "    - Economía\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "corpus1 = [\"Jane bought me these books.\",\n",
        "\"Jane bought a book for me.\",\n",
        "\"She dropped a line to him. Thank you.\",\n",
        "\"She sleeps\",\n",
        "\"I sleep a lot\",\n",
        "\"I was born in Madrid\",\n",
        "\"the cat was chased by the dog\",\n",
        "\"I was born in Madrid during 1995\",\n",
        "\"Out of all this , something good will come\",\n",
        "\"Susan left after the rehearsal. She did it well\",\n",
        "\"She sleeps during the morning, but she sleeps.\"]"
      ],
      "metadata": {
        "id": "gLnhIZLnSBkf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "iL9cvJo916Gh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkagjbAF2e2o",
        "outputId": "091e3287-5f50-4ca0-f4a0-c1d6287c4f36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "corpus1 = [\"Jane bought me these books.\",\n",
        "\"Jane bought a book for me.\",\n",
        "\"She dropped a line to him. Thank you.\",\n",
        "\"She sleeps\",\n",
        "\"I sleep a lot\",\n",
        "\"I was born in Madrid\",\n",
        "\"the cat was chased by the dog\",\n",
        "\"I was born in Madrid during 1995\",\n",
        "\"Out of all this , something good will come\",\n",
        "\"Susan left after the rehearsal. She did it well\",\n",
        "\"She sleeps during the morning, but she sleeps.\"]\n",
        "\n",
        "# Join the list of strings into a single string\n",
        "text = ' '.join(corpus1)\n",
        "\n",
        "# Tokenize the single string\n",
        "words = nltk.word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXliRalh3RVj",
        "outputId": "769eeeba-6895-4c63-9044-858857c80770"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jane', 'bought', 'me', 'these', 'books', '.', 'Jane', 'bought', 'a', 'book', 'for', 'me', '.', 'She', 'dropped', 'a', 'line', 'to', 'him', '.', 'Thank', 'you', '.', 'She', 'sleeps', 'I', 'sleep', 'a', 'lot', 'I', 'was', 'born', 'in', 'Madrid', 'the', 'cat', 'was', 'chased', 'by', 'the', 'dog', 'I', 'was', 'born', 'in', 'Madrid', 'during', '1995', 'Out', 'of', 'all', 'this', ',', 'something', 'good', 'will', 'come', 'Susan', 'left', 'after', 'the', 'rehearsal', '.', 'She', 'did', 'it', 'well', 'She', 'sleeps', 'during', 'the', 'morning', ',', 'but', 'she', 'sleeps', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos\n",
        "documents = [word_tokenize(doc) for doc in corpus1]  # Tokenize each document individually\n",
        "\n",
        "# Creamos el diccionario (vocabulario)\n",
        "frequency = defaultdict(int)\n",
        "for doc in documents:  # Iterate through tokenized documents\n",
        "    for token in doc:  # Iterate through words in each document\n",
        "        frequency[token] += 1\n",
        "\n",
        "dictionary = corpora.Dictionary(documents)  # Create dictionary based on tokenized documents\n",
        "print('Diccionario:')\n",
        "pprint(dictionary.token2id)\n",
        "\n",
        "\n",
        "# Creamos la Bolsa de Palabras\n",
        "corpus1 = [dictionary.doc2bow(doc) for doc in documents]  # Create bag-of-words representation\n",
        "print('\\nBolsa de Palabras:')\n",
        "pprint(corpus1)  # Print the bag-of-words representation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiPKK_NT30pw",
        "outputId": "33ff6a46-3543-48ef-8845-a29a035bfc92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario:\n",
            "{',': 31,\n",
            " '.': 0,\n",
            " '1995': 29,\n",
            " 'I': 17,\n",
            " 'Jane': 1,\n",
            " 'Madrid': 20,\n",
            " 'Out': 32,\n",
            " 'She': 9,\n",
            " 'Susan': 40,\n",
            " 'Thank': 10,\n",
            " 'a': 6,\n",
            " 'after': 41,\n",
            " 'all': 33,\n",
            " 'book': 7,\n",
            " 'books': 2,\n",
            " 'born': 21,\n",
            " 'bought': 3,\n",
            " 'but': 47,\n",
            " 'by': 24,\n",
            " 'cat': 25,\n",
            " 'chased': 26,\n",
            " 'come': 34,\n",
            " 'did': 42,\n",
            " 'dog': 27,\n",
            " 'dropped': 11,\n",
            " 'during': 30,\n",
            " 'for': 8,\n",
            " 'good': 35,\n",
            " 'him': 12,\n",
            " 'in': 22,\n",
            " 'it': 43,\n",
            " 'left': 44,\n",
            " 'line': 13,\n",
            " 'lot': 18,\n",
            " 'me': 4,\n",
            " 'morning': 48,\n",
            " 'of': 36,\n",
            " 'rehearsal': 45,\n",
            " 'she': 49,\n",
            " 'sleep': 19,\n",
            " 'sleeps': 16,\n",
            " 'something': 37,\n",
            " 'the': 28,\n",
            " 'these': 5,\n",
            " 'this': 38,\n",
            " 'to': 14,\n",
            " 'was': 23,\n",
            " 'well': 46,\n",
            " 'will': 39,\n",
            " 'you': 15}\n",
            "\n",
            "Bolsa de Palabras:\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
            " [(0, 1), (1, 1), (3, 1), (4, 1), (6, 1), (7, 1), (8, 1)],\n",
            " [(0, 2), (6, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
            " [(9, 1), (16, 1)],\n",
            " [(6, 1), (17, 1), (18, 1), (19, 1)],\n",
            " [(17, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
            " [(23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2)],\n",
            " [(17, 1), (20, 1), (21, 1), (22, 1), (23, 1), (29, 1), (30, 1)],\n",
            " [(31, 1),\n",
            "  (32, 1),\n",
            "  (33, 1),\n",
            "  (34, 1),\n",
            "  (35, 1),\n",
            "  (36, 1),\n",
            "  (37, 1),\n",
            "  (38, 1),\n",
            "  (39, 1)],\n",
            " [(0, 1),\n",
            "  (9, 1),\n",
            "  (28, 1),\n",
            "  (40, 1),\n",
            "  (41, 1),\n",
            "  (42, 1),\n",
            "  (43, 1),\n",
            "  (44, 1),\n",
            "  (45, 1),\n",
            "  (46, 1)],\n",
            " [(0, 1),\n",
            "  (9, 1),\n",
            "  (16, 2),\n",
            "  (28, 1),\n",
            "  (30, 1),\n",
            "  (31, 1),\n",
            "  (47, 1),\n",
            "  (48, 1),\n",
            "  (49, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clb9ZKJgQrCe"
      },
      "source": [
        "## Creamos el Diccionario y la Matriz (Bolsa de Palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uLlwmC7RQrCd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "corpus = [\"balon balon balon futbol futbol liga liga liga ronaldo ronaldo ronaldo ronaldo ronaldo messi\",\n",
        "          \"futbol futbol futbol futbol futbol ronaldo ronaldo ronaldo ronaldo messi messi\",\n",
        "          \"balon balon futbol futbol futbol futbol futbol futbol futbol messi messi messi messi messi\",\n",
        "          \"politica politica politica politica pp pp pp pp pp pp rajoy rajoy rajoy rajoy rajoy\",\n",
        "          \"politica politica politica politica pp pp pp psoe psoe psoe psoe zapatero zapatero zapatero rajoy\",\n",
        "          \"politica politica politica politica psoe psoe psoe psoe psoe psoe zapatero zapatero zapatero zapatero zapatero \",\n",
        "          \"dinero fmi fmi fmi fmi fmi ue ue ue ue pib pib pib ibex ibex\",\n",
        "          \"zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue ue pib\",\n",
        "          \"pp psoe zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue \",\n",
        "          \"futbol politica pib\",\n",
        "          \"futbol zapatero liga rajoy\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t--wtlgOQrCe",
        "outputId": "dbdbdf8f-1684-4d6c-9855-93051f0c3a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario:\n",
            "{'balon': 0,\n",
            " 'dinero': 10,\n",
            " 'fmi': 11,\n",
            " 'futbol': 1,\n",
            " 'ibex': 12,\n",
            " 'liga': 2,\n",
            " 'messi': 3,\n",
            " 'pib': 13,\n",
            " 'politica': 5,\n",
            " 'pp': 6,\n",
            " 'psoe': 8,\n",
            " 'rajoy': 7,\n",
            " 'ronaldo': 4,\n",
            " 'ue': 14,\n",
            " 'zapatero': 9}\n",
            "\n",
            "Bolsa de Palabras:\n",
            "[[(0, 3), (1, 2), (2, 3), (3, 1), (4, 5)],\n",
            " [(1, 5), (3, 2), (4, 4)],\n",
            " [(0, 2), (1, 7), (3, 5)],\n",
            " [(5, 4), (6, 6), (7, 5)],\n",
            " [(5, 4), (6, 3), (7, 1), (8, 4), (9, 3)],\n",
            " [(5, 4), (8, 6), (9, 5)],\n",
            " [(10, 1), (11, 5), (12, 2), (13, 3), (14, 4)],\n",
            " [(7, 1), (9, 1), (10, 4), (11, 4), (13, 1), (14, 4)],\n",
            " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 4), (11, 4), (14, 3)],\n",
            " [(1, 1), (5, 1), (13, 1)],\n",
            " [(1, 1), (2, 1), (7, 1), (9, 1)]]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "\n",
        "# Tokenizamos\n",
        "documents = [word.split() for word in corpus]\n",
        "\n",
        "# Creamos el diccionario (vocabulario)\n",
        "frequency = defaultdict(int)\n",
        "for doc in documents:\n",
        "    for token in doc:\n",
        "        frequency[token] += 1\n",
        "\n",
        "documents = [[token for token in doc] for doc in documents]\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "print('Diccionario:')\n",
        "pprint(dictionary.token2id)\n",
        "\n",
        "\n",
        "# Creamos la Bolsa de Palabras\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "print('\\nBolsa de Palabras:')\n",
        "pprint(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LiY6YUXQrCg"
      },
      "source": [
        "## Creamos el Modelo:\n",
        "\n",
        "* Gensim tiene implementado el LSI en la clase ***LsiModel***: https://radimrehurek.com/gensim/models/lsimodel.html\n",
        "\n",
        "\n",
        "* Como parámetros relevantes necesita:\n",
        "    1. Corpus\n",
        "    2. Número de Topics\n",
        "    3. Diccionario o Vocabulario del Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xo35znxNQrCg"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LsiModel\n",
        "\n",
        "lsi_model = LsiModel(corpus=corpus, num_topics=3, id2word=dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LsiModel\n",
        "\n",
        "lsi_model1 = LsiModel(corpus=corpus1, num_topics=3, id2word=dictionary)"
      ],
      "metadata": {
        "id": "Um6mPiDeVcGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n-A8rfdQrCg"
      },
      "source": [
        "### Matriz U\n",
        "\n",
        "\n",
        "* En esta matriz obtenemos los ***factores latentes de cada una de las palabras del corpus***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_matrix1 = lsi_model1.projection.u\n",
        "u_matrix1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "y0-GE6BHVk0y",
        "outputId": "32238a70-230e-41b7-e2bf-f67bafcd9fe6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lsi_model1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c507d36ffa1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu_matrix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsi_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mu_matrix1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lsi_model1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "2UNOT1CZQrCh",
        "outputId": "b8ac0a67-d3f7-4509-9bb1-672b25533dcc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lsi_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c275a2cdb9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mu_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lsi_model' is not defined"
          ]
        }
      ],
      "source": [
        "u_matrix = lsi_model.projection.u\n",
        "u_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiGww0cPQrCh"
      },
      "source": [
        "* Para ver los factores de cada una de las palabras lo vamos a mostrar de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(u_matrix1, columns=['Topic 1', 'Topic 2', 'Topic 3'], index=dictionary.token2id.keys()).head(15)"
      ],
      "metadata": {
        "id": "GhPVX9nDVsCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2UhaoR6QrCh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(u_matrix, columns=['Topic 1', 'Topic 2', 'Topic 3'], index=dictionary.token2id.keys()).head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ0HPAPCQrCi"
      },
      "source": [
        "### Matriz S\n",
        "\n",
        "\n",
        "* En esta matriz obtenemos la importancia de los ***factores latentes***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_matrix1 = lsi_model1.projection.s\n",
        "s_matrix1"
      ],
      "metadata": {
        "id": "XRQn7m1PVwj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ItK5oKQrCi"
      },
      "outputs": [],
      "source": [
        "s_matrix = lsi_model.projection.s\n",
        "s_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BJFlYEbQrCj"
      },
      "source": [
        "### Matriz V\n",
        "\n",
        "\n",
        "* En esta matriz obtenemos los ***factores latentes de cada uno de los documentos del corpus***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.matutils import corpus2dense\n",
        "v_matrix1 = corpus2dense(lsi_model1[corpus1], len(lsi_model1.projection.s)).T / lsi_model1.projection.s\n",
        "v_matrix1"
      ],
      "metadata": {
        "id": "T7JvBtjFV1d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vro-wcv5QrCj"
      },
      "outputs": [],
      "source": [
        "from gensim.matutils import corpus2dense\n",
        "v_matrix = corpus2dense(lsi_model[corpus], len(lsi_model.projection.s)).T / lsi_model.projection.s\n",
        "v_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEQ2zygRQrCj"
      },
      "source": [
        "* Para ver los factores de cada uno de los documentos lo vamos a mostrar de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP1oeY3UQrCj"
      },
      "outputs": [],
      "source": [
        "index = ['Doc {}'.format(i+1) for i,doc in enumerate(documents)]\n",
        "pd.DataFrame(v_matrix, index=index, columns=['Topic 1', 'Topic 2', 'Topic 3']).head(11)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = ['Doc {}'.format(i+1) for i,doc in enumerate(documents)]\n",
        "pd.DataFrame(v_matrix1, index=index, columns=['Topic 1', 'Topic 2', 'Topic 3']).head(11)"
      ],
      "metadata": {
        "id": "LjV2TR6hV8Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTH3dHhQrCk"
      },
      "source": [
        "## Conclusiones:\n",
        "\n",
        "* Tras mostrar los valores de los factores latentes de las palabras y de los documentos, podemos ver como el valor de los factores guarda cierta relación con el tema del que trata el documento.\n",
        "\n",
        "\n",
        "* Se puede observar como los grupos de documentos (1,2,3), (4,5,6) y (7,8,9) que tratan de temas distintos tienen unos valores de factores latentes diferenciables entre sí (sobre todo los factores 2 y 3).\n",
        "\n",
        "\n",
        "* De la misma manera las palabras que son carácterísticas de cada tema tienen también unos valores en los factores latentes similares entre si y entre los valores de los factores latentes de los documentos a los que pertenecen.\n",
        "\n",
        "\n",
        "* Si representamos en un espacio de dos dimensiones los factores latentes de los documentos y de las palabras (factores 2 y 3) podemos ver no solo que ***los documentos y las palabras de los mismos temas comparte un mismo \"espacio latente\"***, si no también que los documentos y las palabras del mismo tema están muy próximas entre sí.\n",
        "\n",
        "\n",
        "* ***NOTA***: *Al tratarse de un ejemplo didáctico podemos hacer ciertas suposiciones y representar y mostrar ciertos concepto en un espacio reducido; en este caso en 2 dimensiones. En un caso real con muchos documentos, muchas palabras y muchos temas (topics) sera casi imposible visualizar este tipo de cosas con el LSI*\n",
        "\n",
        "\n",
        "### Representación visual de los documentos en 2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.DataFrame({'f2': v_matrix1[:,2], 'f3': v_matrix1[:,1],\n",
        "                   'group': ['Doc {}'.format(i+1) for i,doc in enumerate(documents)]})\n",
        "\n",
        "new_var = sns.lmplot(x='f2', y='f3', data=df, fit_reg=False, height=6, aspect=2)\n",
        "\n",
        "new_var\n",
        "\n",
        "plt.title('Representación de los documentos (Factores 2 y 3)')\n",
        "plt.xlabel('Factor 2')\n",
        "plt.ylabel('Factor 3')\n",
        "\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(point['val']))\n",
        "\n",
        "label_point(df.f2, df.f3, df.group, plt.gca())"
      ],
      "metadata": {
        "id": "lGByw78ZWDBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps0zvbAsQrCk"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.DataFrame({'f2': v_matrix[:,2], 'f3': v_matrix[:,1],\n",
        "                   'group': ['Doc {}'.format(i+1) for i,doc in enumerate(documents)]})\n",
        "\n",
        "new_var = sns.lmplot(x='f2', y='f3', data=df, fit_reg=False, height=6, aspect=2)\n",
        "\n",
        "new_var\n",
        "\n",
        "plt.title('Representación de los documentos (Factores 2 y 3)')\n",
        "plt.xlabel('Factor 2')\n",
        "plt.ylabel('Factor 3')\n",
        "\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(point['val']))\n",
        "\n",
        "label_point(df.f2, df.f3, df.group, plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHTbdOuUQrCk"
      },
      "source": [
        "### Representación visual de las Palabras en 2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'f2': u_matrix1[:,2], 'f3': u_matrix1[:,1],\n",
        "                   'group': [word for word in dictionary.token2id.keys()]})\n",
        "\n",
        "sns.lmplot(x='f2', y='f3', data=df, fit_reg=False, height=6, aspect=2)\n",
        "\n",
        "plt.title('Representación de las palabras (Factores 2 y 3)')\n",
        "plt.xlabel('Factor 2')\n",
        "plt.ylabel('Factor 3')\n",
        "\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(point['val']))\n",
        "\n",
        "label_point(df.f2, df.f3, df.group, plt.gca())"
      ],
      "metadata": {
        "id": "60XzhyHrWJTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoFej73nQrCl"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'f2': u_matrix[:,2], 'f3': u_matrix[:,1],\n",
        "                   'group': [word for word in dictionary.token2id.keys()]})\n",
        "\n",
        "sns.lmplot(x='f2', y='f3', data=df, fit_reg=False, height=6, aspect=2)\n",
        "\n",
        "plt.title('Representación de las palabras (Factores 2 y 3)')\n",
        "plt.xlabel('Factor 2')\n",
        "plt.ylabel('Factor 3')\n",
        "\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(point['val']))\n",
        "\n",
        "label_point(df.f2, df.f3, df.group, plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu0iqd9gQrCl"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Topics & Terms\n",
        "\n",
        "\n",
        "* Cada uno de los temas (***Topics***) esta definido por una importancia (o peso) que tienen las palabras (***Terms***) del corpus en el tema.\n",
        "\n",
        "\n",
        "* Por eso cada tema tiene unas palabras relevantes que en función de si aparecen o no esas palabra en el documentos hacen que este pertenezca más a un tema que a otro.\n",
        "\n",
        "\n",
        "* Gensim nos devuelve un \"formula\" por tema (Topic) que aplicada a las apariciones de las palabras en los documentos nos indica el grado de pertenencia del nuevo documento a ese tema. El que mayor valor tenga tras aplicar la fórmula del tema al documento significará que tiene mayor propensión a pertenecer a ese tema."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsi_model1.print_topics()"
      ],
      "metadata": {
        "id": "5p_oXWwuWPhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJJyDh9PQrCl"
      },
      "outputs": [],
      "source": [
        "lsi_model.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIsMgVGvQrCm"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "# Topics de nuevos documentos\n",
        "\n",
        "\n",
        "* Al tratarse el Topic Modeling de un aprendizaje no supervisado (básicamente es un clustering de documentos), tenemos que ser capaces de poder asignar a un nuevo documento el tema al que pertenece.\n",
        "\n",
        "\n",
        "* Para ello tenemos que:\n",
        "    1. ***Obtener los factores latentes del nuevo documento*** en función del vector de apariciones de palabras\n",
        "    2. Calcular la ***similaridad con el resto de documentos*** en función de sus factores latentes\n",
        "    \n",
        "    \n",
        "* Para obtener los factores latentes de un nuevo documento lo podemos hacer con la técnica del \"***Folding-In***\" que básicamente consiste en el siguiente cálculo teniendo las matrices U<sub>k</sub> y S:\n",
        "\n",
        "\n",
        "$$ Factores Latentes_{new doc} = Vector Palabras_{new doc} \\cdot U_{k} \\cdot S^{-1}$$\n",
        "\n",
        "\n",
        "* Gensim ya tiene implementada esta funcionalidad y también la del cálculo de similaridades entre documentos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1a_UGRmQrCm"
      },
      "outputs": [],
      "source": [
        "new_doc = \"futbol futbol messi zapatero\"\n",
        "new_vec = dictionary.doc2bow(new_doc.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhP1PyzvQrCm"
      },
      "outputs": [],
      "source": [
        "# Obtenemos los factores latentes del nuevo documento\n",
        "vec_lsi = lsi_model[new_vec]\n",
        "\n",
        "pd.DataFrame(vec_lsi, index=['Topic 1', 'Topic 2', 'Topic 3'], columns=['', 'Valor']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DF6CM2qQrCm"
      },
      "source": [
        "* Ahora pasamos a calcular las similaridades entre documentos y podemos observar como este nuevo documento tiene mayor similaridad con los documentos 1, 2 y 3 que hablan sobre fútbol."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import similarities\n",
        "\n",
        "# Calculamos las similaridades\n",
        "index = similarities.MatrixSimilarity(lsi_model1[corpus1])\n",
        "sims = index[vec_lsi]\n",
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "\n",
        "pd.DataFrame(sims, columns=['Documento (indice)', 'Similaridad']).head(11)"
      ],
      "metadata": {
        "id": "WRbt1m0UWUsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc11y6MlQrCn"
      },
      "outputs": [],
      "source": [
        "from gensim import similarities\n",
        "\n",
        "# Calculamos las similaridades\n",
        "index = similarities.MatrixSimilarity(lsi_model[corpus])\n",
        "sims = index[vec_lsi]\n",
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "\n",
        "pd.DataFrame(sims, columns=['Documento (indice)', 'Similaridad']).head(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0_7S4eQrCn"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "# Bonus Track - LSI, Cálculo Analítico\n",
        "\n",
        "\n",
        "* Dada una matriz que representa una Bolsa de Palabras donde:\n",
        "    - Las filas representan a los documentos ('n' documentos)\n",
        "    - Las columnas representan a las palabras ('m' palabras)\n",
        "    \n",
        "    \n",
        "* Podemos descomponer esa matriz en tres matrices ***U***, ***S*** y ***V***, cuyo producto matricial es la matriz original ***A***.\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  & A &  &  & \\\\\n",
        " &  &  & _{nxm} &  &  & \\\\\n",
        " &  &  &  &  &  &\n",
        "\\end{bmatrix} =  \\begin{bmatrix}\n",
        " &  &  &  & \\\\\n",
        " &  &  &  & \\\\\n",
        " &  & U &  & \\\\\n",
        " &  & _{nxn} &  & \\\\\n",
        " &  &  &  &\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  & S &  &  & \\\\\n",
        " &  &  & _{nxm} &  &  & \\\\\n",
        " &  &  &  &  &  &\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  & V^{t} &  &  & \\\\\n",
        " &  &  & _{mxm} &  &  & \\\\\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  &  &  &  & \\\\\n",
        " &  &  &  &  &  &\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "\n",
        "* Cada una de estas matrices va a contener la siguiente información:\n",
        "\n",
        "    * ***Matriz U***: Contiene los valores de los ***factores latentes de las palabras***.\n",
        "    * ***Matriz V***: Contiene los valores de los ***factores latentes de los documentos***.\n",
        "    * ***Matriz S***: Matriz en cuya diagonal estan los llamados ***Valores singulares*** que tienen que ser valores decrecientes y no negativos. Los valores de la diagonal representan la importancia que tienen cada uno de los factores latentes de las palabras y de los documentos.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "# Cálculo Analítico: de las matrices U, S y V\n",
        "\n",
        "### Cálculo de la matriz U\n",
        "\n",
        "1. Se multiplica la matriz ***A*** por su traspuesta (***A·A <sup>T</sup>***) para obtener una matriz cuadrada de dimensión ***nxn***\n",
        "\n",
        "\n",
        "2. Se calculan los autovalores de la matriz cuadrada (***A<sub>1</sub> = A·A <sup>T</sup>***) a partir de su polinómio característico:\n",
        "\n",
        "$$det(A_{1}-\\lambda I) = 0$$\n",
        "\n",
        "\n",
        "3. Se calculan los autovectores asociados a cada autovalor (en orden decreciente por autovalor)\n",
        "\n",
        "\n",
        "### Cálculo de la matriz S\n",
        "\n",
        "\n",
        "1. Obtenidos los autovalores tras el cálculo de la matriz ***A***, se pone en la diagonal de la matriz ***S*** la raiz cuadrada del los autovalores en orden decreciente. El resto de elementos de la matriz ***S*** tendrán valor ***0***\n",
        "\n",
        "\n",
        "### Cálculo de la matriz V\n",
        "\n",
        "1. Se multiplica ***A <sup>T</sup>·A*** para obtener una matriz cuadrada de dimensión ***mxm***\n",
        "\n",
        "\n",
        "2. Se calculan los autovectores asociados a cada autovalor (ya calculados) en orden descendente.\n",
        "\n",
        "$$det(A_{2}-\\lambda I) = 0$$\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Ejemplo:\n",
        "\n",
        "\n",
        "* Supongamos que tenemos la siguiente matriz:\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "1 & 0 & 1\\\\\n",
        "2 & 3 & 0\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "### Cálculo de la matriz U:\n",
        "\n",
        "1. Calculamos ***A<sub>1</sub> = A·A <sup>T</sup>*** para obtener una matriz cuadrada de dimensión ***nxn***\n",
        "\n",
        "$$A_{1} = A \\cdot A' = \\begin{bmatrix}\n",
        "1 & 0 &1 \\\\\n",
        "2 & 3 & 0\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "1&2 \\\\\n",
        "0 & 3\\\\\n",
        "1 & 0\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "2 & 2\\\\\n",
        "2 & 13\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "2. Cálculo de los autovalores de la matriz cuadrada (***A<sub>1</sub> = A·A <sup>T</sup>***) a partir de su polinómio característico:\n",
        "<br><br><br>\n",
        "$$det(A_{1}-\\lambda I) = \\begin{bmatrix}\n",
        "2 - \\lambda & 2\\\\\n",
        "2 & 13 - \\lambda\n",
        "\\end{bmatrix} = (2  - \\lambda) \\cdot (13 - \\lambda) - 4 =  \\lambda^{2} - 15 \\lambda + 22$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ \\lambda = \\frac{15\\pm \\sqrt{15^2 - 4 \\cdot 1 \\cdot 22}}{2\\cdot1} $$\n",
        "<br>\n",
        "$$\\lambda_{1} = 13,35 $$\n",
        "<br>\n",
        "$$\\lambda_{2} = 1,65 $$\n",
        "\n",
        "\n",
        "3. Se calculan los autovectores asociados a cada autovalor (en orden decreciente por autovalor)\n",
        "\n",
        "  * Autovalor 1 = 13,35\n",
        "\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "2 & 2\\\\\n",
        "2 & 13\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "x\\\\y\n",
        "\\end{bmatrix} = 13,35 \\cdot \\begin{bmatrix}\n",
        "x\\\\y\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "2x + 2y = 13,35x\n",
        "\\\\\n",
        "2x + 13y = 13,35y\n",
        "\\end{matrix} $$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ -11,35x + 2y = 0; \\: si \\:  x = 1; \\:  y = \\frac{11,35}{2} = 5,675$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Normalizo \\: \\rightarrow \\sqrt{1^2  + 5,675^2} = 5,848$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Autovector (1) \\: = \\left [ \\frac{1}{5,848} \\:\\: \\frac{5,675}{5,848} \\right ] = \\left [ 0,17 \\:\\:\\: 0,98\\right ]$$\n",
        "\n",
        "   * Para el autovalor 2 realizo los mismos pasos y obtengo el siguiente resultado:\n",
        "   \n",
        "$$\\begin{bmatrix}\n",
        "2 & 2\\\\\n",
        "2 & 13\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "x\\\\y\n",
        "\\end{bmatrix} = 1,65 \\cdot \\begin{bmatrix}\n",
        "x\\\\y\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Autovector (2) \\: = \\left [ 0,98 \\:\\:\\: -0,17 \\right ]$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "* Por tanto la matriz U quedaría:\n",
        "\n",
        "$$U = \\begin{bmatrix}\n",
        "0,17 & 0,98\\\\\n",
        "0,98 & -0,17\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "### Cálculo de la matriz S\n",
        "\n",
        "\n",
        "1. Como ya se tienen calculados los autovalores obtenidos tras el cálculo de la matriz ***U***, nos es facil obtener la matriz ***S*** que tendrá en la diagonal la raiz cuadrada de los autovales colocados de manera decreciente:\n",
        "\n",
        "\n",
        "$$ S = \\begin{bmatrix}\n",
        "\\sqrt{13,35} & 0 & 0 \\\\\n",
        "0 & \\sqrt{1,65} & 0\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "3,65 & 0 & 0 \\\\\n",
        "0 & 1,28 & 0\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "\n",
        "### Cálculo de la matriz V\n",
        "\n",
        "1. Se multiplica ***A <sup>T</sup>·A*** para obtener una matriz cuadrada de dimensión ***mxm***\n",
        "\n",
        "$$A_{2} = A' \\cdot A = \\begin{bmatrix}\n",
        "1&2 \\\\\n",
        "0 & 3\\\\\n",
        "1 & 0\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "1 & 0 &1 \\\\\n",
        "2 & 3 & 0\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "5 & 6 & 1 \\\\\n",
        "6 & 9 & 0 \\\\\n",
        "1 & 0 & 1\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "2. Se calculan los autovectores asociados a cada autovalor (ya calculados) en orden descendente.\n",
        "\n",
        "    - Autovalores:\n",
        "<br>\n",
        "$$\\lambda_{1} = 13,35 $$\n",
        "<br>\n",
        "$$\\lambda_{2} = 1,65 $$\n",
        "\n",
        "* Calculamos los autovectores a partir de su polinomio característico (para estos cálculos no mostramos el procedimiento):\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "5 & 6 & 1 \\\\\n",
        "6 & 9 & 0 \\\\\n",
        "1 & 0 & 1\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix} = 13,35 \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Autovector (1) \\: = \\left [ 0,58 \\:\\:\\: 0,49 \\:\\: -0,64\\right ]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "5 & 6 & 1 \\\\\n",
        "6 & 9 & 0 \\\\\n",
        "1 & 0 & 1\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix} = 1,65 \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Autovector (2) \\: = \\left [ 0,81 \\:\\: -0,41 \\:\\: 0,43\\right ]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "5 & 6 & 1 \\\\\n",
        "6 & 9 & 0 \\\\\n",
        "1 & 0 & 1\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix} = 0 \\cdot \\begin{bmatrix}\n",
        "x\\\\y\\\\z\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Autovector (3) \\: = \\left [ 0,05 \\:\\: 0,77 \\:\\: 0,64\\right ]$$\n",
        "\n",
        "* Por tanto la matriz V quedaría:\n",
        "\n",
        "$$V = \\begin{bmatrix}\n",
        "0,58 & 0,49 & -0,64 \\\\\n",
        "0,81 & -0,41 & 0,43 \\\\\n",
        "0,05 & -0,77 & 0,64\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "## La Descomposición Matricial quedaría:\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "1 & 0 &1 \\\\\n",
        "2 & 3 & 0\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0,17 & 0,98\\\\\n",
        "0,98 & -0,17\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "3,65 & 0 & 0 \\\\\n",
        "0 & 1,28 & 0\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "0,58 & 0,81 & 0,05 \\\\\n",
        "0,49 & -0,41 & 0,77 \\\\\n",
        "-0,64 & 0,43 & 0,64\n",
        "\\end{bmatrix}$$"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}