{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/UNED/blob/main/Text_Classification_using_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "organizations_snap_amazon_fine_food_reviews_path = kagglehub.dataset_download('organizations/snap/amazon-fine-food-reviews')\n",
        "honnibal_spacyen_vectors_web_lg_path = kagglehub.dataset_download('honnibal/spacyen-vectors-web-lg')\n",
        "poonaml_reddit_vectors_for_sense2vec_spacy_path = kagglehub.dataset_download('poonaml/reddit-vectors-for-sense2vec-spacy')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Gl0UckrHwQgB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "0715c83044a6f612c86e5315b22c16c2d02bf173",
        "id": "fnHBUMy1wQgC"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "#### About Dataset:\n",
        "We will be using rich dataset of amazon fine food reviews.\n",
        "\n",
        "####  What are we trying to achieve??\n",
        "We are going to tackle an interesting natural language processing problem i.e sentiment or text classification.\n",
        "We will explore texual data using amazing spaCy library and build a text classification model.\n",
        "\n",
        "### Here is breakdown of concepts I will try to explain.\n",
        "We will extract linguistic features like\n",
        "1. tokenization,\n",
        "1. part-of-speech tagging,\n",
        "1. dependency parsing,\n",
        "1. lemmatization ,\n",
        "1. named entities recognition,\n",
        "1. Sentence Boundary Detection\n",
        "for building language models later.\n",
        "\n",
        "Visualizing Data\n",
        "1. explacy - explaining how parsing is done\n",
        "1. displaCy - visualizing named entities\n",
        "\n",
        "Word vectors and similarity\n",
        "1. sense2vec - using contextual information for building word embeddings\n",
        "\n",
        "Text classification model\n",
        "1. SpaCy TextCategorizer"
      ]
    },
    {
      "metadata": {
        "_uuid": "2551b57f3a848ae2f6e2951952922b8de35d370a",
        "id": "RwrengBewQgD"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9919e0bb871106a57320152b6159a79a361a92d3",
        "id": "JllQnD_RwQgD"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78b0ec0fe0ac689ae1f4b56a9a87aaa85a4ad28b",
        "id": "ZkTkhcTnwQgE"
      },
      "cell_type": "markdown",
      "source": [
        "Let's read in food reviews data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3249bd33a5451044bff7a4513dfe4baaef6c1efb",
        "id": "EBsqN2EEwQgE"
      },
      "cell_type": "code",
      "source": [
        "food_reviews_df=pd.read_csv('../input/amazon-fine-food-reviews/Reviews.csv')\n",
        "food_reviews_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5eab874d485e4b353f28d01d1450e5f35a3103f6",
        "id": "7fMEWqoTwQgE"
      },
      "cell_type": "code",
      "source": [
        "food_reviews_df.head().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cfa3619255af6836b75b9afa22856ba92021733e",
        "id": "ZXI8Ou0AwQgF"
      },
      "cell_type": "markdown",
      "source": [
        "Text column contains review given by customer."
      ]
    },
    {
      "metadata": {
        "_uuid": "f10a32c6f40f091c2ed376d2354f849ef5deaa22",
        "id": "U7S_61SPwQgF"
      },
      "cell_type": "markdown",
      "source": [
        "Let's focus on texual data and ratings for text classification."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2beca8fa08a7e5f57a185e81843064780a270938",
        "id": "mRJdyet5wQgF"
      },
      "cell_type": "code",
      "source": [
        "food_reviews_df = food_reviews_df[['Text','Score']].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "934b2d98b61f7acdadc3c95db2a1c966bd8d1188",
        "id": "gakJ3ZIdwQgF"
      },
      "cell_type": "code",
      "source": [
        "ax=food_reviews_df.Score.value_counts().plot(kind='bar')\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"score.png\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8a02ea9da92ee2fafbeb81e7c1d815d1038dd20",
        "id": "deEh-KqVwQgF"
      },
      "cell_type": "markdown",
      "source": [
        "We have five-star rating system.\n",
        "It looks like we have more reviews with ratings 5, this can lead to unbalanced classes. We will treat rating 4 and 5 as positive and rest as negative reviews."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c35968c0535457a171dc620298dbf4863d6f269",
        "_kg_hide-output": true,
        "id": "FHC5uQXNwQgF"
      },
      "cell_type": "code",
      "source": [
        "food_reviews_df.Score[food_reviews_df.Score<=3]=0\n",
        "food_reviews_df.Score[food_reviews_df.Score>=4]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a29d89ea63f0cc365fd57f58834f6f678940059",
        "id": "Cyxt_fZzwQgG"
      },
      "cell_type": "code",
      "source": [
        "ax=food_reviews_df.Score.value_counts().plot(kind='bar')\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"score_boolean.png\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21249c348db38d135c4d6d9a560fa829d009c430",
        "id": "VdgW_G18wQgG"
      },
      "cell_type": "code",
      "source": [
        "food_reviews_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3015c49982aac3b69f4e286760dc66c8656341cb",
        "id": "6SsU8g54wQgG"
      },
      "cell_type": "markdown",
      "source": [
        "Since we have huge data, since it might be difficult to train in kernel, I will reduce data size of 100K rows.\n",
        "To balance classes, i have selected equal samples from each class."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c90e5fb27a1a10707853d470c4354f321b2f479",
        "id": "miG-VZmjwQgG"
      },
      "cell_type": "code",
      "source": [
        "train_pos_df=food_reviews_df[food_reviews_df.Score==1][:50000]\n",
        "train_neg_df=food_reviews_df[food_reviews_df.Score==0][:50000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebb304fcb88339861e2dfe4c865b91e6870d2cea",
        "id": "OL-56v6PwQgG"
      },
      "cell_type": "code",
      "source": [
        "train_df=train_pos_df.append(train_neg_df)\n",
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b439885ac9288394481948ab400fce8a94593153",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "gXFc3wdpwQgG"
      },
      "cell_type": "code",
      "source": [
        "val_pos_df=food_reviews_df[food_reviews_df.Score==1][50000:60000]\n",
        "val_neg_df=food_reviews_df[food_reviews_df.Score==0][50000:60000]\n",
        "val_df=val_pos_df.append(val_neg_df)\n",
        "val_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "_uuid": "6b8aca580867d3767e3c3b24f55cc1db443f365b",
        "id": "SS96-90iwQgG"
      },
      "cell_type": "markdown",
      "source": [
        "### Linguistic features"
      ]
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "_uuid": "6b28ff4da665c5bde9a7a4b85d937993071f35c2",
        "id": "0e98zAQKwQgG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tokenization\n",
        "First step in any nlp pipeline is tokenizing text i.e breaking down paragraphs into sentenses and then sentenses into words, punctuations and so on.\n",
        "\n",
        "we will load english language model to tokenize our english text.\n",
        "\n",
        "Every language is different and have different rules. Spacy offers 8 different language models."
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "6c1fef3ada345d4ea8d4b8ceac51d317151a231f",
        "id": "6nKH-yXMwQgG"
      },
      "cell_type": "code",
      "source": [
        "spacy_tok = spacy.load('en_core_web_sm')\n",
        "sample_review=food_reviews_df.Text[54]\n",
        "sample_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "d34364d38de3125bc8f9473fa8a3bac2289e4fa5",
        "id": "sHnW5c_xwQgG"
      },
      "cell_type": "code",
      "source": [
        "parsed_review = spacy_tok(sample_review)\n",
        "parsed_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21ef54c7df2f3ff4c58184d6abc2a9d4dd2e0811",
        "id": "Z9b7FqZNwQgG"
      },
      "cell_type": "markdown",
      "source": [
        "There is not much difference between parsed review and original one. But we will see ahead what has actually happened.\n",
        "We can see how parsing has been done visually through **explacy**."
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "686b7fb23a5d33abc9080dc110df6ee113eff650",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "id": "xlqlTGHcwQgG"
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tylerneylon/explacy/master/explacy.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "8383d83c813120f39e6960530fbb353870391162",
        "id": "vdkxO6NDwQgG"
      },
      "cell_type": "code",
      "source": [
        "import explacy\n",
        "explacy.print_parse_info(spacy_tok, 'The salad was surprisingly tasty.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c0325c2c9e89878e668ea843c0694d13c04fe18",
        "id": "c1OQoUthwQgG"
      },
      "cell_type": "code",
      "source": [
        "explacy.print_parse_info(spacy_tok,food_reviews_df.Text[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "_uuid": "2797fa3eadbf5b553c80a4a269c69e9870a0cc25",
        "id": "cGw8WHO8wQgG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Part-of-speech tagging\n",
        "After tokenization we can parse and tag variety of parts of speech to paragraph text. SpaCy uses statistical models in background to predict which tag will go for each word(s) based on the context.\n",
        "\n",
        "##### Lemmatization\n",
        "It is the process of extracting uninflected/base form of the word.\n",
        "Lemma can be like\n",
        "For eg.\n",
        "\n",
        "Adjectives: best, better → good\n",
        "Adverbs: worse, worst → badly\n",
        "Nouns: ducks, children → duck, child\n",
        "Verbs: standing,stood → stand\n"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "f3afec00062f8b3395bf2369dae40a6eaf5400b5",
        "id": "60hHMmWkwQgH"
      },
      "cell_type": "code",
      "source": [
        "tokenized_text = pd.DataFrame()\n",
        "\n",
        "for i, token in enumerate(parsed_review):\n",
        "    tokenized_text.loc[i, 'text'] = token.text\n",
        "    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n",
        "    tokenized_text.loc[i, 'pos'] = token.pos_\n",
        "    tokenized_text.loc[i, 'tag'] = token.tag_\n",
        "    tokenized_text.loc[i, 'dep'] = token.dep_\n",
        "    tokenized_text.loc[i, 'shape'] = token.shape_\n",
        "    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n",
        "    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n",
        "    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n",
        "\n",
        "tokenized_text[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "_uuid": "d8bb42b294ecaf912cc5cc3749c7955dcafa02b3",
        "id": "DiwNHsKwwQgH"
      },
      "cell_type": "markdown",
      "source": [
        "#### Named Entity Recognition (NER)\n",
        "Named entity is real world object like Person, Organization etc\n",
        "\n",
        "Spacy figures out below entities automatically:\n",
        "\n",
        "|Type\t|Description|\n",
        "|------|------|\n",
        "|PERSON|\tPeople, including fictional.\n",
        "|NORP|\tNationalities or religious or political groups.|\n",
        "|FAC|\tBuildings, airports, highways, bridges, etc.|\n",
        "|ORG|\tCompanies, agencies, institutions, etc.|\n",
        "|GPE|\tCountries, cities, states.|\n",
        "|LOC|\tNon-GPE locations, mountain ranges, bodies of water.|\n",
        "|PRODUCT|\tObjects, vehicles, foods, etc. (Not services.)|\n",
        "|EVENT|\tNamed hurricanes, battles, wars, sports events, etc.|\n",
        "|WORK_OF_ART|\tTitles of books, songs, etc.|\n",
        "|LAW|\tNamed documents made into laws.|\n",
        "|LANGUAGE|\tAny named language.|\n",
        "|DATE|\tAbsolute or relative dates or periods.|\n",
        "|TIME|\tTimes smaller than a day.|\n",
        "|PERCENT|\tPercentage, including \"%\".|\n",
        "|MONEY|\tMonetary values, including unit.|\n",
        "|QUANTITY|\tMeasurements, as of weight or distance.|\n",
        "|ORDINAL|\t\"first\", \"second\", etc.|\n",
        "|CARDINAL|\tNumerals that do not fall under another type|"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "8a6f77fec8bd915e7499375822c253fc5742edf7",
        "id": "UpCCjkNOwQgH"
      },
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(parsed_review, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5de88086fb1412a1f38cc1af135d1cb35dd8a6d",
        "id": "uK2LMFLbwQgH"
      },
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE') # to explain POS tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "_uuid": "14d10f9d14b578add72f3894d419db4083e71f76",
        "id": "l7VhcZt7wQgI"
      },
      "cell_type": "markdown",
      "source": [
        "#### Dependency parsing\n",
        "Syntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it.\n",
        "As in Subject combined with object makes a sentence.\n",
        "Spacy provides parse tree which can be used to generate this structure.\n",
        "\n",
        "##### Sentense Boundry Detection\n",
        "Figuring out where sentense starts and ends is very imporatnt part of nlp."
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "4622f182416f31136c456c7da33d3d35c48f7454",
        "id": "zIw7e_xRwQgI"
      },
      "cell_type": "code",
      "source": [
        "sentence_spans = list(parsed_review.sents)\n",
        "sentence_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "trusted": true,
        "_uuid": "e46015628663109cd690c3a10b43c2342ee2fd4b",
        "id": "qTztdgM0wQgI"
      },
      "cell_type": "code",
      "source": [
        "displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "_uuid": "8a06fe267e43251696b85ab0d9360babac2ae452",
        "id": "WpimhcBuwQgI"
      },
      "cell_type": "markdown",
      "source": [
        "Kindly scroll down if you can't see the output above.\n",
        "You can even customize dependency parser's output as below."
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "370cb18088dcb6d7921b0cc0a0a99ef1791f465e",
        "id": "fCN3ZQ5YwQgJ"
      },
      "cell_type": "code",
      "source": [
        "options = {'compact': True, 'bg': 'violet','distance': 140,\n",
        "           'color': 'white', 'font': 'Trebuchet MS'}\n",
        "displacy.render(parsed_review, jupyter=True, style='dep', options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8bbf982767f2d6c7a8023014ef0e4846c3b0853",
        "id": "ksKcibnKwQgJ"
      },
      "cell_type": "code",
      "source": [
        "spacy.explain(\"ADJ\") ,spacy.explain(\"det\") ,spacy.explain(\"ADP\") ,spacy.explain(\"prep\")  # to understand tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "_uuid": "f44109e7d3bba0f4233db5e362ea230a14d62ca3",
        "id": "9VPKalQkwQgJ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Processing Noun chunks"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "8521bb6e4b35a635ddb07dd842064769af13b762",
        "id": "Dg9LWYoswQgK"
      },
      "cell_type": "code",
      "source": [
        "noun_chunks_df = pd.DataFrame()\n",
        "\n",
        "for i, chunk in enumerate(parsed_review.noun_chunks):\n",
        "    noun_chunks_df.loc[i, 'text'] = chunk.text\n",
        "    noun_chunks_df.loc[i, 'root'] = chunk.root,\n",
        "    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n",
        "    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n",
        "    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n",
        "\n",
        "noun_chunks_df[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36bdab6cdaa456cd06ea7ddc98a1515ed70e54ab",
        "id": "UWmqs3rNwQgK"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing using Scattertext"
      ]
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "d08938394af752f96b7ea23baf76d411ec1eb518",
        "_kg_hide-input": true,
        "id": "nDrf7WzCwQgK"
      },
      "cell_type": "code",
      "source": [
        "!pip install scattertext\n",
        "import scattertext as st\n",
        "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8513457e5fefc2752b49652e1c3305c67fc5f90",
        "id": "cqYWFNcAwQgK"
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\n",
        "train_df['parsed'] = train_df.Text[49500:50500].apply(nlp)\n",
        "corpus = st.CorpusFromParsedDocuments(train_df[49500:50500],\n",
        "                             category_col='Score',\n",
        "                             parsed_col='parsed').build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b45bbf1fe689c35173a29b5203f1099e9e2abec",
        "_kg_hide-output": true,
        "id": "9POrV2hQwQgL"
      },
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(corpus,\n",
        "          category=1,\n",
        "          category_name='Positive',\n",
        "          not_category_name='Negative',\n",
        "          width_in_pixels=700,\n",
        "          minimum_term_frequency=15,\n",
        "          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6adc5e405d629097c8f56d476b8975108d055c59",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "id": "yRG5t-kFwQgL"
      },
      "cell_type": "code",
      "source": [
        "# uncomment this cell to load the interactive scattertext visualisation\n",
        "filename = \"positive-vs-negative.html\"\n",
        "open(filename, 'wb').write(html.encode('utf-8'))\n",
        "IFrame(src=filename, width = 900, height=900)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05f9d0cd5eacd1a1c56da174203cb96e88fb00a7",
        "id": "fihTqAppwQgL"
      },
      "cell_type": "markdown",
      "source": [
        "### Word vectors and similarity"
      ]
    },
    {
      "metadata": {
        "_uuid": "a7df3d1f449bdb569766202ffe20890949409fc0",
        "id": "OeUctgB3wQgL"
      },
      "cell_type": "markdown",
      "source": [
        "Ok let's do some modelling and focus on scoring our food!!"
      ]
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "_uuid": "a88a94ba02c9b4e7f57f5f2159d402c042e88e6e",
        "id": "2JiVx44swQgL"
      },
      "cell_type": "markdown",
      "source": [
        "### Sence2vec\n",
        "\n",
        "The idea is get something better than word2vec model.\n",
        "\n",
        "The idea behind sense2vec is super simple. If the problem is that duck as in waterfowl and duck as in crouch are different concepts, the straight-forward solution is to just have two entries, duckN and duckV.  Trask et al (2015) published a nice set of experiments showing that the idea worked well.\n",
        "\n",
        "It assight parts of speech tags like verb, noun , adjective to words, which will in turn be used to make sence of context.\n",
        "1. Please book [VERB] my ticket.\n",
        "2. Read the book [NOUN].\n",
        "\n",
        "Read more [here](https://explosion.ai/blog/sense2vec-with-spacy) and [here](https://github.com/explosion/sense2vec)\n",
        "\n",
        "Reddit talks about food a lot so we can get nice similarity vectors for food items."
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "39c873570d123f3e96b71ad3506ac3b7e6a7d6ee",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "HY_L__cZwQgM"
      },
      "cell_type": "code",
      "source": [
        "!pip install sense2vec==1.0.0a0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "a18f79503c8bc8389baab0a41d550a782a9cdced",
        "id": "oasaiYZawQgM"
      },
      "cell_type": "code",
      "source": [
        "import sense2vec\n",
        "from sense2vec import Sense2VecComponent\n",
        "\n",
        "s2v = Sense2VecComponent('../input/reddit-vectors-for-sense2vec-spacy/reddit_vectors-1.1.0/reddit_vectors-1.1.0/')\n",
        "spacy_tok.add_pipe(s2v)\n",
        "doc = spacy_tok(u\"dessert.\")\n",
        "freq = doc[0]._.s2v_freq\n",
        "vector = doc[0]._.s2v_vec\n",
        "most_similar = doc[0]._.s2v_most_similar(5)\n",
        "most_similar,freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "0ab1d9e89fd7cae0cfc95f7b21ad90507a7e02e2",
        "id": "xjGSuFo_wQgM"
      },
      "cell_type": "code",
      "source": [
        "doc = spacy_tok(u\"burger\")\n",
        "most_similar = doc[0]._.s2v_most_similar(4)\n",
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "trusted": true,
        "_uuid": "7fb72a57656ef9e21930534e5bb9dbc956c77ad0",
        "id": "n0NkUcoQwQgM"
      },
      "cell_type": "code",
      "source": [
        "doc = spacy_tok(u\"peanut butter\")\n",
        "most_similar = doc[0]._.s2v_most_similar(4)\n",
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f114333faf40dbff7dcf9d4939de7f0e554141ce",
        "id": "NKMyOiNNwQgM"
      },
      "cell_type": "markdown",
      "source": [
        "Similarity between entities can be kind of fun.\n",
        "\n",
        "\n",
        "The following attributes are available via the ._ property – for example token._.in_s2v:\n",
        "\n",
        "Name\t|Attribute Type|\tType|\tDescription|\n",
        "--------|---------------|-------------|---------------|\n",
        "in_s2v\t|property|\tbool|\tWhether a key exists in the vector map.\n",
        "s2v_freq|\tproperty|\tint|\tThe frequency of the given key.\n",
        "s2v_vec|\tproperty|\tndarray[float32]|\tThe vector of the given key.\n",
        "s2v_most_similar|\tmethod|\tlist|\tGet the n most similar terms. Returns a list of ((word, sense), score) tuples.\n",
        "\n",
        "\n",
        "\n",
        "## SpaCy Text Categorizer\n",
        "\n",
        "We will train a multi-label convolutional neural network text classifier on our food reviews, using spaCy's new TextCategorizer  component.\n",
        "\n",
        "SpaCy provides classification model with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components. The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length nr_class, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present."
      ]
    },
    {
      "metadata": {
        "_uuid": "9ffce959d65f1761f9b31545ad82c35f509a4f54",
        "id": "HgklO2gowQgM"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prepare data\n",
        "Let's prepare the data as SpaCy would like it.\n",
        "It accepts list of tuples of text and labels."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62289b6556a5b63e4c85e825f5fe5f2f0ff0e105",
        "id": "8-BcDn0BwQgM"
      },
      "cell_type": "code",
      "source": [
        "train_df['tuples'] = train_df.apply(\n",
        "    lambda row: (row['Text'],row['Score']), axis=1)\n",
        "train = train_df['tuples'].tolist()\n",
        "train[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40af723e2e6c7dba39f2a4efe6ea28cae69087bf",
        "id": "CievMV5nwQgM"
      },
      "cell_type": "code",
      "source": [
        "train[-2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dbcea3ccdb03ee795d8d0c9b65d2ddcd055b3b3",
        "id": "Ca_V8iKbwQgM"
      },
      "cell_type": "code",
      "source": [
        "#functions from spacy documentation\n",
        "def load_data(limit=0, split=0.8):\n",
        "    train_data = train\n",
        "    np.random.shuffle(train_data)\n",
        "    train_data = train_data[-limit:]\n",
        "    texts, labels = zip(*train_data)\n",
        "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 1e-8  # True positives\n",
        "    fp = 1e-8  # False positives\n",
        "    fn = 1e-8  # False negatives\n",
        "    tn = 1e-8  # True negatives\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if score >= 0.5 and gold[label] >= 0.5:\n",
        "                tp += 1.\n",
        "            elif score >= 0.5 and gold[label] < 0.5:\n",
        "                fp += 1.\n",
        "            elif score < 0.5 and gold[label] < 0.5:\n",
        "                tn += 1\n",
        "            elif score < 0.5 and gold[label] >= 0.5:\n",
        "                fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
        "\n",
        "#(\"Number of texts to train from\",\"t\" , int)\n",
        "n_texts=30000\n",
        "#You can increase texts count if you have more computational power.\n",
        "\n",
        "#(\"Number of training iterations\", \"n\", int))\n",
        "n_iter=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a7ba3ef169e93644c6d324c030e5b0065aa9e13",
        "id": "k6za8CD7wQgM"
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')  # create english Language class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e913351d8c6340008621eec634f19a448a07f478",
        "id": "c_gjbuy4wQgN"
      },
      "cell_type": "code",
      "source": [
        "# add the text classifier to the pipeline if it doesn't exist\n",
        "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "if 'textcat' not in nlp.pipe_names:\n",
        "    textcat = nlp.create_pipe('textcat')\n",
        "    nlp.add_pipe(textcat, last=True)\n",
        "# otherwise, get it, so we can add labels to it\n",
        "else:\n",
        "    textcat = nlp.get_pipe('textcat')\n",
        "\n",
        "# add label to text classifier\n",
        "textcat.add_label('POSITIVE')\n",
        "\n",
        "# load the dataset\n",
        "print(\"Loading food reviews data...\")\n",
        "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
        "print(\"Using {} examples ({} training, {} evaluation)\"\n",
        "      .format(n_texts, len(train_texts), len(dev_texts)))\n",
        "train_data = list(zip(train_texts,\n",
        "                      [{'cats': cats} for cats in train_cats]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf90757df96b0020e96f524ae2a799ffeb92c2d8",
        "id": "gVna-mx-wQgN"
      },
      "cell_type": "markdown",
      "source": [
        "### Training model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd483f12ee2663acb586f9c9b7dc69dc3784791b",
        "id": "xgYdaxupwQgN"
      },
      "cell_type": "code",
      "source": [
        "# get names of other pipes to disable them during training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "    optimizer = nlp.begin_training()\n",
        "    print(\"Training the model...\")\n",
        "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
        "    for i in range(n_iter):\n",
        "        losses = {}\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
        "                       losses=losses)\n",
        "        with textcat.model.use_params(optimizer.averages):\n",
        "            # evaluate on the dev data split off in load_data()\n",
        "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
        "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
        "              .format(losses['textcat'], scores['textcat_p'],\n",
        "                      scores['textcat_r'], scores['textcat_f']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa8aed6aebeb36b24bed6bb71e78efbfaf1b0bbd",
        "id": "M0fvyshnwQgN"
      },
      "cell_type": "code",
      "source": [
        "# test the trained model\n",
        "test_text1 = 'This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"'\n",
        "test_text2=\"I bought this product at a local store, not from this seller.  I usually use Wellness canned food, but thought my cat was bored and wanted something new.  So I picked this up, knowing that Evo is a really good brand (like Wellness).<br /><br />It is one of the most disgusting smelling cat foods I've ever had the displeasure of using.  I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food.  I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\"\n",
        "doc = nlp(test_text1)\n",
        "test_text1, doc.cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1634b107850fd2259fc4a66e6ea1704e1d12cf12",
        "id": "xlGvN6pbwQgN"
      },
      "cell_type": "markdown",
      "source": [
        "Positive review is indeed close to 1"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11d75e3ebbed865e81f3c77a6b7e50367218d60c",
        "id": "Ly0DZkfHwQgN"
      },
      "cell_type": "code",
      "source": [
        "doc2 = nlp(test_text2)\n",
        "test_text2, doc2.cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bcfa478277ddf06b5bfc3f23c9fa454c0fd979e",
        "id": "F8ZeIWoUwQgN"
      },
      "cell_type": "markdown",
      "source": [
        "Negative review is close to 0"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "811972b7bffd711bcd64954ab5d7d94b607bfea3",
        "id": "UYiDbWaEwQgN"
      },
      "cell_type": "code",
      "source": [
        "output_dir=%pwd\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a88593022d2daef645cff7bc4f849dabc7d31795",
        "id": "YJD3kxzLwQgN"
      },
      "cell_type": "code",
      "source": [
        "# test the saved model\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "doc2 = nlp2(test_text2)\n",
        "print(test_text2, doc2.cats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5da42c208abfe16c37fbb177b1f641d584b91068",
        "id": "spZqxP7KwQgN"
      },
      "cell_type": "markdown",
      "source": [
        "Model looks preety good. We can definitely improve it further by feeding more data and data augmentations.\n",
        "Thanks for reading. Hope you learnt something new :)  #TODO Data Augmentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Text Classification using SpaCy",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}