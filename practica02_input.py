# -*- coding: utf-8 -*-
"""practica02-input.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fruSgNNOe7RZy3svChM5mlGh_LX4laxJ

<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/1_Introduccion/Introduccion.ipynb"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>
"""

print("A: Instalacion de Stanza")

!pip install stanza

print("B: descarga de bibliotecas adicionales")

import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

("C: Importacion de Stanza y descarga de stanza en ingles")

import stanza

# Descarga el pipeline para el español
stanza.download('en')

!pip install --upgrade stanza

print("D: 4.1-Lema de las palabras del archivo input.txt, y conversion a archivo json")

import stanza
import json

def extract_lemmas(text):
    """
    Extracts lemmas from the given text using Stanza.

    Args:
        text: The input text.

    Returns:
        A list of lemmas extracted from the text.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')
    doc = nlp(text)
    lemmas = []

    for sentence in doc.sentences:
        for word in sentence.words:
            lemmas.append(word.lemma)

    return lemmas

# Example usage
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

extracted_lemmas = extract_lemmas(text)

# Save lemmas to JSON file
with open('lemmas.json', 'w') as f:
    json.dump(extracted_lemmas, f, indent=4)

print(extracted_lemmas)

print("D: 4.2.Identificando entidades nombradas en input.txt, y conversion a archivo json")

import stanza

# Download the English model with NER
stanza.download('en', processors='tokenize,mwt,pos,lemma,ner')

# Initialize the pipeline
nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,ner')

# Process the text
doc = nlp("""Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well.""")

entities = []

for sent in doc.sentences:
    for ent in sent.ents:
        entity_dict = {"text": ent.text, "type": ent.type}
        entities.append(entity_dict)

# Convertir la lista de entidades a JSON
json_data = json.dumps(entities, indent=4)

# Imprimir o guardar el JSON
print(json_data)

# Para guardar en un archivo:
with open('entities.json', 'w') as f:
    f.write(json_data)

print("E: Análisis sintáctico de dependencias que te permita extraer de cada oración las proposiciones correspondientes a las estructuras,1.NVN sujeto-verbo-objeto (cuando haya objeto directo)")

import stanza

def extract_nvn_propositions(sentence):
    """
    Extracts NVN (Subject-Verb-Object) propositions from a Stanza sentence.

    Args:
        sentence: A Stanza Sentence object.

    Returns:
        A list of strings, each representing an NVN proposition.
    """
    propositions = []
    for word in sentence.words:
        # Check if word.head is a Word object before accessing deprel
        if isinstance(word.head, stanza.models.common.doc.Word) and word.deprel == "obj" and word.head.deprel == "nsubj":
            subject = sentence.words[word.head.id - 1].text
            verb = word.head.text
            object_ = word.text
            propositions.append(f"{subject} {verb} {object_}")
    return propositions

# Download the English model with dependency parsing
stanza.download('en', processors='tokenize,mwt,pos,lemma,ner,depparse')

# Initialize the pipeline
nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,ner,depparse')

# Process the text
doc = nlp("""Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well.""")

# Extract and print NVN propositions
for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        print(f"Sentence: {sent.text}")
        for prop in propositions:
            print(f"  - {prop}")
        print()

"""¿Por qué no se encuentran proposiciones NVN con objeto directo en la salida?

Hay varias razones posibles para esto:

Estructura de las oraciones:

Oraciones incompletas: Algunas oraciones en el texto de entrada podrían estar incompletas o tener estructuras gramaticales que no se ajustan al patrón sujeto-verbo-objeto directo (NVN).
Verbos intransitivos: Algunos verbos no requieren un objeto directo para completar su significado (por ejemplo, "sleep", "leave").
Objetos indirectos: En algunos casos, el verbo podría tener un objeto indirecto en lugar de uno directo.
Limitaciones del análisis de dependencias:

Ambigüedades: El análisis de dependencias puede tener dificultades para resolver ambigüedades en oraciones complejas o con construcciones no estándar.
Errores en la etiqueta: Las etiquetas de dependencia asignadas por Stanza pueden no ser siempre correctas, lo que podría afectar la extracción de proposiciones.
Falta de contexto: El análisis de dependencias se basa en el contexto local de cada palabra, lo que puede llevar a una interpretación incorrecta de la estructura de la oración.
¿Cómo podemos mejorar la extracción de proposiciones NVN?

Aumentar la complejidad de la regla: Podemos ampliar la regla para considerar otros tipos de relaciones de dependencia y estructuras gramaticales. Por ejemplo, podríamos buscar objetos indirectos o complementos circunstanciales.
Utilizar un lematizador más sofisticado: Un lematizador más avanzado podría ayudar a identificar las formas correctas de los verbos y los sustantivos, lo que podría mejorar la precisión del análisis.
Explorar otras bibliotecas de procesamiento de lenguaje natural: Existen otras bibliotecas como spaCy o NLTK que ofrecen diferentes enfoques para el análisis sintáctico y la extracción de información.
Entrenar un modelo de lenguaje personalizado: Si tienes un corpus grande y específico, puedes entrenar un modelo de lenguaje personalizado para mejorar la precisión del análisis de dependencias en tu dominio.
"""

import stanza

def extract_nvn_propositions(sentence):
    """
    Extrae proposiciones NVN (Sujeto-Verbo-Objeto) de una oración de Stanza.

    Args:
        sentence: Un objeto Stanza Sentence.

    Returns:
        Una lista de cadenas, cada una representando una proposición NVN.
    """
    propositions = []
    for word in sentence.words:
        # Check if word.head is an integer (0) before accessing deprel
        if word.head != 0 and word.deprel == "obj" and sentence.words[word.head - 1].deprel == "nsubj":
            subject = sentence.words[word.head - 1].text
            verb = sentence.words[word.head -1].text #Get the verb correctly
            object_ = word.text
            propositions.append(f"{subject} {verb} {object_}")
    return propositions

# Download the English model with dependency parsing
stanza.download('en', processors='tokenize,mwt,pos,lemma,ner,depparse')

# Initialize the pipeline
nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,ner,depparse')

# Procesar el texto
doc = nlp("""Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well.""")

# Extraer e imprimir proposiciones NVN
# The loop to check objects is removed as it was causing incorrect indentation.
# Now 'propositions' is calculated for every sentence and the if condition
# is at the sentence level.
for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        print(f"Sentence: {sent.text}")
        for prop in propositions:
            print(f"  - {prop}")
        print()

"""Excelente trabajo hasta ahora! El código que has proporcionado es un buen punto de partida para extraer proposiciones NVN (sujeto-verbo-objeto) utilizando Stanza. Sin embargo, hay algunas áreas en las que podemos mejorar y expandir su funcionalidad.

Entendiendo el Código Actual
Descarga del modelo: Se descarga el modelo de inglés de Stanza, que incluye el análisis de dependencias.
Inicialización de la pipeline: Se crea una pipeline para procesar el texto y obtener información sobre las palabras, sus partes del discurso y las relaciones entre ellas.
Procesamiento del texto: El texto de entrada se procesa y se obtiene una representación estructurada de las oraciones.
Función extract_nvn_propositions:
Itera sobre cada palabra en una oración.
Verifica si la palabra es un objeto directo y si su cabeza es un sujeto nominal.
Si se cumplen las condiciones, extrae el sujeto, verbo y objeto, y los combina en una proposición.
Mejoras y Consideraciones
1. Manejo de Objetos Indirectos y Complementos Circunstanciales:
Ampliar la búsqueda: En lugar de limitarse a objetos directos (obj), se pueden incluir objetos indirectos (iobj) y complementos circunstanciales (obl) para capturar una gama más amplia de construcciones.

2. Considerar Verbos Multipalabra:
Identificar verbos compuestos: Algunos verbos pueden estar compuestos por múltiples palabras. Se pueden utilizar técnicas de reconocimiento de patrones o consultar un léxico de verbos compuestos para identificarlos.
Ejemplo: Utilizar expresiones regulares o un lematizador más sofisticado para identificar verbos compuestos.
3. Manejar Pasivas:
Invertir sujeto y objeto: En oraciones pasivas, el sujeto lógico se convierte en el objeto gramatical. Se puede adaptar la lógica para identificar estas construcciones y extraer las proposiciones correctamente.
Ejemplo: Buscar verbos en voz pasiva y ajustar la extracción del sujeto y objeto en consecuencia.
4. Considerar Oraciones Complejas:
Manejar cláusulas subordinadas: Para oraciones con cláusulas subordinadas, se puede utilizar un análisis sintáctico más profundo para identificar las proposiciones principales y secundarias.
Ejemplo: Utilizar recursión o algoritmos de búsqueda en árboles de dependencia para descomponer oraciones complejas.
5. Evaluación:
Conjunto de datos de prueba: Crear un conjunto de datos de prueba con oraciones anotadas manualmente para evaluar la precisión del extractor de proposiciones.
Métricas de evaluación: Utilizar métricas como precisión, recall y F1-score para medir el rendimiento del sistema.
6. Visualización:
Dependencias: Visualizar las dependencias de las palabras en una oración puede ayudar a comprender mejor el análisis sintáctico y a identificar posibles errores.
Árboles de sintaxis: Utilizar herramientas como stanza.visualize para crear visualizaciones de los árboles de sintaxis.
"""

def extract_nvn_propositions(sentence):
    for word in sentence.words:
        if word.head != 0 and word.deprel in ["obj", "iobj", "obl"]:
            subject = sentence.words[word.head - 1].text
            verb = sentence.words[word.head - 1].lemma  # Usar el lema del verbo
            object_ = word.text
            [].append(f"{subject} {verb} {object_}")
    return []

for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        print(f"Oración: {sent.text}")
        for prop in propositions:
            print(f"  - {prop}")

"""¡Claro! Si el código que te proporcioné no está imprimiendo las proposiciones NVN, hay varias razones posibles por las que esto podría estar sucediendo.

¿Por qué no se están imprimiendo las proposiciones?

No se encuentran proposiciones NVN:

Estructura de las oraciones: Es posible que las oraciones en tu texto no se ajusten al patrón sujeto-verbo-objeto directo que estás buscando.
Errores en el análisis de dependencias: Stanza puede estar asignando etiquetas de dependencia incorrectas, lo que dificulta la identificación de las proposiciones.
Errores en el código:

Índices incorrectos: Asegúrate de que los índices utilizados para acceder a las palabras en la oración sean correctos y no causen errores fuera de rango.
Condiciones lógicas incorrectas: Verifica que las condiciones en el if estén correctamente formuladas para identificar las proposiciones NVN.
Problemas de impresión: Asegúrate de que no haya errores en la sintaxis de impresión o que las variables estén definidas correctamente.
¿Cómo solucionar el problema?

Depurar el código:

Agregar print dentro del bucle: Imprime los valores de word.head, word.deprel, y sentence.words[word.head - 1].deprel para cada palabra para entender cómo está funcionando el código.
Simplificar el texto: Prueba con oraciones más simples para aislar el problema.
Visualizar el árbol de dependencias: Utiliza stanza.visualize para visualizar el árbol de dependencias de cada oración y verificar si las relaciones entre las palabras son correctas.
Revisar la lógica de la función:

Asegurarse de que las condiciones sean correctas: Verifica que la condición word.deprel == "obj" and sentence.words[word.head - 1].deprel == "nsubj" sea la adecuada para identificar las proposiciones que buscas.
Considerar otros tipos de objetos: Si quieres incluir objetos indirectos o complementos circunstanciales, ajusta la condición para incluir iobj y obl.
Verificar el texto de entrada:

Asegurarse de que el texto esté formateado correctamente: Elimina cualquier carácter especial o errores de formato que puedan afectar el análisis.
Probar con diferentes textos: Utiliza diferentes textos para ver si el problema persiste.
"""

def extract_nvn_propositions(sentence):
    for word in sentence.words:
        if word.head != 0:
            print(f"Palabra: {word.text}, Cabeza: {word.head}, Dependencia: {word.deprel}")
            if word.deprel == "obj" and sentence.words[word.head - 1].deprel == "nsubj":
                subject = sentence.words[word.head - 1].text
                verb = sentence.words[word.head - 1].text
                object_ = word.text
                [].append(f"{subject} {verb} {object_}")
    return extract_nvn_propositions

def extract_nvn_propositions(sentence):
    propositions = []  # Initialize an empty list to store propositions
    for word in sentence.words:
        if word.head != 0:
            print(f"Palabra: {word.text}, Cabeza: {word.head}, Dependencia: {word.deprel}")
            if word.deprel == "obj" and sentence.words[word.head - 1].deprel == "nsubj":
                subject = sentence.words[word.head - 1].text
                verb = sentence.words[word.head - 1].text
                object_ = word.text
                propositions.append(f"{subject} {verb} {object_}") # Append to the list
    return propositions  # Return the list of propositions

import stanza
import json

def extract_nvn_propositions(sentence):
    """
    Extrae estructuras NVN (verbo-sujeto-objeto) de una oración utilizando Stanza.

    Args:
        sentence: Una oración procesada por Stanza.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVN encontrada.
    """

    nvn_structures = []
    root = None
    for word in sentence.words:
        if word.head == 0:
            root = word
            break

    if root:
        verb = root.lemma
        subject = None
        object_ = None

        for dep in sentence.dependencies:
            if dep[0].id == root.id:
                if dep[1] == 'nsubj':
                    subject = sentence.words[dep[2].id - 1].lemma
                elif dep[1] == 'dobj':
                    object_ = sentence.words[dep[2].id - 1].lemma

        if subject and object_:
            nvn_structure = {
                "verb": verb,
                "subject": subject,
                "object": object_
            }
            nvn_structures.append(nvn_structure)

    return nvn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
doc = nlp(text)
all_nvn_structures = []

for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        all_nvn_structures.extend(propositions)

# Guardar en un archivo JSON
with open('nvn_structures.json', 'w') as f:
    json.dump(all_nvn_structures, f, indent=4)

# Imprimir las proposiciones encontradas
for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        print(f"Oración: {sent.text}")
        for prop in propositions:
            print(f"  - {prop}")

for sent in doc.sentences:
    propositions = extract_nvn_propositions(sent)
    if propositions:
        print(f"Oración: {sent.text}")
        for prop in propositions:
            print(f"  - {prop}")

print("E: Análisis sintáctico de dependencias,3.1.NVN sujeto-verbo-objeto (cuando haya objeto directo), conversion a archivo json")

import stanza
import json

def extract_nvn_dependencies(text):
    """
    Extrae estructuras NVN (sujeto-verbo-objeto) utilizando el análisis de dependencias de Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.lemma
            subject = None
            object_ = None

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].lemma

            if subject and object_:  # Solo si se encuentran sujeto y objeto
                nvn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_
                }
                nvn_structures.append(nvn_structure)

    return nvn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvn_structures = extract_nvn_dependencies(text)

# Guardar en un archivo JSON
with open('nvn_structures.json', 'w') as f:
    json.dump(nvn_structures, f, indent=4)

print(nvn_structures)

print("F: Análisis sintáctico de dependencias,3.2-NV sujeto-verbo (cuando no haya objeto directo), y conversion a  archivos json ")

import stanza
import json

def extract_nv_propositions(text):
    """
    Extrae proposiciones sujeto-verbo de un texto en inglés.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de proposiciones NV.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nv_propositions = []

    for sentence in doc.sentences:
        for word in sentence.words:
            if word.deprel == 'nsubj':  # Verificamos si la palabra es un sujeto nominal
                subject = word.text
                for head in sentence.words:
                    if head.head == 0:  # Encontramos el verbo principal
                        verb = head.text
                        nv_propositions.append(f"{subject} {verb}")
                        break

    return nv_propositions

# Ejemplo de uso
text = ( "Jane bought me these books.\nJane bought a book for me.\nShe dropped a line to him. Thank you.\nShe sleeps.\nI sleep a lot.\nI was born in Madrid.\nthe cat was chased by the dog.\nI was born in Madrid during 1995.\nOut of all this , something good will come.\nSusan left after the rehearsal. She did it well.\nShe sleeps during the morning, but she sleeps.")
propositions = extract_nv_propositions(text)

# Crear un diccionario con las oraciones y sus proposiciones
data = []
for i, sentence in enumerate(doc.sentences):
    data.append({
        "sentence": sentence.text,
        "propositions": propositions[i] if i < len(propositions) else []
    })

# Convertir los datos a formato JSON
json_data = json.dumps(data, indent=4)

# Imprimir el JSON
print(json_data)

# Guardar el JSON en un archivo
with open("propositions.json", "w") as outfile:
    outfile.write(json_data)

print("G: Análisis sintáctico de dependencias,3.3-Las proposiciones correspondientes a las estructuras:Todas las ocurrencias NVPN sujeto-verbo-preposición-complemento (una por cada complemento), y conversion a archivo json")

import stanza

def extract_nvpn_propositions(text):
    """
    Extrae proposiciones NVPN (sujeto-verbo-preposición-complemento) de un texto.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una proposición NVPN.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvpn_propositions = []

    for sentence in doc.sentences:
        for word in sentence.words:
            if word.deprel == 'nsubj':  # Si la palabra es el sujeto
                subject = word.text
                # Check if the head is the root by comparing the index to 0
                if word.head == 0:  # Verificar si el padre es el verbo raíz
                    # Iterate through words in the sentence to find the root verb
                    for head_word in sentence.words:
                        if head_word.id == word.head:
                            verb = head_word.text
                            break  # Exit loop once root verb is found
                    else:  # If no root verb is found, continue to next word
                        continue

                    for child in sentence.words:  # Iterate over all words for children
                        if child.head == word.head and child.deprel in ['prep', 'mark']:
                            preposition = child.text
                            for grandchild in sentence.words:  # Iterate for grandchildren
                                if grandchild.head == child.id:
                                    complement = grandchild.text
                                    nvpn_propositions.append({
                                        "subject": subject,
                                        "verb": verb,
                                        "preposition": preposition,
                                        "complement": complement
                                    })

    return nvpn_propositions

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

propositions = extract_nvpn_propositions(text)

# Imprimir las proposiciones en formato JSON
import json
print(json.dumps(propositions, indent=4))

print("H: El análisis puede apoyarse en otra opción más potente es el uso de queries sobre el grafo de dependencias, y conversion archivo json ")

import stanza
import networkx as nx
import matplotlib.pyplot as plt

def analyze_dependency_graph(text):
    """
    Analiza el grafo de dependencias de un texto dado utilizando Stanza y lo visualiza.

    Args:
        text: El texto de entrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)

    G = nx.DiGraph()

    for sentence in doc.sentences:
        for word in sent.words:
            G.add_node(word.text, pos=word.upos)  # Agregamos la etiqueta POS como atributo del nodo

        for dep in sent.dependencies:
            head = sent.words[dep[0].id - 1].text
            dep_word = sent.words[dep[2].id - 1].text
            rel = dep[1]
            G.add_edge(head, dep_word, label=rel)

    # Visualización del grafo
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, font_weight='bold')
    labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
    plt.show()  # Make sure this line is not commented out

# Example usage
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""
analyze_dependency_graph(text)

import stanza
import networkx as nx
import json

def analyze_dependency_graph(text):
  """
  Analiza el grafo de dependencias de un texto y devuelve su representación en JSON.

  Args:
      text: El texto de entrada.

  Returns:
      Un diccionario JSON que representa el grafo de dependencias.
  """

  nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
  doc = nlp(text)

  G = nx.DiGraph()

  for sentence in doc.sentences:
    for word in sentence.words:
      G.add_node(word.text, pos=word.upos)  # Agregamos la etiqueta POS como atributo del nodo

    for dep in sentence.dependencies:
      head = sentence.words[dep[0].id - 1].text
      dep_word = sentence.words[dep[2].id - 1].text
      rel = dep[1]
      G.add_edge(head, dep_word, label=rel)

  # Convertir el grafo a un diccionario JSON
  graph_json = {
      "nodes": [{"id": n, "label": n, "data": {"pos": G.nodes[n]["pos"]}} for n in G.nodes],
      "edges": [{"source": e[0], "target": e[1], "label": G.edges[e]["label"]} for e in G.edges]
  }

  return graph_json

# Example usage
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

graph_json = analyze_dependency_graph(text)

# Puedes guardar el JSON en un archivo o procesarlo como necesites
print(json.dumps(graph_json, indent=4))  # Imprimir el JSON formateado

import stanza
import networkx as nx
import matplotlib.pyplot as plt

# Crea un grafo vacío
G = nx.DiGraph()

nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""
doc = nlp(text)

# Agrega los nodos (palabras) al grafo
for sent in doc.sentences:
    for word in sent.words:
        G.add_node(word.text)
# Agrega las aristas (relaciones) al grafo
for sent in doc.sentences:
    for dep in sent.dependencies:
        # dep[0].id is the index of the governor (head) in the sentence, starting at 1
        # We need to subtract 1 to get the correct index in the list sent.words
        head = sent.words[dep[0].id - 3].text
        # Similar for the dependent word
        dep_word = sent.words[dep[2].id - 3].text
        rel = dep[1]  # dep[1] is the dependency type
        G.add_edge(head, dep_word, label=rel)


# Dibuja el grafo
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, font_weight='bold')
labels = nx.get_edge_attributes(G,'label')
nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)

plt.show()

"""Un grafo de dependencias es una representación visual de las relaciones gramaticales entre las palabras en una oración o un texto. Cada nodo en el grafo representa una palabra, y las aristas (líneas que conectan los nodos) muestran las relaciones de dependencia entre esas palabras.

Interpretación del Grafo Específico

En el grafo que nos presentas, cada círculo representa una palabra y las líneas que los conectan indican cómo se relacionan gramaticalmente esas palabras. Las etiquetas en las líneas indican el tipo de relación (por ejemplo, "nsubj" para sujeto nominal, "obj" para objeto directo, etc.).

Observaciones Clave:

Jerarquía: Las palabras se organizan jerárquicamente. La palabra principal (el verbo en muchas oraciones) suele estar en la parte superior, y las demás palabras dependen de ella.
Relaciones: Las relaciones pueden ser de sujeto a verbo, verbo a objeto, modificadores a palabras, etc. Por ejemplo, en "Jane bought me these books", "Jane" es el sujeto de "bought", y "books" es el objeto directo.
Dependencias: Cada palabra depende de otra en la oración, creando una estructura compleja de relaciones.
Ejemplo de Interpretación:

La oración "Jane bought me these books" se representa en el grafo de la siguiente manera:

"Jane" es el sujeto de "bought" (relación "nsubj").
"bought" es el verbo principal y el núcleo de la oración.
"me" es el objeto indirecto de "bought" (relación "iobj").
"these books" es el objeto directo de "bought" (relación "obj").
Limitaciones y Consideraciones:

Complejidad: Los grafos de dependencias pueden ser bastante complejos, especialmente para oraciones largas o construcciones sintácticas inusuales.
Ambigüedades: En algunos casos, puede haber múltiples interpretaciones posibles para una misma oración, lo que se reflejará en diferentes grafos de dependencias.
Dependencia del analizador: La calidad del grafo depende en gran medida del analizador sintáctico utilizado. Diferentes analizadores pueden producir resultados ligeramente diferentes.
Aplicaciones de los Grafos de Dependencias:

Los grafos de dependencias tienen numerosas aplicaciones en el procesamiento del lenguaje natural, como:

Análisis sintáctico: Entender la estructura de las oraciones.
Traducción automática: Modelar las relaciones gramaticales entre diferentes idiomas.
Resumen de texto: Identificar las partes más importantes de un texto.
Respuesta a preguntas: Comprender las preguntas y encontrar las respuestas relevantes en un texto.
Análisis de sentimientos: Identificar emociones y opiniones en un texto.
En Resumen

Los grafos de dependencias son herramientas poderosas para analizar la estructura sintáctica de un texto. Al visualizar las relaciones entre las palabras, podemos obtener una comprensión más profunda del significado de una oración.
"""

import stanza
import networkx as nx
import json

def analyze_dependency_graph(text):
  """
  Analiza el grafo de dependencias de un texto y devuelve su representación en JSON.

  Args:
      text: El texto de entrada.

  Returns:
      Un diccionario JSON que representa el grafo de dependencias.
  """

  nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
  doc = nlp(text)

  G = nx.DiGraph()

  # Agrega los nodos (palabras) al grafo
  for sent in doc.sentences:
    for word in sent.words:
      G.add_node(word.text, pos=word.upos)  # Agregar etiqueta POS como dato del nodo

  # Agrega las aristas (relaciones) al grafo
  for sent in doc.sentences:
    for dep in sent.dependencies:
      # Correct indexing for governor (head)
      head = sent.words[dep[0].id - 1].text  # Subtract 1 for correct index

      # Correct indexing for dependent word
      dep_word = sent.words[dep[2].id - 1].text  # Subtract 1 for correct index

      rel = dep[1]  # dep[1] is the dependency type
      G.add_edge(head, dep_word, label=rel)

  # Convertir el grafo a un diccionario JSON
  graph_json = {
      "nodes": [{"id": n, "label": n, "data": {"pos": G.nodes[n]["pos"]}} for n in G.nodes],
      "edges": [{"source": e[0], "target": e[1], "label": G.edges[e]["label"]} for e in G.edges]
  }

  return graph_json

# Example usage
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

graph_json = analyze_dependency_graph(text)

# Puedes guardar el JSON en un archivo o procesarlo como necesites

# Guardar en un archivo (opcional)
with open('dependency_graph.json', 'w') as f:
  json.dump(graph_json, f, indent=4)

# Imprimir el JSON formateado
print(json.dumps(graph_json, indent=4))

print("I: 6.Las posibles soluciones .. cada estudiante deberá decidir cómo aborda el problema.: Refinamiento de las Reglas de Extracción:Considerar verbos auxiliares:1. No todos los verbos principales son verbos léxicos. Los verbos auxiliares como \"be\", \"have\" y \"do\" a menudo forman parte del verbo principales, y conversion de sallida a archivo json")

"""# **Mejorando el Análisis Sintáctico de Dependencias: Propuesta de Soluciones**
Excelente pregunta. El análisis de dependencias es una herramienta poderosa para extraer información estructural de un texto, pero podemos refinar aún más nuestra extracción de proposiciones.

Propuesta de Soluciones
1. Refinamiento de las Reglas de Extracción:

Considerar verbos auxiliares: No todos los verbos principales son verbos léxicos. Los verbos auxiliares como "be", "have" y "do" a menudo forman parte del verbo principal.
Manejar construcciones pasivas: En construcciones pasivas, el sujeto lógico se convierte en objeto y viceversa. Es necesario identificar estas construcciones para extraer las proposiciones correctamente.
Identificar complementos indirectos: Además de los objetos directos, podemos buscar complementos indirectos (por ejemplo, "to me" en "Jane bought me these books").
Manejar preposiciones complejas: Algunas preposiciones pueden introducir complementos más complejos que requieren un análisis más profundo.

Explicación:

Identificación del verbo principal:

Se identifica la raíz de la oración.
Se verifica si el nodo raíz es el verbo principal o si es un verbo auxiliar.
Si es un verbo auxiliar, se concatena con el verbo raíz para formar el verbo principal completo.
Extracción de sujeto y objeto:

Se buscan los hijos del verbo (raíz o auxiliar) que tengan las relaciones de dependencia "nsubj" (sujeto nominal) y "dobj" (objeto directo).
Creación de las proposiciones:

Se crea un diccionario para cada proposición, incluyendo el verbo, el sujeto y el objeto directo.
Ejemplo de salida:

[{'verb': 'bought', 'subject': 'Jane', 'direct_object': 'books'},
 {'verb': 'has bought', 'subject': 'Jane', 'direct_object': 'book'},
 {'verb': 'is sleeping', 'subject': 'She', 'direct_object': None},
 {'verb': 'have been', 'subject': 'I', 'direct_object': 'to Paris'}]
Nota:

Este código es una demostración básica. Para un análisis más robusto, es necesario considerar otros tipos de verbos auxiliares, construcciones pasivas, y diferentes tipos de objetos (indirectos, preposicionales, etc.).
Se puede mejorar aún más mediante el uso de patrones de coincidencia más complejos para identificar diferentes construcciones gramaticales.
"""

import stanza
import json

def extract_propositions(text):
  """
  Extrae proposiciones de un texto utilizando Stanza, considerando verbos auxiliares.

  Args:
      text: El texto de entrada.

  Returns:
      Una lista de diccionarios JSON que representan las proposiciones extraídas.
  """

  nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
  doc = nlp(text)
  propositions = []

  for sent in doc.sentences:
    root = None
    for word in sent.words:
      if word.head == 0:
        root = word
        break

    if root:
      proposition = {}
      proposition['verb'] = ""  # Inicializamos el verbo como una cadena vacía

      # Identificar el verbo principal (considerando auxiliares)
      verb = root.text  # Assign initial value to verb
      if root.deprel != 'ROOT':  # Check for non-root verbs and search for auxiliaries
        for dep in sent.dependencies:
          if dep[0].id == root.id and dep[1] == 'aux':
            verb = dep[2].text + " " + root.text
            break

      proposition['verb'] = verb

      subject = None
      direct_object = None
      # Iterate through the sentence's dependencies to find subject and object
      for dep in sent.dependencies:
        if dep[0].id == root.id and dep[1] == 'nsubj':
          subject = dep[2].text
        elif dep[0].id == root.id and dep[1] == 'dobj':
          direct_object = dep[2].text

      proposition['subject'] = subject
      proposition['direct_object'] = direct_object
      propositions.append(proposition)

  return propositions

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

propositions_json = json.dumps(extract_propositions(text), indent=4)
print(propositions_json)  # Imprime las proposiciones en formato JSON

print("I: 6.Las posibles soluciones .. cada estudiante deberá decidir cómo aborda el problema.: Refinamiento de las Reglas de Extracción:Considerar verbos auxiliares: 2.Manejar construcciones pasivas, y conversion a archivo json ")

import stanza
import json

def extract_propositions(text):
    """
    Extrae proposiciones de un texto utilizando Stanza, considerando construcciones pasivas,
    verbos auxiliares, y otras estructuras gramaticales.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios JSON que representan las proposiciones extraídas.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    propositions = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            proposition = {
                'verb': "",
                'verb_pos': "",
                'is_passive': False,
                'subject': None,
                'direct_object': None,
                'indirect_object': None,
            }

            # Identificar el verbo principal y sus auxiliares
            verb = root.text
            if root.deprel != 'ROOT':
                for dep in sentence.dependencies:
                    if dep[0].id == root.id and dep[1] == 'aux':
                        verb = dep[2].text + " " + root.text
                        break
            proposition['verb'] = verb
            proposition['verb_pos'] = root.upos

            # Identificar voz pasiva y asignar sujeto y objeto
            is_passive = False
            for dep in sentence.dependencies:
                if dep[0].id == root.id and dep[1] == 'auxpass':
                    is_passive = True
                    break

            for dep in sentence.dependencies:
                if is_passive:
                    if dep[0].id == root.id and dep[1] == 'nsubjpass':
                        proposition['subject'] = dep[2].text
                    elif dep[0].id == root.id and dep[1] == 'obj':
                        proposition['direct_object'] = dep[2].text
                else:
                    if dep[0].id == root.id and dep[1] == 'nsubj':
                        proposition['subject'] = dep[2].text
                    elif dep[0].id == root.id and dep[1] == 'dobj':
                        proposition['direct_object'] = dep[2].text
                    elif dep[0].id == root.id and dep[1] == 'iobj':  # Agregar complemento indirecto
                        proposition['indirect_object'] = dep[2].text

            propositions.append(proposition)

    return propositions

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

propositions = extract_propositions(text)

# Guardar las proposiciones en un archivo JSON
with open('propositions_pasivas.json', 'w') as f:
    json.dump(propositions, f, indent=4)

# Imprimir las proposiciones en formato JSON
print(json.dumps(propositions, indent=4))

"""Explicación:

Identificación de la voz pasiva: Se verifica si el verbo raíz tiene un hijo con la relación de dependencia "auxpass" (auxiliar pasivo).
Extracción de sujeto y objeto en voz pasiva: Si se detecta una construcción pasiva:
Se busca el sujeto pasivo (nsubjpass).
Se busca el objeto directo, que en este caso sería el sujeto lógico de la oración.
Extracción de sujeto y objeto en voz activa: Si no es una construcción pasiva, se extrae el sujeto y el objeto directo como en la versión anterior.
Ejemplo de salida (parcial):

[{'verb': 'bought', 'subject': 'Jane', 'direct_object': 'books'},
 {'verb': 'bought', 'subject': 'Jane', 'direct_object': 'book'},
 {'verb': 'dropped', 'subject': 'She', 'direct_object': 'line'},
 {'verb': 'sleeps', 'subject': 'She', 'direct_object': None},
 {'verb': 'sleep', 'subject': 'I', 'direct_object': None},
 {'verb': 'was born', 'subject': 'I', 'direct_object': 'Madrid'},
 {'verb': 'was chased', 'subject': 'cat', 'direct_object': 'dog'},
 {'verb': 'was born', 'subject': 'I', 'direct_object': 'Madrid'},
 ...]
Nota:

Este código maneja solo un tipo básico de construcción pasiva. Para un manejo más robusto, es necesario considerar casos más complejos y utilizar técnicas más avanzadas de análisis sintáctico.
Puedes mejorar aún más este código agregando lógica para manejar otros tipos de construcciones pasivas, como aquellas con verbos auxiliares diferentes o con estructuras más complejas.
"""

print("J: 6.Las posibles soluciones .. cada estudiante deberá decidir cómo aborda el problema.: 3.Utilizando patrones regulares, y conversion a archivo json ")

import re
import json

def extract_propositions_with_regex(text):
    """
    Extrae proposiciones utilizando expresiones regulares a partir del texto dado.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una proposición
        extraída mediante expresiones regulares.
    """

    # Patrones regulares (simplificados para este ejemplo)
    pattern_svo = r"(?P<subject>\w+) (?P<verb>\w+) (?P<object>\w+)"

    # Aplicación de los patrones
    matches_svo = re.finditer(pattern_svo, text)

    # Extracción de diccionarios
    propositions = [match.groupdict() for match in matches_svo]

    return propositions

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

propositions = extract_propositions_with_regex(text)

# Guardar las proposiciones en un archivo JSON
with open('propositions_regex.json', 'w') as f:
    json.dump(propositions, f, indent=4)

# Imprimir las proposiciones en formato JSON
print(json.dumps(propositions, indent=4))

print("K: 7.En el proceso, sustituye todas las palabras comunes por su lema (e.g. ran --> run, runners --> runner), y conversion a archivo json")

import stanza
import json

def analyze_dependency_graph(text, include_ner=True):
  """
  Analyzes the dependency graph of a given text using Stanza,
  substituting words by their lemma or named entity category.

  Args:
      text: The input text.
      include_ner (bool, optional): Whether to include Named Entity Recognition (NER). Defaults to True.

  Returns:
      A list of dictionaries, where each dictionary represents a dependency relation.
  """

  # Download the English model with optional NER
  stanza.download('en', processors='tokenize,mwt,pos,lemma,depparse' + (',ner' if include_ner else ''))

  # Create a pipeline explicitly including NER if desired
  nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma' + (',ner' if include_ner else ''))

  doc = nlp(text)
  dependencies = []

  for sentence in doc.sentences:
    for word in sentence.words:
      # Check if NER is enabled and the word has an entity type
      if include_ner and hasattr(word, 'ent_type') and word.ent_type != "O":
        word_text = word.ent_type  # Use entity type for named entities (except 'O')
      else:
        word_text = word.lemma  # Use lemma by default

      dependency = {
          "word": word_text,
          "pos": word.upos,
          "lemma": word.lemma,
          "head": word.head,
          "deprel": word.deprel
      }
      dependencies.append(dependency)

  return dependencies

# Example usage
text = """Jane bought me these books. (PERSON)
Jane bought a book for me. (PERSON)
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid. (LOCATION)
the cat was chased by the dog.
I was born in Madrid during 1995. (LOCATION, DATE)
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

dependency_graph = analyze_dependency_graph(text)

# Save to JSON file
with open('dependency_graph_ner_lemmas.json', 'w') as f:
    json.dump(dependency_graph, f, indent=4)

print(dependency_graph)

print("L: 8.Opcionalmente, sustituye todas las entidades nombradas por su categoría (e.g. John Smith --> PERSON), y conversion a archivo json ")

import stanza
import json

def analyze_dependency_graph(text):
  """
  Analyzes the dependency graph of a given text using Stanza,
  substituting named entities by their category.

  Args:
      text: The input text.

  Returns:
      A list of dictionaries, where each dictionary represents a dependency relation.
  """

  # Download the English model with NER
  stanza.download('en', processors='tokenize,mwt,pos,lemma,depparse,ner')

  # Create a pipeline including NER
  nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,ner,depparse')

  doc = nlp(text)
  dependencies = []

  for sentence in doc.sentences:
    for word in sentence.words:
      # Check if NER is enabled and the word has an entity type
      if hasattr(word, 'ent_type') and word.ent_type != "O":
        word_text = word.ent_type  # Use entity type for all named entities (except 'O')
      else:
        word_text = word.text  # Use original text for non-named entities

      dependency = {
          "word": word_text,
          "pos": word.upos,
          "lemma": word.lemma,
          "head": word.head,
          "deprel": word.deprel
      }
      dependencies.append(dependency)

  return dependencies

# Example usage
text = """Jane bought me these books. (PERSON)
Jane bought a book for me. (PERSON)
She dropped a line to him. Thank you. (PERSON)
She sleeps.
I sleep a lot.
I was born in Madrid. (LOCATION)
the cat was chased by the dog.
I was born in Madrid during 1995. (LOCATION, DATE)
Out of all this , something good will come.
Susan left after the rehearsal. She did it well. (PERSON)"""

dependency_graph = analyze_dependency_graph(text)

# Save to JSON file
with open('dependency_graph_ner_categories.json', 'w') as f:
    json.dump(dependency_graph, f, indent=4)

print(dependency_graph)

"""¡Claro! Con gusto te explico el código y su salida en español.

¿Qué hace este código?

Este código Python utiliza la biblioteca stanza para analizar el grafo de dependencias de un texto. En términos más simples, descompone un texto en sus partes constituyentes (palabras) y analiza cómo se relacionan entre sí gramaticalmente.

Paso a paso:

Importación de la biblioteca: Se importa la biblioteca stanza, una herramienta poderosa para el procesamiento del lenguaje natural.
Definición de la función: Se define una función llamada analyze_dependency_graph que toma como entrada un texto y devuelve una representación del grafo de dependencias de ese texto.
Descarga del modelo: Se descarga el modelo de lenguaje en inglés de stanza con las características necesarias para realizar el análisis (tokenización, lematización, etiquetado de partes del discurso y análisis de dependencias).
Creación del pipeline: Se crea un pipeline de procesamiento de lenguaje natural utilizando stanza. Este pipeline incluye las características descargadas en el paso anterior, y opcionalmente, el reconocimiento de entidades nombradas (NER) para identificar nombres de personas, lugares, organizaciones, etc.
Procesamiento del texto: El texto de entrada se pasa al pipeline, que lo procesa y genera una representación interna del texto.
Iteración sobre las palabras: Se itera sobre cada palabra en cada oración del texto.
Reemplazo de entidades nombradas: Si el reconocimiento de entidades nombradas está habilitado y la palabra es una entidad nombrada, se reemplaza la palabra por su tipo (por ejemplo, "PERSON", "LOCATION").
Creación de la representación de la dependencia: Para cada palabra, se crea un diccionario que contiene información sobre la palabra (texto, parte del discurso, lema, cabeza y relación de dependencia) y se añade a una lista.
Retorno de la salida: La función devuelve la lista de diccionarios, que representa el grafo de dependencias del texto.

###Qué significa la salida?

La salida de este código es una lista de diccionarios, donde cada diccionario representa una palabra y su relación con otras palabras en la oración. Por ejemplo, un diccionario podría tener la siguiente estructura:

{
  "word": "Jane",
  "pos": "PROPN",
  "lemma": "Jane",
  "head": 2,
  "deprel": "nsubj"
}
Esto significa que la palabra "Jane" es un nombre propio (PROPN), su forma lema es "Jane", su cabeza es la palabra en la posición 2 de la oración (probablemente el verbo "bought"), y su relación de dependencia con la cabeza es de sujeto nominal (nsubj).

¿Para qué sirve este código?

Este tipo de análisis es fundamental en muchas tareas de procesamiento del lenguaje natural, como la traducción automática, la generación de texto, el análisis de sentimientos y la extracción de información. Al comprender cómo se relacionan las palabras en una oración, podemos realizar inferencias más profundas sobre el significado del texto.

¿Qué mejoras se podrían hacer?

Visualización: Se podría visualizar el grafo de dependencias utilizando bibliotecas como networkx para obtener una representación gráfica de las relaciones entre las palabras.
Análisis más profundo: Se podrían realizar análisis más profundos del grafo de dependencias, como identificar sintagmas nominales, verbales, etc.
Idiomas: Se podrían utilizar modelos de stanza para otros idiomas además del inglés.
"""

print("M: 9.Finalmente, ordena las proposiciones obtenidas de mayor a menor frecuencia y conversion a archivo json  ")

import stanza

def analyze_dependency_graph(text):
  """
  Analiza el grafo de dependencias de un texto dado utilizando Stanza,
  sustituyendo las entidades nombradas por su categoría (si está habilitado).

  Args:
      text: El texto de entrada.

  Returns:
      Una lista de diccionarios, donde cada diccionario representa una relación de dependencia.
  """

  # Descarga el modelo de inglés con los procesadores necesarios
  stanza.download('en', processors='tokenize,mwt,pos,lemma,depparse')

  # Crea el pipeline de procesamiento
  nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse')

  doc = nlp(text)
  dependencies = []

  for sentence in doc.sentences:
    for word in sentence.words:
      dependency = {
          "word": word.text,  # Usamos word.text en lugar de word.lemma o ent_type
          "pos": word.upos,
          "head": word.head,
          "deprel": word.deprel
      }
      dependencies.append(dependency)

  return dependencies

def find_most_frequent_propositions(dependencies):
  """
  Encuentra las proposiciones más frecuentes en el grafo de dependencias.

  Args:
      dependencies: Lista de diccionarios representando las relaciones de dependencia.

  Returns:
      Una lista de tuplas (proposición, frecuencia), ordenada de mayor a menor frecuencia.
  """
  propositions = []
  for dependency in dependencies:
    propositions.append((dependency['deprel'], dependency['pos']))

  prop_counts = {}
  for prop in propositions:
    prop_counts[prop] = prop_counts.get(prop, 0) + 1

  sorted_props = sorted(prop_counts.items(), key=lambda x: x[1], reverse=True)
  return sorted_props

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

dependencies = analyze_dependency_graph(text)
most_frequent_props = find_most_frequent_propositions(dependencies)

print("Proposiciones ordenadas por frecuencia:")
for prop, count in most_frequent_props:
  print(f"{prop}: {count}")
# Guarda los resultados en un archivo JSON
with open('propositions_mayor_menor_frec.json', 'w') as f:
    json.dump(most_frequent_props, f, indent=4)

print(most_frequent_props)

"""Python

import stanza

def analyze_dependency_graph(text):
  '''
  Analiza el grafo de dependencias de un texto dado utilizando Stanza,
  sustituyendo las entidades nombradas por su categoría (si está habilitado).

  Args:
      text: El texto de entrada.

  Returns:
      Una lista de diccionarios, donde cada diccionario representa una relación de dependencia.
  '''

  # Descarga el modelo de inglés con los procesadores necesarios
  stanza.download('en', processors='tokenize,mwt,pos,lemma,depparse')

  # Crea el pipeline de procesamiento
  nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse')

  doc = nlp(text)
  dependencies = []

  for sentence in doc.sentences:
    for word in sentence.words:
      dependency = {
          "word": word.text,  # Usamos word.text en lugar de word.lemma o ent_type
          "pos": word.upos,
          "head": word.head,
          "deprel": word.deprel
      }
      dependencies.append(dependency)

  return dependencies

def find_most_frequent_propositions(dependencies):
  '''
  Encuentra las proposiciones más frecuentes en el grafo de dependencias.

  Args:
      dependencies: Lista de diccionarios representando las relaciones de dependencia.

  Returns:
      Una lista de tuplas (proposición, frecuencia), ordenada de mayor a menor frecuencia.
  '''
  propositions = []
  for dependency in dependencies:
    propositions.append((dependency['deprel'], dependency['pos']))

  prop_counts = {}
  for prop in propositions:
    prop_counts[prop] = prop_counts.get(prop, 0) + 1

  sorted_props = sorted(prop_counts.items(), key=lambda x: x[1], reverse=True)
  return sorted_props

# Ejemplo de uso
text = '''Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well.'''

dependencies = analyze_dependency_graph(text)
most_frequent_props = find_most_frequent_propositions(dependencies)

print("Proposiciones ordenadas por frecuencia:")
for prop, count in most_frequent_props:
  print(f"{prop}: {count}")
Explicación:

Función analyze_dependency_graph:

Se mantiene igual que en la respuesta anterior, analizando el texto con stanza y generando una lista de diccionarios con información sobre las palabras y sus dependencias.
Función find_most_frequent_propositions:

Crea una lista de tuplas (deprel, pos) para cada dependencia.
Utiliza un diccionario prop_counts para contar la frecuencia de cada tupla (proposición).
Ordena las tuplas de proposiciones por frecuencia descendente.
Devuelve la lista de proposiciones ordenadas.
Ejemplo de uso:

Se analiza el texto con la función analyze_dependency_graph.
Se encuentran las proposiciones más frecuentes utilizando la función find_most_frequent_propositions.
Se imprime la lista de proposiciones ordenadas por frecuencia.

Salida (aproximada):

Proposiciones ordenadas por frecuencia:
('punct', '.'): 10
('det', 'DET'): 4
('obj', 'NOUN'): 3
('nsubj', 'PROPN'): 3
('root', 'VERB'): 3
('case', 'ADP'): 3
('obl', 'NOUN'): 2
# ... otras proposiciones con menor frecuencia
Nota: La salida exacta puede variar ligeramente dependiendo de la versión de stanza y los detalles de la implementación.

Este código proporciona una manera de identificar las proposiciones más comunes en el texto analizado, lo que puede ser útil para entender los patrones de construcción de oraciones y las relaciones gramaticales más frecuentes.
"""

print("N: Por completitud, en el fichero de salida se incluye la estructura NVNPN, que no se menciona en el enunciado. y conversion a archivo json ")

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo) de un texto utilizando Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            # Inicializar variables
            verb = root.text
            subject = None
            object_ = None
            complement = None
            predicative = None

            # Buscar dependencias del verbo
            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'iobj':  # Complemento indirecto
                        complement = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'acomp':  # Complemento predicativo
                        predicative = sentence.words[dep[2].id - 1].text

            # Verificar si se encontró una estructura NVNPN
            if subject and object_:  # Se requiere al menos sujeto y objeto
                nvnpn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo) de un texto utilizando Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.text
            subject = None
            object_ = None
            complement = None
            predicative = None

            # Buscar dependencias del verbo
            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'iobj':  # Complemento indirecto
                        complement = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'acomp':  # Complemento predicativo
                        predicative = sentence.words[dep[2].id - 1].text

            # Verificar si se encontró una estructura NVNPN
            if subject and object_:  # Se requiere al menos sujeto y objeto
                nvnpn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

"""¿Por qué se obtiene esta salida?
Falta de estructuras NVNPN: El texto de entrada puede no contener oraciones con la estructura básica de verbo-sujeto-objeto-complemento-predicativo (NVNPN).
Limitaciones del análisis sintáctico: Stanza, a pesar de ser una herramienta poderosa, puede tener dificultades para identificar estructuras complejas o ambiguas.
Errores en la definición de la estructura NVNPN: La definición de NVNPN que estamos utilizando puede ser demasiado restrictiva o no capturar todas las posibles variaciones de esta estructura.
¿Cómo interpretar esta salida?
Texto sin estructuras NVNPN: Si el texto es muy corto o no contiene oraciones declarativas simples, es probable que no se encuentren estructuras NVNPN.
Textos complejos: Textos con estructuras sintácticas complejas, como oraciones subordinadas o construcciones idiomáticas, pueden dificultar la identificación de estructuras NVNPN.
Necesidad de ajustar la definición de NVNPN: Si se requiere identificar estructuras más complejas o variantes de NVNPN, es necesario ajustar la definición y las reglas de extracción.
"""

print("O: Por completitud, en el fichero de salida se incluye la estructura NVNPN, Expandir la definición de NVNPN: Considerar variantes de la estructura, como oraciones pasivas, verbos frasales y construcciones idiomáticas.. y conversion a archivo json ")

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo) de un texto
    utilizando Stanza, considerando variantes como oraciones pasivas,
    verbos frasales y construcciones idiomáticas (de manera simplificada).

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.text
            subject = None
            object_ = None
            complement = None
            predicative = None

            # Manejar verbos frasales (simplificado)
            if root.deprel == 'prt':
                verb += " " + sentence.words[root.id - 1].text

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'iobj':
                        complement = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'acomp':
                        predicative = sentence.words[dep[2].id - 1].text
                    # Manejar oraciones pasivas
                    elif dep[1] == 'nsubjpass':
                        subject = sentence.words[dep[2].id - 1].text
                    elif dep[1] == 'obj' and is_passive(sentence, root):
                        object_ = sentence.words[dep[2].id - 1].text

            # Verificar si se encontró una estructura NVNPN
            if subject and object_:
                nvnpn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

def is_passive(sentence, root):
    """
    Verifica si la oración está en voz pasiva.

    Args:
        sentence: La oración analizada por Stanza.
        root: El nodo raíz de la oración.

    Returns:
        True si la oración está en voz pasiva, False en caso contrario.
    """
    for dep in sentence.dependencies:
        if dep[0].id == root.id and dep[1] == 'auxpass':
            return True
    return False

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures_improved.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

"""# Sección nueva
Explicación de las mejoras:

Manejo de verbos frasales: Se agrega una lógica simplificada para manejar verbos frasales. Se verifica si la raíz del verbo tiene una dependencia prt (partícula) y, en caso afirmativo, se concatena la partícula al verbo.
Manejo de oraciones pasivas: Se crea una función auxiliar is_passive para verificar la presencia de la etiqueta auxpass en las dependencias del verbo.
Identificación del objeto en voz pasiva: Se verifica si la oración está en voz pasiva y, en ese caso, se identifica el objeto usando la etiqueta obj.
"""

print("P: Por completitud, en el fichero de salida se incluye la estructura NVNPN, Expandir la definición de NVNPN: Utilizar un lematizador más preciso: Un lematizador más preciso puede ayudar a identificar las raíces de las palabras y mejorar la identificación de las estructuras sintácticas.. y conversion a archivo json ")

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo) de un texto
    utilizando Stanza, considerando variantes como oraciones pasivas,
    verbos frasales y construcciones idiomáticas.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb_lemma = root.lemma  # Utilizar el lema del verbo
            subject = None
            object_ = None
            complement = None
            predicative = None

            # Manejar verbos frasales (simplificado)
            if root.deprel == 'prt':
                verb_lemma += " " + sentence.words[root.id - 1].lemma

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'iobj':
                        complement = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'acomp':
                        predicative = sentence.words[dep[2].id - 1].lemma

            # Manejar oraciones pasivas
            is_passive = False
            for dep in sentence.dependencies:
                if dep[0].id == root.id and dep[1] == 'auxpass':
                    is_passive = True
                    break

            if is_passive:
                if subject is None:  # Buscar sujeto pasivo
                    for dep in sentence.dependencies:
                        if dep[0].id == root.id and dep[1] == 'nsubjpass':
                            subject = sentence.words[dep[2].id - 1].lemma
                if object_ is None:  # Buscar objeto en voz pasiva
                    for dep in sentence.dependencies:
                        if dep[0].id == root.id and dep[1] == 'obj':
                            object_ = sentence.words[dep[2].id - 1].lemma

            if subject and object_:
                nvnpn_structure = {
                    "verb": verb_lemma,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures_lemmas.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

"""# Explicación de las mejoras:

Uso del lema del verbo: Se utiliza root.lemma en lugar de root.text para obtener el lema del verbo principal, lo cual puede mejorar la identificación de variaciones del mismo verbo (por ejemplo, "is", "are", "was", "were" se reducen a "be").
Uso del lema en la búsqueda de sujeto y objeto: Se utiliza sentence.words[dep[2].id - 1].lemma para obtener el lema del sujeto y el objeto, lo que permite identificar variaciones de las palabras.
Manejo de oraciones pasivas mejorado: Se verifica si se ha encontrado un sujeto en el caso de oraciones pasivas antes de buscar el objeto.
"""

print("Q: Por completitud, en el fichero de salida se incluye la estructura NVNPN, Expandir la definición de NVNPN para el análisis sintáctico. y conversion a archivo json ")

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo)
    de un texto utilizando Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa
        una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.lemma
            subject = None
            object_ = None
            complement = None
            predicative = None

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'iobj':  # Complemento indirecto
                        complement = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'acomp':  # Complemento predicativo
                        predicative = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'nsubjpass':  # Sujeto en voz pasiva
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'obj' and is_passive(sentence, root):  # Objeto en voz pasiva
                        object_ = sentence.words[dep[2].id - 1].lemma

            if subject and object_:
                nvnpn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

def is_passive(sentence, root):
    """
    Verifica si la oración está en voz pasiva.

    Args:
        sentence: La oración procesada por Stanza.
        root: El nodo raíz de la oración.

    Returns:
        True si la oración está en voz pasiva, False en caso contrario.
    """
    for dep in sentence.dependencies:
        if dep[0].id == root.id and dep[1] == 'auxpass':
            return True
    return False

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

"""Importaciones: Se importan las bibliotecas stanza y json.
Función extract_nvnpn_structures:
Se crea un pipeline de Stanza con los procesadores necesarios.
Se itera sobre cada oración en el documento.
Se identifica el nodo raíz (el verbo principal).
Se inicializan variables para almacenar el verbo, sujeto, objeto, complemento y predicativo.
Se itera sobre las dependencias del verbo:
Se identifican los elementos clave de la estructura NVNPN (sujeto, objeto, complemento, predicativo) utilizando las etiquetas de dependencia correspondientes.
Se maneja la voz pasiva verificando la presencia de auxpass en las dependencias.
Si se encuentran sujeto y objeto, se crea un diccionario con la estructura NVNPN y se agrega a la lista.
Función is_passive:
Verifica si la oración está en voz pasiva buscando la etiqueta auxpass en las dependencias del verbo.
Ejemplo de uso:
Se define el texto de ejemplo.
Se llama a la función extract_nvnpn_structures para obtener las estructuras NVNPN.
Se guarda la lista de estructuras NVNPN en un archivo JSON.
Se imprime las estructuras NVNPN encontradas en la consola.
Explicación de las mejoras:

Manejo de oraciones pasivas: Se incluye la lógica para identificar y manejar oraciones pasivas, buscando la etiqueta auxpass y ajustando la búsqueda del sujeto y el objeto en consecuencia.
Consideración de complementos: Se incluye la identificación de complementos indirectos (iobj) y complementos predicativos (acomp).
Este código proporciona una implementación más completa de la extracción de estructuras NVNPN, considerando algunos casos más complejos como las oraciones pasivas y los complementos.

Este código aún puede mejorarse para manejar casos más complejos, como verbos frasales, construcciones con verbos copulativos (ser, estar), y otras estructuras sintácticas más avanzadas.
Puedes experimentar con diferentes etiquetas de dependencia y ajustar la lógica según tus necesidades específicas.
El análisis sintáctico puede ser complejo, y la precisión de la extracción dependerá de la complejidad del texto y la calidad del análisis realizado por Stanza.
"""

print("R: Extraccion de estructuras NVNPN en stanza con diferentes etiquetas de dependencia y ajustar logica para la exttraccion y conversion a archivo json ")

import stanza
import json

def extract_nvnpn_structures(text):
    """
    Extrae estructuras NVNPN (verbo-sujeto-objeto-complemento-predicativo)
    de un texto utilizando Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa
        una estructura NVNPN encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvnpn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.lemma
            subject = None
            object_ = None
            complement = None
            predicative = None

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] in ['nsubj', 'nsubjpass', 'csubj', 'expl']:
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] in ['dobj', 'attr', 'ccomp', 'xcomp']:
                        object_ = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] in ['iobj', 'obl']:
                        complement = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] in ['acomp', 'appos']:
                        predicative = sentence.words[dep[2].id - 1].lemma

            if subject and object_:
                nvnpn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_,
                    "complement": complement,
                    "predicative": predicative
                }
                nvnpn_structures.append(nvnpn_structure)

    return nvnpn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvnpn_structures = extract_nvnpn_structures(text)

# Guardar en un archivo JSON
with open('nvnpn_structures_expanded.json', 'w') as f:
    json.dump(nvnpn_structures, f, indent=4)

print(nvnpn_structures)

"""

Etiquetas de dependencia expandidas:
Se han agregado etiquetas de dependencia adicionales para capturar una gama más amplia de estructuras:
csubj: Sujeto complementario.
expl: Expletive (e.g., "it" en "it is raining").
ccomp: Complemento del verbo (cláusulas subordinadas).
xcomp: Complemento del verbo (para verbos como "try", "want").
obl: Objeto oblicuo (complemento circunstancial).
appos: Aposición (nombre que explica o reitera otro nombre).
Manejo de oraciones pasivas: Se mantiene la lógica para manejar oraciones pasivas utilizando la etiqueta nsubjpass."""

print("S: Una estructura NVN del archivo input.txt . y conversion a archivo json ")

import stanza
import json

def extract_nvn_structures(text):
    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.lemma
            subject = None
            object_ = None

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] in ['nsubj', 'nsubjpass', 'csubj']:  # Sujeto
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] in ['dobj', 'attr', 'ccomp']:  # Objeto o predicativo
                        object_ = sentence.words[dep[2].id - 1].lemma

            if subject and object_:
                nvn_structure = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_
                }
                [].append(nvn_structure)

    return []
# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nvn_structures = extract_nvn_structures(text)

# Guardar en un archivo JSON
with open('nvn_structures.json', 'w') as f:
    json.dump(nvn_structures, f, indent=4)

print(nvn_structures)

"""xplicación de los cambios:

Más etiquetas de dependencia: Se han agregado csubj para sujetos complementarios y ccomp para complementos del verbo.
Mayor flexibilidad: Se utiliza una lista para incluir múltiples etiquetas de dependencia, permitiendo una mayor personalización.
"""

print("T: Una estructura NVN para extracccion del  archivo input.txt, utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

def extract_nvn_structure(sentence):
    """
    Extrae una estructura NVN (verbo-sujeto-objeto) de una oración utilizando Stanza.

    Args:
        sentence: Una oración procesada por Stanza.

    Returns:
        Un diccionario representando la estructura NVN encontrada,
        o None si no se encuentra una estructura válida.
    """

    root = None
    subject = None
    object_ = None

    for word in sentence.words:
        if word.head == 0:
            root = word
            break

    if root:
        verb = root.lemma

        for dep in sentence.dependencies:
            if dep[0].id == root.id:
                if dep[1] in ['nsubj', 'nsubjpass', 'csubj']:
                    subject = sentence.words[dep[2].id - 1].lemma
                elif dep[1] in ['dobj', 'attr']:
                    object_ = sentence.words[dep[2].id - 1].lemma

        if subject and object_:
            return {
                "verb": verb,
                "subject": subject,
                "object": object_
            }

    return None

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
doc = nlp(text)
nvn_structures = []

for sent in doc.sentences:
    nvn_structure = extract_nvn_structure(sent)
    if nvn_structure:
        nvn_structures.append(nvn_structure)

# Guardar en un archivo JSON
with open('nvn_structures.json', 'w') as f:
    json.dump(nvn_structures, f, indent=4)

print(nvn_structures)

print("U: Una estructura NV para extracccion del  archivo input.txt, utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

def extract_nvn_structures(text):
    """
    Extrae estructuras NV (verbo-sujeto) de un texto utilizando Stanza.

    Args:
        text: El texto de entrada.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una estructura NV encontrada.
    """

    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
    doc = nlp(text)
    nvn_structures = []

    for sentence in doc.sentences:
        root = None
        for word in sentence.words:
            if word.head == 0:
                root = word
                break

        if root:
            verb = root.lemma
            subject = None

            for dep in sentence.dependencies:
                if dep[0].id == root.id:
                    if dep[1] in ['nsubj', 'nsubjpass', 'csubj']:
                        subject = sentence.words[dep[2].id - 1].lemma

            if subject:
                nvn_structure = {
                    "verb": verb,
                    "subject": subject
                }
                nvn_structures.append(nvn_structure)

    return nvn_structures

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nv_structures = extract_nvn_structures(text)

# Guardar en un archivo JSON
with open('nv_structures.json', 'w') as f:
    json.dump(nv_structures, f, indent=4)

print(nvn_structures)

"""Cambios clave:

Eliminación del objeto: Se ha eliminado la parte del código que busca el objeto directo (dobj o attr), ya que solo queremos extraer estructuras NV.
Simplificación de la estructura: La estructura del diccionario ahora solo contiene el verbo y el sujeto.
Explicación:

Este código busca el verbo principal de cada oración y su sujeto, ignorando el objeto. Las etiquetas de dependencia nsubj, nsubjpass y csubj se utilizan para identificar al sujeto.
"""

print("V: Identificar Una misma oración puede contener varias proposiciones, archivo input.txt, utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

def extract_propositions(sentence):
    """
    Extrae proposiciones de una oración utilizando Stanza.

    Args:
        sentence: Una oración procesada por Stanza.

    Returns:
        Una lista de diccionarios, donde cada diccionario representa una proposición
        encontrada en la oración.
    """

    propositions = []
    for word in sentence.words:
        if word.head == 0:  # Verificar si es el verbo raíz
            verb = word.lemma
            subject = None
            object_ = None

            for dep in sentence.dependencies:
                if dep[0].id == word.id:
                    if dep[1] == 'nsubj':
                        subject = sentence.words[dep[2].id - 1].lemma
                    elif dep[1] == 'dobj':
                        object_ = sentence.words[dep[2].id - 1].lemma

            if subject and object_:
                proposition = {
                    "verb": verb,
                    "subject": subject,
                    "object": object_
                }
                propositions.append(proposition)

    return propositions

# Ejemplo de uso
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
doc = nlp(text)
all_propositions = []

for sent in doc.sentences:
    sentence_propositions = extract_propositions(sent)
    if sentence_propositions:
        all_propositions.extend(sentence_propositions)

# Guardar en un archivo JSON
with open('all_propositions.json', 'w') as f:
    json.dump(all_propositions, f, indent=4)

print(all_propositions)

print("W: Debemos contar proposiciones (ej. \"she, do, it\"), archivo input.txt, utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para extraer proposiciones y contarlas
def extraer_proposiciones_y_contar(sentence):
    proposiciones = []
    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']:
            proposiciones.append(word.text)
    return proposiciones, len(proposiciones)

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y extraer proposiciones
for sentence in doc.sentences:
    proposiciones, num_proposiciones = extraer_proposiciones_y_contar(sentence)
    resultados.append({
        "sentence": sentence.text,
        "propositions": proposiciones,
        "num_propositions": num_proposiciones
    })

# Imprimir la salida en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON (opcional)
with open("proposiciones_contadas.json", "w") as f:
     json.dump(resultados, f, indent=4)

"""
Explicación de la modificación:

Impresión directa:
Hemos agregado la línea print(json.dumps(resultados, indent=4)) fuera del bucle.
json.dumps() convierte la lista de diccionarios resultados en una cadena JSON formateada.
El argumento indent=4 añade sangría para mejorar la legibilidad del JSON.
Esta línea imprime directamente el JSON en la consola.
¿Por qué imprimir directamente y no solo guardar en un archivo?

Flexibilidad: Puedes ver los resultados inmediatamente en la consola sin tener que abrir un archivo.
Pruebas rápidas: Es útil para pruebas rápidas y depuración.
Integración en otros scripts: Puedes capturar la salida y utilizarla en otros scripts o aplicaciones."""

print("Y: los ejemplos no se ha tratado la pasiva, Tratar las voz pasiva en archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para extraer proposiciones y contarlas, considerando voz pasiva
def extraer_proposiciones_y_contar(sentence):
    proposiciones = []
    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']:
            proposiciones.append(word)  # Append the Word object, not word.text

    # Identificar y manejar voz pasiva (simplificado)
    for i, word in enumerate(proposiciones):
        if word.lemma == "be" and i + 1 < len(proposiciones) and proposiciones[i + 1].text in ["chased", "eaten"]:  # Access text attribute here
            try:
                agente_index = [w.text for w in proposiciones].index("by") + 1  # Find "by" in the list of text
                if agente_index < len(proposiciones):
                    proposiciones[i:i+3] = [proposiciones[agente_index], proposiciones[i+1], proposiciones[i-1]]
            except ValueError:
                pass  # No se encontró "by"

    return [word.text for word in proposiciones], len(proposiciones) # Extract text at the end

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y extraer proposiciones
for sentence in doc.sentences:
    proposiciones, num_proposiciones = extraer_proposiciones_y_contar(sentence)
    resultados.append({
        "sentence": sentence.text,
        "propositions": proposiciones,
        "num_propositions": num_proposiciones
    })

# Imprimir la salida en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON
with open("proposiciones_con_pasiva.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Proposiciones extraídas y guardadas en 'proposiciones_con_pasiva.json'")

print("Z: Analisis de los pronombres personales, puede ser útil sustituirlos por su categoría o incluso por PERSON para agrupar más las proposiciones en archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para extraer proposiciones y analizar pronombres
def extraer_proposiciones_con_pronombres(sentence):
    proposiciones = []
    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PROPN', 'ADJ', 'ADV']:
            if word.upos == 'PRON':
                # Sustituir pronombre por su categoría (ejemplo)
                proposiciones.append(word.feats['PronType'])
                # Sustituir pronombre por "PERSON" (agrupación)
                # proposiciones.append("PERSON")
            else:
                proposiciones.append(word.text)
    return proposiciones

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y extraer proposiciones
for sentence in doc.sentences:
    proposiciones = extraer_proposiciones_con_pronombres(sentence)
    resultados.append({
        "sentence": sentence.text,
        "propositions": proposiciones
    })

# Imprimir la salida en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON
with open("proposiciones_con_pronombres.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Proposiciones extraídas y guardadas en 'proposiciones_con_pronombres.json'")

"""# Explicación:

Extracción de Proposiciones:

Se itera sobre cada palabra en la oración.
Se incluyen en la lista de proposiciones las palabras con las siguientes etiquetas POS:
VERB: Verbos
NOUN: Sustantivos
PROPN: Nombres propios
ADJ: Adjetivos
ADV: Adverbios
Análisis de Pronombres:

Si la palabra es un pronombre (word.upos == 'PRON'):
Se accede a las características del pronombre utilizando word.feats['PronType']. Esto devuelve la categoría del pronombre (por ejemplo, 'Prs', 'Refl').
Se agrega la categoría del pronombre a la lista de proposiciones.
Opción alternativa: Se puede agregar simplemente "PERSON" a la lista de proposiciones para agrupar todos los pronombres.
Generación de JSON:

Se crea una lista de diccionarios, donde cada diccionario contiene la oración original y la lista de proposiciones.
Se imprime la lista de diccionarios en formato JSON utilizando json.dumps().
Se guarda la lista de diccionarios en un archivo JSON utilizando json.dump().
"""

print("AA:  la sustitución que debemos hacer de los nombres propios: \"She sleeps\" y \"Susan sleeps\" ejemplo en archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para extraer proposiciones y sustituir nombres propios
def extraer_proposiciones_con_sustitucion_nombres(sentence):
    proposiciones = []
    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'ADJ', 'ADV']:
            if word.upos == 'PROPN':
                # Sustituir nombres propios por un marcador (por ejemplo, "PERSON")
                proposiciones.append("PERSON")
            else:
                proposiciones.append(word.text)
    return proposiciones

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y extraer proposiciones
for sentence in doc.sentences:
    proposiciones = extraer_proposiciones_con_sustitucion_nombres(sentence)
    resultados.append({
        "sentence": sentence.text,
        "propositions": proposiciones
    })

# Imprimir la salida en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON
with open("proposiciones_con_sustitucion_nombres.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Proposiciones extraídas y guardadas en 'proposiciones_con_sustitucion_nombres.json'")

"""Explicación:

Extracción de Proposiciones:

Se itera sobre cada palabra en la oración.
Se incluyen en la lista de proposiciones las palabras con las siguientes etiquetas POS:
VERB: Verbos
NOUN: Sustantivos
ADJ: Adjetivos
ADV: Adverbios
Sustitución de Nombres Propios:

Si la palabra es un nombre propio (word.upos == 'PROPN'):
Se reemplaza el nombre propio por un marcador (en este caso, "PERSON"). Esto permite agrupar todas las ocurrencias de nombres propios bajo un mismo marcador.

"""

print("BB:  Analisis de sentimientos, en cadena y por palkabra  del  archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

!pip install textblob

import stanza
import json
from textblob import TextBlob

# Download the Stanza model for English (if not already downloaded)
stanza.download('en')

# Initialize the Stanza pipeline
nlp = stanza.Pipeline('en')

# Define the text
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Process the text with Stanza
doc = nlp(text)

# Perform sentiment analysis using TextBlob
text_content = " ".join([sentence.text for sentence in doc.sentences])
blob = TextBlob(text_content)
sentiment = blob.sentiment

# Extract named entities
entities = [{'text': ent.text, 'type': ent.type} for ent in doc.ents]

# Create a dictionary to store the results
results = {
    "sentiment": {
        "polarity": sentiment.polarity,
        "subjectivity": sentiment.subjectivity
    },
    "named_entities": entities
}

# Print the results in JSON format
print(json.dumps(results, indent=4))

# Save the results to a JSON file
with open("nlp_results.json", "w") as f:
    json.dump(results, f, indent=4)

print("Results saved to nlp_results.json")

"""¡Claro! Con gusto te explico el código modificado y los resultados que obtendrás:

Explicación del Código Modificado:

El código que te presenté combina las capacidades de Stanza y TextBlob para realizar un análisis más completo del texto. Aquí te detallo cada parte:

Importación de bibliotecas:

stanza: Se utiliza para el procesamiento del lenguaje natural, incluyendo el reconocimiento de entidades nombradas.
json: Permite guardar los resultados en formato JSON, que es fácil de leer y procesar por máquinas.
textblob: Se utiliza para el análisis de sentimientos.
Descarga y inicialización del modelo de Stanza:

Se descarga el modelo de Stanza para el idioma inglés, si aún no está disponible.
Se inicializa el pipeline de Stanza para procesar el texto en inglés.
Definición del texto:

Se define el texto de entrada que se desea analizar.
Procesamiento del texto con Stanza:

Se procesa el texto con Stanza para obtener información sobre las oraciones, palabras y entidades nombradas.
Análisis de sentimientos con TextBlob:

Se unen todas las oraciones del texto en una sola cadena.
Se crea un objeto TextBlob con esta cadena para realizar el análisis de sentimientos.
Se obtiene la polaridad (si es positiva, negativa o neutral) y la subjetividad (qué tan subjetivo es el texto) del sentimiento.
Extracción de entidades nombradas:

Se itera sobre las entidades nombradas identificadas por Stanza.
Para cada entidad, se extrae el texto y su tipo (persona, lugar, fecha, etc.).
Se crea una lista de diccionarios para almacenar la información de cada entidad.
Creación del diccionario de resultados:

Se crea un diccionario para almacenar los resultados del análisis de sentimientos y del reconocimiento de entidades nombradas.
Impresión y guardado de los resultados:

Se imprime el diccionario de resultados en formato JSON, lo que facilita su lectura.
Se guarda el diccionario de resultados en un archivo JSON para su posterior uso.
"""

import stanza
import json
from textblob import TextBlob

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para realizar análisis de sentimiento por palabra
def analizar_sentimiento_por_palabra(sentence):
    resultados_palabras = []
    for word in sentence.words:
        blob = TextBlob(word.text)
        polaridad = blob.sentiment.polarity
        subjetividad = blob.sentiment.subjectivity
        resultados_palabras.append({"word": word.text, "polarity": polaridad, "subjectivity": subjetividad})
    return resultados_palabras

# Define the missing function to analyze sentiment for a sentence
def analizar_sentimiento(sentence):
    """
    Realiza análisis de sentimiento para una oración utilizando TextBlob.

    Args:
        sentence: Una oración procesada por Stanza.

    Returns:
        Una tupla con la polaridad y subjetividad de la oración.
    """
    blob = TextBlob(sentence.text)
    polaridad = blob.sentiment.polarity
    subjetividad = blob.sentiment.subjectivity
    return polaridad, subjetividad

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y realizar análisis de sentimiento
for sentence in doc.sentences:
    polaridad_general, subjetividad_general = analizar_sentimiento(sentence) # Call the defined function
    palabras_con_sentimiento = analizar_sentimiento_por_palabra(sentence)
    resultados.append({
        "sentence": sentence.text,
        "polarity": polaridad_general,
        "subjectivity": subjetividad_general,
        "words": palabras_con_sentimiento
    })

# Imprimir la salida en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON
with open("analisis_sentimientos_por_palabra.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Análisis de sentimientos por palabra guardado en 'analisis_sentimientos_por_palabra.json'")

"""

Explicación:

sentence: Esta clave representa la oración original del texto que se está analizando. En este caso, la oración es "Jane bought me these books.".

polarity: Esta clave indica la polaridad del sentimiento de la oración completa. Un valor de 0.0 significa que la oración se considera neutral en términos de sentimiento (ni positiva ni negativa).

subjectivity: Esta clave indica la subjetividad de la oración. Un valor de 0.0 sugiere que la oración es objetiva, es decir, que presenta información factual o descriptiva sin expresar opiniones o emociones personales.

words: Esta clave contiene una lista de diccionarios, donde cada diccionario representa una palabra de la oración y su análisis de sentimiento.

word: Esta clave dentro del diccionario de palabras contiene el texto de la palabra individual. En el ejemplo, la primera palabra es "Jane".
polarity: Esta clave indica la polaridad de la palabra individual. En este caso, la palabra "Jane" se considera neutral en términos de sentimiento.
subjectivity: Esta clave indica la subjetividad de la palabra individual. En este caso, la palabra "Jane" se considera objetiva, ya que es un nombre propio y no expresa ninguna opinión o emoción.
En resumen:

Este fragmento de JSON muestra el análisis de sentimientos de la oración "Jane bought me these books.". La oración se considera neutral y objetiva en términos de sentimiento general. Además, se proporciona un análisis de sentimientos a nivel de palabra, donde cada palabra se clasifica como neutral y objetiva."""

print("CC: Extraccion de proposiciones  en español, deteccion de la voz pasiva y resultados del  archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para español (si no está descargado)
stanza.download('es')

# Inicializar el pipeline de Stanza para español
nlp = stanza.Pipeline('es')

# Definir el texto
text = """Jane compró estos libros para mí.
Jane compró un libro para mí.
Ella le envió un mensaje. Gracias.
Ella duerme.
Yo duermo mucho.
Nací en Madrid.
El gato fue perseguido por el perro.
Nací en Madrid en 1995.
De todo esto, algo bueno saldrá.
Susan se fue después del ensayo. Lo hizo bien."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para extraer proposiciones y identificar agentes en voz pasiva
def extraer_proposiciones(sentence):
    proposiciones = []
    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']:
            proposiciones.append(word.text)

    # Identificar voz pasiva y extraer agente (mejorado)
    for i in range(len(proposiciones) - 2):
        if proposiciones[i] in ["ser", "estar"] and proposiciones[i+1] in ["perseguido", "comido"]:
            try:
                agente_index = proposiciones.index("por") + 1
                if agente_index < len(proposiciones):
                    proposiciones[i:i+3] = [proposiciones[agente_index], proposiciones[i+1], proposiciones[i-1]]
            except ValueError:
                pass  # No se encontró "por"

    return proposiciones

# Crear una lista de diccionarios para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y extraer proposiciones
for sentence in doc.sentences:
    proposiciones = extraer_proposiciones(sentence)
    resultados.append({
        "oracion": sentence.text,
        "proposiciones": proposiciones
    })

# Imprimir los resultados en formato JSON
print(json.dumps(resultados, indent=4, ensure_ascii=False))

# Guardar los resultados en un archivo JSON
with open("proposiciones_espanol.json", "w", encoding="utf-8") as f:
    json.dump(resultados, f, indent=4, ensure_ascii=False)

print("Proposiciones extraídas y guardadas en 'proposiciones_espanol.json'")

"""# Explicación:

Descarga y configuración: Descarga el modelo de Stanza para español y configura el pipeline para procesar texto en español.
Definición del texto: Se define el texto de entrada.
Procesamiento con Stanza: Se procesa el texto con Stanza para obtener información sobre las palabras y su función gramatical.
Extracción de proposiciones:
La función extraer_proposiciones identifica palabras relevantes (verbos, sustantivos, etc.) y las agrega a una lista de proposiciones.
Mejora: La detección de voz pasiva ahora busca verbos como "ser" o "estar" seguidos de participios pasados, y extrae el agente si encuentra la preposición "por".
Creación de resultados: Se crea una lista de diccionarios, donde cada diccionario contiene la oración original y las proposiciones extraídas.
Salida: Los resultados se imprimen en formato JSON y se guardan en un archivo JSON con codificación UTF-8 para manejar caracteres especiales en español.
"""

print("DD: Analisis completo y complejo de oraciones  y resultados del  archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Descargar el modelo de Stanza para inglés (si aún no está descargado)
stanza.download('en')

# Inicializar el pipeline de Stanza
nlp = stanza.Pipeline('en')

# Definir el texto
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Procesar el texto con Stanza
doc = nlp(text)

# Función para realizar un análisis más complejo
def analizar_oracion(sentence):
    """
    Realiza un análisis más complejo de una oración, incluyendo:
    - Extracción de proposiciones.
    - Identificación de voz pasiva.
    - Análisis de entidades nombradas.
    - Dependencias sintácticas (sujeto-verbo-objeto).

    Args:
        sentence: La oración a analizar.

    Returns:
        Un diccionario con los resultados del análisis.
    """
    proposiciones = []
    dependencias = []

    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']:
            proposiciones.append(word.text)

        # Extraer dependencias sintácticas
        dependencias.append({
            "word": word.text,
            "pos": word.upos,
            "head": word.head,
            "deprel": word.deprel
        })

    # Identificar voz pasiva
    for i in range(len(proposiciones) - 2):
        if proposiciones[i] in ["be", "been", "is", "was"] and proposiciones[i+1] in ["chased", "eaten"]:
            try:
                agente_index = proposiciones.index("by") + 1
                if agente_index < len(proposiciones):
                    proposiciones[i:i+3] = [proposiciones[agente_index], proposiciones[i+1], proposiciones[i-1]]
            except ValueError:
                pass  # No se encontró "by"

    return {
        "oracion": sentence.text,
        "proposiciones": proposiciones,
        "dependencias": dependencias,
        "entidades": [{'text': ent.text, 'type': ent.type} for ent in sentence.ents]
    }

# Crear una lista para almacenar los resultados
resultados = []

# Iterar a través de las oraciones y realizar el análisis
for sentence in doc.sentences:
    resultados.append(analizar_oracion(sentence))

# Imprimir los resultados en formato JSON
print(json.dumps(resultados, indent=4))

# Guardar los resultados en un archivo JSON
with open("analisis_completo.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Análisis completo guardado en 'analisis_completo.json'")

"""
# Explicación:

Importación de bibliotecas: Se importan las bibliotecas necesarias: stanza para el procesamiento del lenguaje natural y json para manejar archivos JSON.

Descarga e inicialización de Stanza: Se descarga el modelo de Stanza para inglés y se inicializa el pipeline.

Procesamiento del texto: Se procesa el texto con Stanza para obtener información detallada sobre cada oración.

Función analizar_oracion():

Esta función realiza un análisis más complejo de cada oración:
Extracción de proposiciones: Se extraen las palabras relevantes (verbos, sustantivos, etc.).
Identificación de voz pasiva: Se busca la presencia de verbos auxiliares y participios pasivos para identificar oraciones pasivas y reordenar las proposiciones.
Extracción de dependencias sintácticas: Se extraen las relaciones entre las palabras en la oración (por ejemplo, sujeto-verbo, verbo-objeto).
Extracción de entidades nombradas: Se identifican y extraen las entidades nombradas (personas, lugares, organizaciones, etc.).
Iteración y análisis: Se itera a través de cada oración en el documento y se llama a la función analizar_oracion() para obtener los resultados del análisis.

Guardado de resultados: Los resultados se almacenan en una lista de diccionarios y se guardan en un archivo JSON.

Salida:

El archivo JSON resultante contendrá información más detallada sobre cada oración, incluyendo:

La oración original.
Una lista de proposiciones extraídas.
Una lista de dependencias sintácticas entre las palabras.
Una lista de entidades nombradas identificadas.
Este análisis más complejo proporciona una visión más profunda del texto, permitiendo identificar no solo las proposiciones, sino también las relaciones entre las palabras y las entidades presentes en el texto."""

import stanza
import json
import re

# Download the Stanza model for English (if not already downloaded)
stanza.download('en')

# Initialize the Stanza pipeline
nlp = stanza.Pipeline('en')

# Define the text
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Process the text with Stanza
doc = nlp(text)

def analyze_sentence(sentence):
    """
    Performs a more complex analysis of a sentence, including:
    - Proposition extraction
    - Named Entity Recognition
    - Dependency parsing
    - Sentiment analysis (basic)
    - Extracting dates and locations
    """
    propositions = []
    dependencies = []
    entities = []
    dates = []
    locations = []

    for word in sentence.words:
        if word.upos in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']:
            propositions.append(word.text)

        dependencies.append({
            "word": word.text,
            "pos": word.upos,
            "head": word.head,
            "deprel": word.deprel
        })

    for ent in sentence.ents:
        entities.append({"text": ent.text, "type": ent.type})

    # Extract dates (basic example)
    date_pattern = r'\d{4}-\d{2}-\d{2}|\d{2}/\d{2}/\d{4}|\d{4}'
    dates = re.findall(date_pattern, sentence.text)

    # Extract locations (basic example)
    location_pattern = r'[A-Z][a-z]+'  # Simple pattern for place names
    locations = re.findall(location_pattern, sentence.text)

    # Basic sentiment analysis (using TextBlob)
    blob = TextBlob(sentence.text)
    sentiment = {
        "polarity": blob.sentiment.polarity,
        "subjectivity": blob.sentiment.subjectivity
    }

    return {
        "sentence": sentence.text,
        "propositions": propositions,
        "dependencies": dependencies,
        "entities": entities,
        "dates": dates,
        "locations": locations,
        "sentiment": sentiment
    }

# Create a list to store the results
results = []

# Iterate through the sentences and perform analysis
for sentence in doc.sentences:
    results.append(analyze_sentence(sentence))

# Print the results in JSON format
print(json.dumps(results, indent=4))

# Save the results to a JSON file
with open("complex_analysis.json", "w") as f:
    json.dump(results, f, indent=4)

print("Complex analysis results saved to 'complex_analysis.json'")

"""# Explicación en Español:

Este código realiza un análisis más profundo del texto, combinando las capacidades de Stanza con expresiones regulares para extraer información adicional como fechas, ubicaciones y un análisis de sentimiento básico.

Análisis de Proposiciones: Identifica las palabras clave en cada oración.
Reconocimiento de Entidades: Identifica nombres de personas, lugares, organizaciones, etc.
Análisis de Dependencias: Muestra las relaciones gramaticales entre las palabras.
Análisis de Sentimiento: Utiliza TextBlob para determinar la polaridad y la subjetividad de cada oración.
Extracción de Fechas y Ubicaciones: Emplea expresiones regulares para encontrar patrones comunes de fechas y nombres de lugares.
Resultados:

El resultado final es un archivo JSON que contiene un análisis detallado de cada oración, incluyendo:

Oración original: El texto completo de la oración.
Proposiciones: Las palabras clave de la oración.
Dependencias: Las relaciones gramaticales entre las palabras.
Entidades: Las entidades nombradas encontradas.
Fechas: Las fechas extraídas de la oración.
Ubicaciones: Los nombres de lugares encontrados.
Sentimiento: La polaridad y la subjetividad de la oración.
"""

print("EE: Analisis de sentimiento mas complejo  y resultados del  archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

!pip install vaderSentiment # Install the missing library

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import stanza
import json
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Download the Stanza model for English (if not already downloaded)
stanza.download('en')

# Initialize the Stanza pipeline
nlp = stanza.Pipeline('en')

# Define the text
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Process the text with Stanza
doc = nlp(text)

# Initialize sentiment analyzers
vader_analyzer = SentimentIntensityAnalyzer()

def analyze_sentence(sentence):
    """
    Performs a more complex analysis of a sentence, including:
    - Sentiment analysis using TextBlob and VADER
    """
    textblob_sentiment = TextBlob(sentence.text).sentiment
    vader_sentiment = vader_analyzer.polarity_scores(sentence.text)

    return {
        "sentence": sentence.text,
        "textblob": {
            "polarity": textblob_sentiment.polarity,
            "subjectivity": textblob_sentiment.subjectivity
        },
        "vader": {
            "neg": vader_sentiment['neg'],
            "neu": vader_sentiment['neu'],
            "pos": vader_sentiment['pos'],
            "compound": vader_sentiment['compound']
        }
    }

# Create a list to store the results
results = []

# Iterate through the sentences and perform analysis
for sentence in doc.sentences:
    results.append(analyze_sentence(sentence))

# Print the results in JSON format
print(json.dumps(resultados, indent=4))

# Save the results to a JSON file
with open("advanced_sentiment_analysis.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Advanced sentiment analysis results saved to 'advanced_sentiment_analysis.json'")

"""Importaciones:

stanza: Para procesamiento del lenguaje natural con Stanza.
json: Para trabajar con archivos JSON.
TextBlob: Para análisis de sentimientos básico.
SentimentIntensityAnalyzer: Para utilizar el analizador de sentimientos VADER.
Descarga e inicialización de Stanza: Se descarga el modelo de Stanza para inglés y se inicializa el pipeline.

Definición del texto: Se define el texto de entrada.

Procesamiento con Stanza: Se procesa el texto con Stanza para obtener información sobre las oraciones.

Análisis de Sentimiento:

Se utiliza TextBlob para obtener la polaridad y la subjetividad del sentimiento.
Se utiliza vader_analyzer para obtener las puntuaciones de sentimiento VADER (negativo, neutro, positivo, compuesto).
Creación de resultados: Se crea un diccionario para almacenar los resultados de cada oración, incluyendo el texto de la oración y los resultados de ambos analizadores de sentimiento.

Iteración y análisis: Se itera a través de cada oración y se realiza el análisis de sentimiento para cada una.

Impresión y guardado de resultados: Se imprime y guarda los resultados en formato JSON en un archivo.

Este código proporciona un análisis de sentimiento más avanzado al utilizar tanto TextBlob como VADER, lo que puede ofrecer una perspectiva más completa y robusta sobre el sentimiento expresado en cada oración.

Consideraciones:

VADER: VADER es un analizador de sentimientos específico para el inglés y puede ser más preciso que TextBlob para el idioma inglés.
Combinación de métodos: Combinar los resultados de diferentes analizadores de sentimientos puede mejorar la precisión general.
Contexto: El análisis de sentimientos puede verse afectado por el contexto de la oración y del texto completo.
"""

print("FF: Analisis grafico de dependencias del  archivo input.txt .  utilizar stanza y modificaciones . y conversion a archivo json ")

import stanza
import json

# Download the Stanza model for English (if not already downloaded)
stanza.download('en')

# Initialize the Stanza pipeline
nlp = stanza.Pipeline('en')

# Define the text
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Process the text with Stanza
doc = nlp(text)

def analyze_sentence(sentence):
    """
    Performs an analysis of a sentence, including:
    - Dependency parsing
    - Creating a simple dependency graph

    Returns:
        A dictionary containing the sentence text, dependencies, and a basic graph representation.
    """
    dependencies = []
    graph_edges = []

    for word in sentence.words:
        dependencies.append({
            "word": word.text,
            "pos": word.upos,
            "head": word.head,
            "deprel": word.deprel
        })

        if word.head != 0:  # Exclude root node
            graph_edges.append((word.id, word.head))

    return {
        "sentence": sentence.text,
        "dependencies": dependencies,
        "graph_edges": graph_edges
    }

# Create a list to store the results
results = []

# Iterate through the sentences and perform analysis
for sentence in doc.sentences:
    results.append(analyze_sentence(sentence))

# Print the results in JSON format
print(json.dumps(resultados, indent=4))

# Save the results to a JSON file
with open("dependency_graph_analysis.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Dependency graph analysis results saved to 'dependency_graph_analysis.json'")

"""Explicación:

Análisis de Dependencias: Se extraen las relaciones gramaticales entre las palabras de cada oración utilizando la información proporcionada por Stanza (word.head, word.deprel).
Creación de Grafo Simple: Se crea una lista de tuplas graph_edges que representan las conexiones entre las palabras en la oración. Cada tupla representa una arista del grafo, donde el primer elemento es el ID del nodo hijo y el segundo elemento es el ID del nodo padre.
Salida: El resultado final incluye una lista de diccionarios, donde cada diccionario contiene la oración original, las dependencias sintácticas y la lista de aristas del grafo.
Nota:

Este ejemplo proporciona una representación básica del grafo de dependencias.
Para una visualización más completa, puedes utilizar la biblioteca networkx para construir y visualizar el grafo utilizando las aristas extraídas.
Este análisis puede servir como base para realizar tareas más avanzadas, como:
Extracción de frases nominales: Identificando los núcleos de las frases nominales.
Análisis de roles semánticos: Determinando el rol que desempeña cada palabra en la oración (sujeto, objeto, verbo, etc.).
Generación de resúmenes: Identificando las oraciones más importantes y las relaciones entre las ideas principales.
"""

!pip install networkx

!pip install matplotlib

import matplotlib.pyplot as plt

import stanza
import json
import networkx as nx
import matplotlib.pyplot as plt

# Download the Stanza model for English (if not already downloaded)
stanza.download('en')

# Initialize the Stanza pipeline
nlp = stanza.Pipeline('en')

# Define the text
text = """Jane bought me these books.
Jane bought a book for me.
She dropped a line to him. Thank you.
She sleeps.
I sleep a lot.
I was born in Madrid.
the cat was chased by the dog.
I was born in Madrid during 1995.
Out of all this , something good will come.
Susan left after the rehearsal. She did it well."""

# Process the text with Stanza
doc = nlp(text)

def analyze_sentence(sentence):
    """
    Performs an analysis of a sentence, including:
    - Dependency parsing
    - Creating a dependency graph
    """
    dependencies = []
    graph_edges = []

    for word in sentence.words:
        dependencies.append({
            "word": word.text,
            "pos": word.upos,
            "head": word.head,
            "deprel": word.deprel
        })

        if word.head != 0:  # Exclude root node
            graph_edges.append((word.id, word.head))

    return {
        "sentence": sentence.text,
        "dependencies": dependencies,
        "graph_edges": graph_edges
    }

# Create a list to store the results
results = []

# Iterate through the sentences and perform analysis
for sentence in doc.sentences:
    results.append(analyze_sentence(sentence))

# Print the results in JSON format
print(json.dumps(resultados, indent=4))

# Save the results to a JSON file
with open("dependency_graph_analysis.json", "w") as f:
    json.dump(resultados, f, indent=4)

print("Dependency graph analysis results saved to 'dependency_graph_analysis.json'")

# Visualize the graphs (optional)
for sentence_data in results:
    G = nx.DiGraph()
    for edge in sentence_data['graph_edges']:
        G.add_edge(edge[0], edge[1])

    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10)
    plt.title(sentence_data['sentence'])
    plt.show()